---
title: Cluster Configuration Properties
toc_min_heading_level: 2
toc_max_heading_level: 2
---

<head>
  <meta name="title" content="Cluster configuration properties | Redpanda Docs"/>
  <meta name="description" content="Cluster configuration properties list." />
  <link rel="canonical" href="https://docs.redpanda.com/docs/reference/cluster-properties/" />
</head>

import Aliases from '../shared/_aliases.mdx'

Cluster configuration properties are the same for all nodes in a cluster. They can be set at the cluster level.

For information on how to edit cluster properties, see [Configuring Cluster Properties](../../manage/cluster-maintenance/cluster-property-configuration). Some properties require the cluster to be restarted in order to be applied; see a specific property's reference for whether restart is required.

-------------------------------------------------------------------------------

## Admin properties

### admin_api_require_auth

Whether admin API clients must provide HTTP basic authentication headers.

**Default**: false

**Restart required**: no

---

### superusers

List of superuser usernames.

**Default**: null

**Restart required**: no

-------------------------------------------------------------------------------

## Object storage properties


### cloud_storage_azure_storage_account

The name of the Azure storage account to use with Tiered Storage. If `null`, the property is disabled.

**Type**: string

**Default**: null

**Restart required**: yes

---

### cloud_storage_azure_container

The name of the Azure container to use with Tiered Storage. If `null`, the property is disabled.

:::note
The container must belong to [cloud_storage_azure_storage_account](#cloud_storage_azure_storage_account).
:::

**Type**: string

**Default**: null

**Restart required**: yes

---

### cloud_storage_azure_shared_key

The shared key to be used for Azure Shared Key authentication with the Azure storage account configured by [cloud_storage_azure_storage_account](#cloud_storage_azure_storage_account).  If `null`, the property is disabled.

:::note
Redpanda expects this key string to be Base64 encoded.
:::

**Type**: string

**Default**: null

**Restart required**: yes

---

### cloud_storage_access_key

AWS or GCP access key.

**Default**: null

**Restart required**: yes

---

### cloud_storage_api_endpoint

Optional API endpoint.

**Default**: null

**Restart required**: yes

---

### cloud_storage_api_endpoint_port

TLS port override.

**Default**: 443

**Restart required**: yes

---

### cloud_storage_bucket

AWS or GCS bucket that should be used to store data.

**Default**: null

**Restart required**: yes

---

### cloud_storage_cache_size

Max size of object storage cache.

**Units**: bytes

**Default**: 21474836480 (20 GiB)

**Restart required**: yes

---

### cloud_storage_credentials_source

The source of credentials used to connect to cloud services.

**Default**: config_file

**Valid values**: config_file, aws_instance_metadata, sts, gcp_instance_metadata

**Restart required**: yes

---

### cloud_storage_disable_tls

Disable TLS for all S3 or GCS connections.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### cloud_storage_enabled

Enable object storage. Must be set to `true` to use [Tiered Storage](../../manage/tiered-storage) or Remote Read Replicas.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### cloud_storage_max_connections

Max number of simultaneous connections to S3 per shard. Includes connections used for both uploads and downloads.

**Units**: number of simultaneous connections

**Default**: 20

**Restart required**: yes

---

### cloud_storage_region

AWS or GCP region that houses the bucket used for storage.

**Type**: string

**Default**: null

**Restart required**: yes

---

### cloud_storage_secret_key

AWS or GCP secret key.

**Type**: string

**Default**: null

**Restart required**: yes

---

### cloud_storage_trust_file

Path to certificate that should be used to validate server certificate during TLS handshake.

**Type**: string

**Default**: null

**Restart required**: yes

-------------------------------------------------------------------------------

## Cluster management properties

### cluster_id

Cluster identifier.

**Type**: string

**Default**: null

**Restart required**: no

---

### enable_auto_rebalance_on_node_add

<Aliases name="badge-deprecated" />

Enable automatic partition rebalancing when new nodes are added.

**Type**: boolean

**Default**: false

**Restart required**: no

---

### enable_controller_log_rate_limiting

Flag to enable limiting the write rate for the controller log.

**Type**: boolean

**Default**: false

**Restart required**: no

---

### enable_leader_balancer

Enable automatic leadership rebalancing. Mode is set by [`leader_balancer_mode`](#leader_balancer_mode).

**Type**: boolean

**Default**: true

**Restart required**: no

---

### enable_rack_awareness

Enable rack-aware replica assignment.

**Type**: boolean

**Default**: false

**Restart required**: no

---

### leader_balancer_mode

Mode of the leader balancer for optimizing movements of leadership between shards (logical CPU cores). Enabled by [`enable_leader_balancer`](#enable_leader_balancer).

Valid modes:
- `random_hill_climbing`: a shard is randomly chosen and leadership is moved to it if the load on the original shard is reduced.
- `greedy_balanced_shards`: leadership movement is based on a greedy heuristic of moving leaders from the most loaded shard to the least loaded shard.

**Default**: `random_hill_climbing`

**Restart required**: no

---

### partition_autobalancing_mode

Mode of [partition balancing](../../manage/cluster-maintenance/cluster-balancing) for a cluster.

Available modes:
- `node_add`: partition balancing happens when a node is added.
- `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an [Enterprise license](../../get-started/licenses), and it is customized by [partition_autobalancing_node_availability_timeout_sec](#partition_autobalancing_node_availability_timeout_sec) and [partition_autobalancing_max_disk_usage_percent](#partition_autobalancing_max_disk_usage_percent) properties.
- `off`: partition balancing is disabled. This option is not recommended for production clusters.

**Default**: `node_add`

**Restart required**: no

**Related topics**:
- [Configure Continuous Data Balancing](../../manage/cluster-maintenance/continuous-data-balancing)

---

### partition_autobalancing_node_availability_timeout_sec

:::note
This property applies only when [partition_autobalancing_mode](#partition_autobalancing_mode) is set to `continuous`.
:::

When a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node.

**Units**: seconds

**Default**: 900 (15 min)

**Restart required**: no

**Related topics**:
- [Configure Continuous Data Balancing](../../manage/cluster-maintenance/continuous-data-balancing)

---

### partition_autobalancing_max_disk_usage_percent

:::note
This property applies only when [partition_autobalancing_mode](#partition_autobalancing_mode) is set to `continuous`.
:::

When the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node.

**Units**: percent of disk used

**Default**: 80

**Range**: [5, 100]

**Related topics**:
- [Configure Continuous Data Balancing](../../manage/cluster-maintenance/continuous-data-balancing)

-------------------------------------------------------------------------------

## Kafka API properties

### kafka_admin_topic_api_rate

Target quota rate for partition mutations per [`default_window_sec`](../tunable-properties#default_window_sec). If `null`, the property is disabled, and no quota rate is applied.

**Units**: partition mutations per default_window_second

**Default**: null

**Range**: [1, ...]

**Restart required**: no

**Related properties**: 
- [`default_window_sec`](../tunable-properties#default_window_sec)

---

### kafka_client_group_byte_rate_quota

A map specifying the produce-rate quota per client group. 

The configurable fields:
- `group_name`: name of a client group
- `clients_prefix`: prefix to prepend to the name of each client belonging to the group specified by `group_name`
- `quota`: produce-rate quota of each client in bytes per second

An example: `([{'group_name': 'first_group','clients_prefix': 'group_1','quota': 10240}])`

**Default**: {} (empty map)

**Restart required**: no

**Related topics**:
- [Client group throughput limits](../../manage/cluster-maintenance/manage-throughput#client-group-throughput-limits)

---

### kafka_client_group_fetch_byte_rate_quota

A map specifying the fetch-rate quota per client group. 

The configurable fields:
- `group_name`: name of a client group
- `clients_prefix`: prefix to prepend to the name of each client belonging to the group specified by `group_name`
- `quota`: fetch-rate quota of each client in bytes per second

An example: `([{'group_name': 'first_group','clients_prefix': 'group_1','quota': 10240}])`

**Default**: {} (empty map)

**Restart required**: no

**Related topics**:
- [Client group throughput limits](../../manage/cluster-maintenance/manage-throughput#client-group-throughput-limits)

---

### enable_idempotence

Enable idempotent producers.

**Type**: boolean

**Default**: true

**Restart required**: yes

---

### enable_sasl

Enable SASL authentication for Kafka connections.

**Type**: boolean

**Default**: false

**Restart required**: no

---

### fetch_max_bytes

Maximum number of bytes returned in a fetch request.

**Units**: bytes

**Default**: 57671680 (55 MiB)

**Restart required**: no

---

### group_max_session_timeout_ms

The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.

**Units**: milliseconds

**Default**: 300000 (300 sec)

**Restart required**: no

---

### group_min_session_timeout_ms

The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating which can overwhelm broker resources.

**Units**: milliseconds

**Default**: 6000 (6 sec)

**Restart required**: no

---

### kafka_connection_rate_limit

Maximum connections per second for one core. If `null` (the default), the number of connections per second is unlimited.

**Units**: number of connections per second, per core

**Default**: null

**Range**: [1, ...]

**Restart required**: yes

**Related topics**:
- [Limit client connections](../../manage/cluster-maintenance/configure-availability#limit-client-connections)

---

### kafka_connection_rate_limit_overrides

Overrides the maximum connections per second for one core for the specified IP addresses (for example, `['127.0.0.1:90', '50.20.1.1:40']`)

**Type**: string

**Default**: null

**Restart required**: no

**Related topics**:
- [Limit client connections](../../manage/cluster-maintenance/configure-availability#limit-client-connections)

---

### kafka_connections_max

Maximum number of Kafka client connections per broker. If `null`, the property is disabled.

**Units**: number of Kafka client connections per broker

**Default**: null

**Restart required**: no

**Related topics**:
- [Limit client connections](../../manage/cluster-maintenance/configure-availability#limit-client-connections)

---

### kafka_connections_max_overrides

A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`

**Default**: {} (empty list)

**Restart required**: no

**Related topics**:
- [Limit client connections](../../manage/cluster-maintenance/configure-availability#limit-client-connections)

---

### kafka_connections_max_per_ip

Maximum number of Kafka client connections per IP address, per broker. If `null`, the property is disabled.

**Units**: number of Kafka client connections per IP address, per broker

**Default**: null

**Restart required**: no

**Related topics**:
- [Limit client connections](../../manage/cluster-maintenance/configure-availability#limit-client-connections)

---

### kafka_enable_authorization

Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by [enable_sasl](#enable_sasl). 

Valid values:
- `null`: Ignored. Authorization is enabled with [`enable_sasl`](#enable_sasl): `true`
- `true`: authorization is required.
- `false`: authorization is disabled.

**Type**: boolean

**Default**: null

**Related properties**:
- [enable_sasl](#enable_sasl)
- `kafka_api[].authentication_method`

---

### kafka_enable_partition_reassignment

Enable the Kafka partition reassignment API.

**Type**: boolean

**Default**: true

**Restart required**: no

---

### kafka_group_recovery_timeout_ms

Kafka group recovery timeout.

**Units**: milliseconds

**Default**: 30000 (30 sec)

**Restart required**: no

---

### kafka_mtls_principal_mapping_rules

Principal mapping rules for mTLS authentication on the Kafka API. If `null`, the property is disabled.

**Default**: null

**Restart required**: no

---

### kafka_nodelete_topics

A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.

**Default**: `['__audit', '__consumer_offsets', '_schemas']`

**Restart required**: no

**Related topics**:
- [Consumer Offsets](../../develop/consume-data/consumer-offsets)
- [Schema Registry](../../manage/schema-registry)

---

### kafka_noproduce_topics

A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.

**Default**: `['__audit']`

**Restart required**: no

---

### kafka_qdc_enable

Enable Kafka queue depth control.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### kafka_qdc_max_latency_ms

Maximum latency threshold for Kafka queue depth control depth tracking.

**Units**: milliseconds

**Default**: 80

**Restart required**: yes

---

### kafka_quota_balancer_node_period_ms

The period at which the intra-node throughput quota balancer runs. 

It may take longer for the balancer to complete a single balancing step than the period this property specifies, so the actual period may be more than configured here.

If `0`, the balancer is disabled and all throughput quotas are immutable.

**Units**: milliseconds

**Default**: 750

**Range**: [0, ]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---

### kafka_quota_balancer_min_shard_throughput_ratio

The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. While the value applies equally to ingress and egress traffic, the default shard quota can be different for ingress and egress and therefore result in different minimum throughput bytes-per-second (bps) values. 

Both `kafka_quota_balancer_min_shard_throughput_ratio` and [kafka_quota_balancer_min_shard_throughput_bps](#kafka_quota_balancer_min_shard_throughput_bps) can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties.

If set to `0.0`, the minimum is disabled. If set to `1.0`, then the balancer won't be able to rebalance quota without violating this ratio, consequently precluding the balancer from adjusting shards' quotas.

**Type**: double

**Units**: ratio of default shard quota

**Default**: 0.01

**Range**: [0.0, 1.0]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---

### kafka_quota_balancer_min_shard_throughput_bps

The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed in bytes per second. The value applies equally to ingress and egress traffic.

kafka_quota_balancer_min_shard_throughput_bps doesn't override the limit settings, [kafka_throughput_limit_node_in_bps](#kafka_throughput_limit_node_in_bps) and [kafka_throughput_limit_node_out_bps](#kafka_throughput_limit_node_out_bps). Consequently, the value of 
`kafka_throughput_limit_node_in_bps` or `kafka_throughput_limit_node_out_bps` can result in lesser throughput than kafka_quota_balancer_min_shard_throughput_bps.

Both [kafka_quota_balancer_min_shard_throughput_ratio](#kafka_quota_balancer_min_shard_throughput_ratio) and kafka_quota_balancer_min_shard_throughput_bps can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties.

If set to `0`, no minimum is enforced.

**Units**: bytes per second

**Default**: 256

**Range**: [0, ...]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---

### kafka_quota_balancer_window_ms

Time window used to average the current throughput measurement for the quota balancer.

**Units**: milliseconds

**Default**: 5000

**Range**: [1, ...]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---


### kafka_rpc_server_tcp_recv_buf

Size of the Kafka server TCP receive buffer. If `null`, the property is disabled.

**Units**: bytes

**Default**: null

**Range**: [32 KiB, ...], aligned to 4096 bytes

---

### kafka_rpc_server_tcp_send_buf

Size of the Kafka server TCP transmit buffer. If `null`, the property is disabled.

**Units**: bytes

**Default**: null

**Range**: [32 KiB, ...], aligned to 4096 bytes

---

### kafka_throughput_limit_node_in_bps

The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.).

If `null`, the property is disabled, and traffic is not limited.

**Units**: bytes per second

**Default**: null

**Range**: [1, ...]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---

### kafka_throughput_limit_node_out_bps

The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.).

If `null`, the property is disabled, and traffic is not limited. 

**Units**: bytes per second

**Default**: null

**Range**: [1, ...]

**Restart required**: no

**Related topics**:
- [Node-wide throughput limits](../../manage/cluster-maintenance/manage-throughput#node-wide-throughput-limits)

---

### log_segment_ms

Default lifetime of log segments. If `null`, the property is disabled, and no default lifetime is set. This property can also be set in the Kafka API using the Kafka-compatible alias, `log.roll.ms`.

The topic property [`segment.ms`](../topic-properties#segmentms) overrides the value of `log_segment_ms` at the topic level.

:::note
The default value of `log_segment_ms` (14 days) is different than Kafka's default value (7 days). Redpanda's segments store timestamps in 32-bit signed values that represent a range of up to about 24 days, and we chose as default a 14 day roll period that's a nice round number that keeps us well within that range.
:::

**Units**: milliseconds

**Default**: 1209600000 (14 days)

**Range**: [60000 (60 sec), ...]

**Restart required**: no

**Related properties**: 
- [log_segment_ms_min](../tunable-properties#log_segment_ms_min) 
- [log_segment_ms_max](../tunable-properties#log_segment_ms_max)

---

### rm_sync_timeout_ms

Resource manager's synchronization timeout. Maximum time for this node to wait for internal state machine to catch up with all events written by previous leaders before rejecting a request.

**Units**: milliseconds

**Default**: 10000 (10 sec)

**Restart required**: yes

---

### rpc_server_listen_backlog

Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.

**Units**: number of queue entries

**Default**: null

**Range**: [1, ...]

**Restart required**: yes

---

### rpc_server_tcp_recv_buf

Internal RPC TCP receive buffer size. If `null` (the default value), no buffer size is set by Redpanda.

**Units**: bytes

**Default**: null

**Range**: [32 KiB, ...], aligned to 4096 bytes

**Restart required**: yes

---

### rpc_server_tcp_send_buf

Internal RPC TCP send buffer size. If `null` (the default value), no buffer size is set by Redpanda.

**Units**: bytes

**Default**: null

**Range**: [32 KiB, ...], aligned to 4096 bytes

**Restart required**: yes

---

### sasl_kerberos_config

The location of the Kerberos `krb5.conf` file for Redpanda.

**Type**: string

**Default**: `/etc/krb5.conf`

**Restart required**: no

---

### sasl_kerberos_keytab

The location of the Kerberos keytab file for Redpanda.

**Type**: string

**Default**: `/var/lib/redpanda/redpanda.keytab`

**Restart required**: no

---

### sasl_kerberos_principal

The primary of the Kerberos Service Principal Name (SPN) for Redpanda.

**Type**: string

**Default**: `redpanda`

**Restart required**: no

---

### sasl_kerberos_principal_mapping

Rules for mapping Kerberos principal names to Redpanda user principals.

**Type**: array of string

**Default**: `["DEFAULT"]`

**Restart required**: no

---

### sasl_mechanisms

A list of supported SASL mechanisms. `SCRAM` and `GSSAPI` are allowed.

**Type**: array of string

**Default**: `["SCRAM"]`

**Valid values**: `"SCRAM"`, `"GSSAPI"`

**Restart required**: no

---

### target_quota_byte_rate

Target quota byte rate.

The `target_quota_byte_rate` property applies to a producer client that isn't a member of a client group configured by [`kafka_client_group_byte_rate_quota`](#kafka_client_group_byte_rate_quota). It sets the maximum throughput quota of a client sending to a Redpanda broker node.

**Units**: bytes per second

**Default**: 2147483648 (2 GiB)

**Range**: [1048576 (1 MiB), ...]

**Restart required**: no

**Related topics**:
- [Client throughput limits](../../manage/cluster-maintenance/manage-throughput#client-throughput-limits)

---

### target_fetch_quota_byte_rate

Target fetch-size quota byte rate. If `null`, the property is disabled, and no quota byte rate is applied.

**Units**: bytes per second

**Default**: null

**Restart required**: no

-------------------------------------------------------------------------------

## Metrics properties

### aggregate_metrics

Enable aggregation of metrics returned by the [/metrics](../../reference/internal-metrics-reference) endpoint. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### disable_metrics

Disable registering metrics exposed on the internal metrics endpoint.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### disable_public_metrics

Disable registering metrics exposed on the public metrics endpoint.

**Type**: boolean

**Default**: false

**Restart required**: yes

---

### enable_metrics_reporter

Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by [metrics_reporter_report_interval](../tunable-properties#metrics_reporter_report_interval).

:::note
The cluster metrics of the metrics reporter are different from [monitoring metrics](../../manage/monitoring). 
- The metrics reporter exports customer usage metrics for consumption by Redpanda Data.
- Monitoring metrics are exported for consumption by Redpanda users to monitor their system's health.
:::

**Type**: boolean

**Default**: true

**Restart required**: no

-------------------------------------------------------------------------------

## Raft properties

### raft_learner_recovery_rate

Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders.  

**Units**: bytes per second

**Default**: 104857600 (100 MB/sec) 

**Restart required**: no

-------------------------------------------------------------------------------

## Storage properties

### delete_retention_ms

Delete segments that are older than this age.

The topic property [`retention.ms`](../topic-properties#retentionms) overrides the value of `delete_retention_ms` at the topic level.

:::note
The `delete_retention_ms` cluster property is different than the `delete.retention.ms` Apache Kafka topic property, and Redpanda doesn't support `delete.retention.ms`.
- `delete_retention_ms` sets the same time-based retention limit value for all topics in the cluster. 
- `delete.retention.ms` sets the time-based retention limit for tombstone markers of a compacted topic. 
::: 

**Units**: milliseconds

**Default**: 604800000 (1 week)

**Restart required**: no

---

### log_cleanup_policy

Default cleanup policy for topic logs. 

The topic property [`cleanup.policy`](../topic-properties#cleanuppolicy) overrides the value of `log_cleanup_policy` at the topic level.

**Default**: `delete`

**Valid Values**: `compact`, `delete`, `compact,delete`, `none`

**Restart required**: no

---

### log_compaction_interval_ms

How often to trigger background compaction.

**Units**: milliseconds

**Default**: 10000 (10 sec)

**Restart required**: no

---

### log_compression_type

Default topic compression type (gzip, snappy, lz4, zstd, producer, or none).

The topic property [`compression.type`](../topic-properties#compressiontype) overrides the value of `log_compression_type` at the topic level.

**Default**: `producer`

**Valid values**: `gzip`, `snappy`, `lz4`, `zstd`, `producer`, `none`

**Restart required**: no

---

### log_message_timestamp_type

Default timestamp type for topic messages (CreateTime or LogAppendTime). 

The topic property [`message.timestamp.type`](../topic-properties#messagetimestamptype) overrides the value of `log_message_timestamp_type` at the topic level.

**Default**: `CreateTime`

**Valid values**: `CreateTime`, `LogAppendTime`

**Restart required**: no

---

### retention_local_target_bytes_default

Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled.

This property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. See [Configure message retention](../../manage/cluster-maintenance/disk-utilization/#configure-message-retention).

:::note
Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.
:::

**Units**: bytes

**Default**: null

**Restart required**: no

**Related properties**: 
- [retention_local_target_ms_default](#retention_local_target_ms_default)

---

### retention_local_target_ms_default

Local retention time target for partitions of topics with object storage write enabled.

This property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. See [Configure message retention](../../manage/cluster-maintenance/disk-utilization/#configure-message-retention).

:::note
Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.
:::

**Units**: milliseconds

**Default**: 86400000 (24 hours)

**Restart required**: no

**Related properties**: 
- [retention_local_target_bytes_default](#retention_local_target_bytes_default)

---

### storage_strict_data_init

Requires that an empty file named `.redpanda_data_dir` be present in the [data directory](../node-properties#data_directory). If set `true`, Redpanda will refuse to start if the file is not found in the data directory.

**Default**: false

**Restart required**: no

---

### storage_ignore_timestamps_in_future_sec

The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda.

When determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime).

By default, `storage_ignore_timestamps_in_future_sec` is disabled (null).

:::tip
To figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system:
1. Look for logs with segments that are unexpectedly large and not being cleaned up.
1. In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized.
1. If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed.
1. If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`. 
:::

**Units**: seconds

**Default**: null

**Restart required**: no

-------------------------------------------------------------------------------

## Support properties

### metrics_reporter_url

URL of the cluster metrics reporter.

**Default**: `https://m.rp.vectorized.io/v2`

-------------------------------------------------------------------------------

## Topic and partition properties

### auto_create_topics_enabled

Allow automatic topic creation.

**Type**: boolean

**Default**: false

**Restart required**: no

---

### default_topic_partitions

Default number of partitions per topic.

**Units**: number of partitions per topic

**Default**: 1

**Restart required**: no

---

### default_topic_replications

Default replication factor for new topics.

The topic property [`replication.factor`](../topic-properties#replicationfactor) overrides the value of `default_topic_replications` at the topic level.

**Units**: number of replicas per topic

**Default**: 1

**Range**: [1, ...], must be odd

**Restart required**: no

---

### internal_topic_replication_factor

Target replication factor for internal topics.

**Units**: number of replicas per topic

**Default**: 3

**Restart required**: yes

---

### retention_bytes

Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied.

The topic property [`retention.bytes`](../topic-properties#retentionbytes) overrides the value of `retention_bytes` at the topic level.

**Units**: bytes per partition

**Default**: null

**Restart required**: no

---

### rm_violation_recovery_policy

<Aliases name="badge-deprecated" />

Describes how to recover from an invariant violation on the partition level.

**Default**: 0

**Restart required**: yes

-------------------------------------------------------------------------------

## Transaction properties

### enable_transactions

Enable transactions (atomic writes).

**Type**: boolean

**Default**: true

**Restart required**: yes

---

### seq_table_min_size

The minimum threshold number of sessions to keep in the seq table. Not affected by compaction.

**Default**: 1000

**Restart required**: yes

---

### tm_sync_timeout_ms

Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.

**Units**: milliseconds

**Default**: 10000 (10 sec)

**Restart required**: yes

---

### tm_violation_recovery_policy

<Aliases name="badge-deprecated" />

Describes how to recover from an invariant violation at the transaction coordinator level.

**Default**: crash

**Restart required**: yes

---

### transaction_coordinator_cleanup_policy

Cleanup policy for a transaction coordinator topic.

**Default**: `delete`

**Valid Values**: `compact`, `delete`, `compact,delete`, `none`

**Restart required**: no

---

### transaction_coordinator_delete_retention_ms

Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than [transactional_id_expiration_ms](#transactional_id_expiration_ms).

**Units**: milliseconds

**Default**: 604800000 (1 week)

**Restart required**: no

---

### transactional_id_expiration_ms

Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.

**Units**: milliseconds

**Default**: 604800000 (1 week)

**Restart required**: yes

---

### tx_timeout_delay_ms

Delay before scheduling the next check for timed out transactions.

**Units**: milliseconds

**Default**: 1000

**Restart required**: yes

-------------------------------------------------------------------------------


## Suggested reading

- Fast distributed transactions with Redpanda [article](https://redpanda.com/blog/fast-transactions/)
