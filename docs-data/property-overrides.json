{
  "properties": {
    "abort_index_segment_size": {
      "description": "Capacity (in number of txns) of an abort index segment.\nEach partition tracks the aborted transaction offset ranges to help service client requests. If the number of transactions increases beyond this threshold, they are flushed to disk to ease memory pressure. Then they're loaded on demand. This configuration controls the maximum number of aborted transactions before they are flushed to disk.",
      "config_scope": "cluster"
    },
    "admin": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  admin:",
        "    - name: <admin-api-name>",
        "      address: <external-broker-hostname>",
        "      port: <admin-api-port>",
        "----"
      ],
      "description": "Network address for the glossterm:Admin API[] server.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "admin_api_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  admin_api_tls:",
        "    - name: <admin-api-tls-name>",
        "      enabled: true",
        "      cert_file: <path-to-cert-file>",
        "      key_file: <path-to-key-file>",
        "      truststore_file: <path-to-truststore-file>",
        "      require_client_auth: true",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "aggregate_metrics": {
      "description": "Enable aggregation of metrics returned by the xref:reference:internal-metrics-reference.adoc[`/metrics`] endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.",
      "related_topics": [
        "xref:reference:internal-metrics-reference.adoc[`/metrics`]"
      ],
      "config_scope": "cluster"
    },
    "cleanup.policy": {
      "description": "The cleanup policy to apply for log segments of a topic.\nWhen `cleanup.policy` is set, it overrides the cluster property xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]"
      ],
      "config_scope": "topic"
    },
    "cloud_storage_azure_adls_endpoint": {
      "description": "Azure Data Lake Storage v2 endpoint override. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.\n\nIf not set, this is automatically generated using `dfs.core.windows.net` and <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>.",
      "config_scope": "object-storage"
    },
    "cloud_storage_azure_adls_port": {
      "description": "Azure Data Lake Storage v2 port override. See also: <<cloud_storage_azure_adls_endpoint,`cloud_storage_azure_adls_endpoint`>>. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.",
      "config_scope": "object-storage"
    },
    "cloud_storage_azure_container": {
      "description": "The name of the Azure container to use with Tiered Storage. If `null`, the property is disabled.\n\nNOTE: The container must belong to <<cloud_storage_azure_storage_account,`cloud_storage_azure_storage_account`>>.",
      "config_scope": "object-storage"
    },
    "cloud_storage_azure_managed_identity_id": {
      "description": "The managed identity ID to use for access to the Azure storage account. To use Azure managed identities, you must set <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>> to `azure_vm_instance_metadata`. See xref:manage:security/iam-roles.adoc[IAM Roles] for more information on managed identities.",
      "related_topics": [
        "xref:manage:security/iam-roles.adoc[IAM Roles]"
      ],
      "config_scope": "object-storage"
    },
    "cloud_storage_bucket": {
      "description": "AWS or GCP bucket that should be used to store data.\n\nWARNING: Modifying this property after writing data to a bucket could cause data loss.",
      "config_scope": "object-storage"
    },
    "cloud_storage_cache_directory": {
      "description": "Directory for archival cache. Set when the xref:reference:properties/cluster-properties.adoc#cloud_storage_enabled[`cloud_storage_enabled`] cluster property is enabled. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_cache_directory: <cache-directory-path>",
        "----",
        "\n",
        "Replace `<cache-directory-path>` with the full path to your desired cache directory."
      ],
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#cloud_storage_enabled[`cloud_storage_enabled`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "cloud_storage_cache_size_percent": {
      "related_topics": [
        "xref:reference:cluster-properties.adoc#disk_reservation_percent[`disk_reservation_percent`]"
      ],
      "config_scope": "object-storage"
    },
    "cloud_storage_chunk_prefetch": {
      "description": "Number of chunks to prefetch ahead of every downloaded chunk. Prefetching additional chunks can enhance read performance by reducing wait times for sequential data access. A value of `0` disables prefetching, relying solely on on-demand downloads. Adjusting this property allows for tuning the balance between improved read performance and increased network and storage I/O.",
      "config_scope": "object-storage"
    },
    "cloud_storage_credentials_host": {
      "description": "The hostname to connect to for retrieving role based credentials. Derived from <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>> if not set. Only required when using IAM role based access. To authenticate using access keys, see <<cloud_storage_access_key,`cloud_storage_access_key`>>.",
      "config_scope": "object-storage"
    },
    "cloud_storage_crl_file": {
      "description": "Path to certificate revocation list for <<cloud_storage_trust_file, `cloud_storage_trust_file`>>.",
      "config_scope": "object-storage"
    },
    "cloud_storage_disable_chunk_reads": {
      "description": "Disable chunk reads and switch back to legacy mode where full segments are downloaded. When set to `true`, this option disables the more efficient chunk-based reads, causing Redpanda to download entire segments. This legacy behavior might be useful in specific scenarios where chunk-based fetching is not optimal.",
      "config_scope": "object-storage"
    },
    "cloud_storage_enable_compacted_topic_reupload": {
      "description": "Enable re-uploading data for compacted topics.\nWhen set to `true`, Redpanda can re-upload data for compacted topics to object storage, ensuring that the most current state of compacted topics is available in the cloud. Disabling this property (`false`) may reduce storage and network overhead but at the risk of not having the latest compacted data state in object storage.",
      "config_scope": "object-storage"
    },
    "cloud_storage_enable_remote_read": {
      "description": "Default remote read config value for new topics.\nWhen set to `true`, new topics are by default configured to allow reading data directly from object storage, facilitating access to older data that might have been offloaded as part of Tiered Storage. With the default set to `false`, remote reads must be explicitly enabled at the topic level.",
      "config_scope": "object-storage"
    },
    "cloud_storage_enable_remote_write": {
      "description": "Default remote write value for new topics.\nWhen set to `true`, new topics are by default configured to upload data to object storage. With the default set to `false`, remote write must be explicitly enabled at the topic level.",
      "config_scope": "object-storage"
    },
    "cloud_storage_enable_segment_merging": {
      "related_topics": [
        "xref:manage:tiered-storage.adoc#object-storage-housekeeping[Object storage housekeeping]"
      ],
      "config_scope": "object-storage"
    },
    "cloud_storage_enabled": {
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "object-storage"
    },
    "cloud_storage_housekeeping_interval_ms": {
      "description": "Interval, in milliseconds, between object storage housekeeping tasks.",
      "config_scope": "object-storage"
    },
    "cloud_storage_inventory_hash_store": {
      "description": "Directory to store inventory report hashes for use by cloud storage scrubber. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_inventory_hash_store: <inventory-hash-directory-path>",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "cloud_storage_max_connection_idle_time_ms": {
      "description": "Defines the maximum duration an HTTPS connection to object storage can stay idle, in milliseconds, before being terminated.\nThis setting reduces resource utilization by closing inactive connections. Adjust this property to balance keeping connections ready for subsequent requests and freeing resources associated with idle connections.",
      "config_scope": "object-storage"
    },
    "cloud_storage_metadata_sync_timeout_ms": {
      "description": "Timeout for xref:manage:tiered-storage.adoc[] metadata synchronization.",
      "config_scope": "object-storage"
    },
    "cloud_storage_recovery_topic_validation_depth": {
      "description": "Number of metadata segments to validate, from newest to oldest, when <<cloud_storage_recovery_topic_validation_mode,`cloud_storage_recovery_topic_validation_mode`>> is set to `check_manifest_and_segment_metadata`.",
      "config_scope": "object-storage"
    },
    "cloud_storage_segment_size_target": {
      "description": "Desired segment size in the object storage. The default is set in the topic-level `segment.bytes` property.",
      "config_scope": "object-storage"
    },
    "cloud_storage_upload_ctrl_update_interval_ms": {
      "description": "The interval (in milliseconds) for updating the controller that manages the priority of Tiered Storage uploads. This property determines how frequently the system recalculates and adjusts the work scheduling for uploads to object storage.\n\nThis is an internal-only configuration and should be enabled only after consulting with Redpanda support.",
      "config_scope": "object-storage"
    },
    "cluster_id": {
      "description": "NOTE: This property is read-only in Redpanda Cloud.\n\nCluster identifier.",
      "config_scope": "cluster"
    },
    "compaction.strategy": {
      "description": "Specifies the strategy used to determine which records to remove during log compaction. The compaction strategy controls how Redpanda identifies and removes duplicate records while preserving the latest value for each key.",
      "related_topics": [
        "xref:./cluster-properties.adoc#compaction_strategy[`compaction_strategy`]"
      ],
      "config_scope": "topic"
    },
    "compaction_ctrl_update_interval_ms": {
      "description": "The interval (in milliseconds) for updating the controller responsible for compaction tasks. The controller uses this interval to decide how to prioritize background compaction work, which is essential for maintaining efficient storage use.\n\nThis is an internal-only configuration and should be enabled only after consulting with Redpanda support.",
      "config_scope": "cluster"
    },
    "compression.type": {
      "description": "Redpanda ignores this property and always uses producer compression semantics. If producers send compressed data, Redpanda stores and serves it as-is. If producers send uncompressed data, Redpanda stores it uncompressed.\n\nThis property exists for Apache Kafka compatibility. Configure compression in your producers instead of using this topic property.\n\nCompression reduces message size and improves throughput, but increases CPU utilization. Enable producer batching to increase compression efficiency.\n\nWhen set, this property overrides the cluster property xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]",
        "xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]",
        "xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]",
        "xref:develop:produce-data/configure-producers.adoc#commonly-used-producer-configuration-options[Common producer configuration options]"
      ],
      "config_scope": "topic"
    },
    "core_balancing_continuous": {
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "crash_loop_sleep_sec": {
      "description": "*Introduced in v24.3.4*\n\nThe amount of time the broker sleeps before terminating when the limit on consecutive broker crashes (<<crash_loop_limit, `crash_loop_limit`>>) is reached. This property provides a debugging window for you to access the broker before it terminates, and is particularly useful in Kubernetes environments.\n\nIf `null`, the property is disabled, and the broker terminates immediately after reaching the crash loop limit.\n\nFor information about how to reset the crash loop limit, see the <<crash_loop_limit, `crash_loop_limit`>> broker property.",
      "version": "v24.3.4",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "data_transforms_binary_max_size": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe maximum size for a deployable WebAssembly binary that the broker can store.",
      "config_scope": "cluster"
    },
    "data_transforms_per_core_memory_reservation": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.",
      "config_scope": "cluster"
    },
    "data_transforms_per_function_memory_limit": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.",
      "config_scope": "cluster"
    },
    "data_transforms_read_buffer_memory_percentage": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nThe percentage of available memory in the transform subsystem to use for read buffers.",
      "config_scope": "cluster"
    },
    "data_transforms_write_buffer_memory_percentage": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nThe percentage of available memory in the transform subsystem to use for write buffers.",
      "config_scope": "cluster"
    },
    "default_leaders_preference": {
      "description": "Default settings for preferred location of topic partition leaders. It can be either \"none\" (no preference), or \"racks:<rack1>,<rack2>,...\" (prefer brokers with rack ID from the list).\nThe list can contain one or more rack IDs. If you specify multiple IDs, Redpanda tries to distribute the partition leader locations equally across brokers in these racks.\nIf config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, leader pinning is disabled across the cluster.\nifndef::env-cloud[]",
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "delete.retention.ms": {
      "description": "The retention time for tombstone records in a compacted topic. Redpanda removes tombstone records after the retention limit is exceeded.\n\nIf you have enabled Tiered Storage and set <<redpandaremoteread,`redpanda.remote.read`>> or <<redpandaremotewrite,`redpanda.remote.write`>> for the topic, you cannot enable tombstone removal.\n\nIf both `delete.retention.ms` and the cluster property config_ref:tombstone_retention_ms,true,properties/cluster-properties[] are set, `delete.retention.ms` overrides the cluster level tombstone retention for an individual topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#tombstone_retention_ms[`tombstone_retention_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]"
      ],
      "config_scope": "topic"
    },
    "developer_mode": {
      "description": "CAUTION: Enabling `developer_mode` isn't recommended for production use.\n\nEnable developer mode, which skips most of the checks performed at startup.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "emergency_disable_data_transforms": {
      "description": "Override the cluster property xref:reference:properties/cluster-properties.adoc#data_transforms_enabled[`data_transforms_enabled`] and disable Wasm-powered data transforms. This is an emergency shutoff button.",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#data_transforms_enabled[`data_transforms_enabled`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "empty_seed_starts_cluster": {
      "description": "Controls how a new cluster is formed. All brokers in a cluster must have the same value.\n\n<<seed_servers,See how the `empty_seed_starts_cluster` broker property works with the `seed_servers` broker property>> to form a cluster.\n\nTIP: For backward compatibility, `true` is the default. Redpanda recommends using `false` in production environments to prevent accidental cluster formation.",
      "config_scope": "broker",
      "category": "redpanda"
    },
    "enable_cluster_metadata_upload_loop": {
      "description": "Enables cluster metadata uploads. Required for xref:manage:whole-cluster-restore.adoc[whole cluster restore].",
      "related_topics": [
        "xref:manage:whole-cluster-restore.adoc[whole cluster restore]"
      ],
      "config_scope": "cluster"
    },
    "enable_consumer_group_metrics": {
      "description": "List of enabled consumer group metrics.\n\n*Accepted values:*\n\n- `group`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_consumers[`redpanda_kafka_consumer_group_consumers`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_topics[`redpanda_kafka_consumer_group_topics`] metrics.\n- `partition`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_committed_offset[`redpanda_kafka_consumer_group_committed_offset`] metric.\n- `consumer_lag`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_max[`redpanda_kafka_consumer_group_lag_max`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_sum[`redpanda_kafka_consumer_group_lag_sum`] metrics\n+\nEnabling `consumer_lag` may add a small amount of additional processing overhead to the brokers, especially in environments with a high number of consumer groups or partitions.\n+\nifndef::env-cloud[]\nUse the xref:reference:properties/cluster-properties.adoc#consumer_group_lag_collection_interval_sec[`consumer_group_lag_collection_interval_sec`] property to control the frequency of consumer lag metric collection.\nendif::[]",
      "related_topics": [
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_consumers[`redpanda_kafka_consumer_group_consumers`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_topics[`redpanda_kafka_consumer_group_topics`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_committed_offset[`redpanda_kafka_consumer_group_committed_offset`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_max[`redpanda_kafka_consumer_group_lag_max`]",
        "xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_sum[`redpanda_kafka_consumer_group_lag_sum`]",
        "xref:reference:properties/cluster-properties.adoc#consumer_group_lag_collection_interval_sec[`consumer_group_lag_collection_interval_sec`]",
        "xref:manage:monitoring.adoc#consumers[Monitor consumer group lag]",
        "xref:manage:monitor-cloud.adoc#consumers[Monitor consumer group lag]"
      ],
      "config_scope": "cluster"
    },
    "enable_host_metrics": {
      "description": "Enable exporting of some host metrics like `/proc/diskstats`, `/proc/snmp` and `/proc/net/netstat`.\n\nHost metrics are prefixed with xref:reference:internal-metrics-reference.adoc#vectorized_host_diskstats_discards[`vectorized_host`] and are available on the `/metrics` endpoint.",
      "related_topics": [
        "xref:reference:internal-metrics-reference.adoc#vectorized_host_diskstats_discards[`vectorized_host`]"
      ],
      "config_scope": "cluster"
    },
    "enable_metrics_reporter": {
      "description": "Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by <<metrics_reporter_report_interval,`metrics_reporter_report_interval`>>.\n\n[NOTE]\n====\nThe cluster metrics of the metrics reporter are different from xref:manage:monitoring.adoc[monitoring metrics].\n\n* The metrics reporter exports customer usage metrics for consumption by Redpanda Data.\n* Monitoring metrics are exported for consumption by Redpanda users.\n====",
      "related_topics": [
        "xref:manage:monitoring.adoc[monitoring metrics]"
      ],
      "config_scope": "cluster"
    },
    "enable_schema_id_validation": {
      "related_topics": [
        "xref:manage:schema-reg/schema-id-validation.adoc[Server-Side Schema ID Validation]"
      ],
      "description": "Mode to enable server-side schema ID validation.\n\n*Accepted values:*\n\n* `none`: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified.\n* `redpanda`: Schema validation is enabled. Only Redpanda topic properties are accepted.\n* `compat`: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.",
      "config_scope": "cluster"
    },
    "flush.bytes": {
      "description": "The maximum bytes not fsynced per partition. If this configured threshold is reached, the log is automatically fsynced, even though it wasn't explicitly requested.",
      "related_topics": [
        "xref:./cluster-properties.adoc#flush_bytes[`flush_bytes`]"
      ],
      "config_scope": "topic"
    },
    "flush.ms": {
      "description": "The maximum delay (in ms) between two subsequent fsyncs. After this delay, the log is automatically fsynced.",
      "related_topics": [
        "xref:./cluster-properties.adoc#flush_ms[`flush_ms`]"
      ],
      "config_scope": "topic"
    },
    "http_authentication": {
      "description": "A list of supported HTTP authentication mechanisms.\n\n*Accepted values:*\n\n* `BASIC`: Basic authentication\n* `OIDC`: OpenID Connect",
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_backlog_controller_p_coeff": {
      "description": "Proportional coefficient for the Iceberg backlog controller. Number of shares assigned to the datalake scheduling group will be proportional to the backlog size error. A negative value means larger and faster changes in the number of shares in the datalake scheduling group.",
      "config_scope": "cluster"
    },
    "iceberg_catalog_type": {
      "description": "Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: `rest`, `object_storage`.\nNOTE: You must set <<iceberg_rest_catalog_endpoint,`iceberg_rest_catalog_endpoint`>> at the same time that you set `iceberg_catalog_type` to `rest`.",
      "config_scope": "cluster"
    },
    "iceberg_default_partition_spec": {
      "description": "ifndef::env-cloud[]\nDefault value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-partition-spec[`redpanda.iceberg.partition.spec`] topic property that determines the partition spec for the Iceberg table corresponding to the topic.\nendif::[]\n\nifdef::env-cloud[]\nDefault value for the `redpanda.iceberg.partition.spec` topic property that determines the partition spec for the Iceberg table corresponding to the topic.\nendif::[]",
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#redpanda-iceberg-partition-spec[`redpanda.iceberg.partition.spec`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_enabled": {
      "description": "ifndef::env-cloud[]\nEnables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-enabled[`redpanda.iceberg.enabled`] topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.\nendif::[]\nifdef::env-cloud[]\nEnables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the `redpanda.iceberg.enabled` topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.\nendif::[]",
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#redpanda-iceberg-enabled[`redpanda.iceberg.enabled`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_invalid_record_action": {
      "description": "ifndef::env-cloud[]\nDefault value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-invalid-record-action[`redpanda.iceberg.invalid.record.action`] topic property.\nendif::[]\nifdef::env-cloud[]\nDefault value for the `redpanda.iceberg.invalid.record.action` topic property.\nendif::[]",
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#redpanda-iceberg-invalid-record-action[`redpanda.iceberg.invalid.record.action`]",
        "xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_latest_schema_cache_ttl_ms": {
      "description": "The TTL for caching the latest schema during translation when using the xref:manage:iceberg/specify-iceberg-schema.adoc#value_schema_latest[`value_schema_latest`] iceberg mode. This setting controls how long the latest schema remains cached during translation, which affects schema refresh behavior and performance.",
      "related_topics": [
        "xref:manage:iceberg/specify-iceberg-schema.adoc#value_schema_latest[`value_schema_latest`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_access_key": {
      "description": "AWS access key for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_access_key[`cloud_storage_access_key`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_access_key[`cloud_storage_access_key`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_region": {
      "description": "AWS region for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_region[`cloud_storage_region`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_region[`cloud_storage_region`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_aws_secret_key": {
      "description": "AWS secret key for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_secret_key[`cloud_storage_secret_key`] when using aws_sigv4 authentication mode.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_secret_key[`cloud_storage_secret_key`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_client_secret": {
      "description": "Secret used with the client ID to query the OAuth token endpoint for Iceberg REST catalog authentication. Required if catalog type is set to `rest` and `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_endpoint": {
      "description": "URL of Iceberg REST catalog endpoint.\nNOTE: If you set <<iceberg_catalog_type,`iceberg_catalog_type`>> to `rest`, you must also set this property at the same time.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_token": {
      "description": "Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties <<iceberg_rest_catalog_client_id,`iceberg_rest_catalog_client_id`>>  and <<iceberg_rest_catalog_client_secret,`iceberg_rest_catalog_client_secret`>>.\nRequired if <<iceberg_rest_catalog_authentication_mode, `iceberg_rest_catalog_authentication_mode`>> is set to `bearer`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_trust": {
      "description": "The contents of a certificate chain to trust for the REST Iceberg catalog.\nifndef::env-cloud[]\nTakes precedence over <<iceberg_rest_catalog_trust_file, `iceberg_rest_catalog_trust_file`>>.\nendif::[]",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_warehouse": {
      "description": "Warehouse to use for the Iceberg REST catalog. Redpanda queries the catalog to retrieve warehouse-specific configurations and automatically configures settings like the appropriate prefix. The prefix is appended to the catalog path (for example, `/v1/\\{prefix}/namespaces`).",
      "config_scope": "cluster"
    },
    "iceberg_target_lag_ms": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#redpanda-iceberg-target-lag-ms[`redpanda.iceberg.target.lag.ms`]"
      ],
      "config_scope": "cluster"
    },
    "initial.retention.local.target.bytes": {
      "description": "A size-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.",
      "related_topics": [
        "xref:./cluster-properties.adoc#initial_retention_local_target_bytes[`initial_retention_local_target_bytes`]",
        "xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "initial.retention.local.target.ms": {
      "description": "A time-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.",
      "related_topics": [
        "xref:./cluster-properties.adoc#initial_retention_local_target_ms[`initial_retention_local_target_ms`]",
        "xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "initial_retention_local_target_bytes_default": {
      "description": "Initial local retention size target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then  all locally-retained data will be delivered to learner when joining the partition replica set.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "cluster"
    },
    "initial_retention_local_target_ms_default": {
      "description": "Initial local retention time target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then all locally-retained data will be delivered to learner when joining the partition replica is set.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "cluster"
    },
    "kafka_api": {
      "description": "IP address and port of the Kafka API endpoint that handles requests. Supports multiple listeners with different configurations.",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`]",
        "xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "kafka_api_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  kafka_api_tls:",
        "    - name: <kafka-api-listener-name>",
        "      enabled: true",
        "      cert_file: <path-to-cert-file>",
        "      key_file: <path-to-key-file>",
        "      truststore_file: <path-to-truststore-file>",
        "      require_client_auth: false",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "kafka_connection_rate_limit_overrides": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_connections_max": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_connections_max_overrides": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_connections_max_per_ip": {
      "related_topics": [
        "xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
      ],
      "config_scope": "cluster"
    },
    "kafka_nodelete_topics": {
      "related_topics": [
        "xref:develop:consume-data/consumer-offsets.adoc[Consumer Offsets]",
        "xref:manage:schema-registry.adoc[Schema Registry]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_control": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc[Manage throughput]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_limit_node_in_bps": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_limit_node_out_bps": {
      "related_topics": [
        "xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]"
      ],
      "config_scope": "cluster"
    },
    "kafka_throughput_replenish_threshold": {
      "related_topics": [
        "xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_in_bps[`kafka_throughput_limit_node_in_bps`]",
        "xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_out_bps[`kafka_throughput_limit_node_out_bps`]",
        "xref:manage:cluster-maintenance/manage-throughput.adoc[Manage Throughput]"
      ],
      "config_scope": "cluster"
    },
    "leader_balancer_mute_timeout": {
      "description": "The length of time that a glossterm:Raft[] group is muted after a leadership rebalance operation. Any group that has been moved, regardless of whether the move succeeded or failed, undergoes a cooling-off period. This prevents Raft groups from repeatedly experiencing leadership rebalance operations in a short time frame, which can lead to instability in the cluster.\n\nThe leader balancer maintains a list of muted groups and reevaluates muted status at the start of each balancing iteration. Muted groups still contribute to overall cluster balance calculations although they can't themselves be moved until the mute period is over.",
      "config_scope": "cluster"
    },
    "leader_balancer_node_mute_timeout": {
      "description": "The duration after which a broker that hasn't sent a heartbeat is considered muted. This timeout sets a threshold for identifying brokers that shouldn't be targeted for leadership transfers when the cluster rebalances, for example, because of unreliable network connectivity.",
      "config_scope": "cluster"
    },
    "log_cleanup_policy": {
      "description": "Default cleanup policy for topic logs.\n\nThe topic property xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`] overrides the value of `log_cleanup_policy` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`]"
      ],
      "config_scope": "cluster"
    },
    "log_compression_type": {
      "description": "IMPORTANT: This property is ignored regardless of the value specified. The behavior is always the same as the `producer` value. Redpanda brokers do not compress or recompress data based on this property. If producers send compressed data, Redpanda stores it as-is; if producers send uncompressed data, Redpanda stores it uncompressed. Other listed values are accepted for Apache Kafka compatibility but are ignored by the broker. This property may appear in Admin API and `rpk topic describe` outputs for compatibility.\n\nDefault for the Kafka-compatible compression.type property. Redpanda does not recompress data.\n\nThe topic property xref:./topic-properties.adoc#compressiontype[`compression.type`] overrides the value of `log_compression_type` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#compressiontype[`compression.type`]"
      ],
      "config_scope": "cluster"
    },
    "log_message_timestamp_type": {
      "description": "Default timestamp type for topic messages (CreateTime or LogAppendTime).\n\nThe topic property xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`] overrides the value of `log_message_timestamp_type` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`]"
      ],
      "config_scope": "cluster"
    },
    "log_retention_ms": {
      "related_topics": [
        "xref:./topic-properties.adoc#retentionms[`retention.ms`]"
      ],
      "config_scope": "cluster"
    },
    "log_segment_ms": {
      "related_topics": [
        "xref:./topic-properties.adoc#segmentms[`segment.ms`]"
      ],
      "config_scope": "cluster"
    },
    "max.compaction.lag.ms": {
      "description": "The maximum amount of time (in ms) that a log segment can remain unaltered before it is eligible for compaction in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]",
        "xref:./cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#configuration-options[Configure maximum compaction lag]"
      ],
      "config_scope": "topic"
    },
    "max.message.bytes": {
      "description": "The maximum size of a message or batch of a topic. If a compression type is enabled, `max.message.bytes` sets the maximum size of the compressed message or batch.\n\nIf `max.message.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]",
        "xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]",
        "xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]"
      ],
      "config_scope": "topic"
    },
    "max_compaction_lag_ms": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#max.compaction.lag.ms[`max.compaction.lag.ms`]"
      ],
      "config_scope": "cluster"
    },
    "max_transactions_per_coordinator": {
      "description": "Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).\n\nFor details, see xref:develop:transactions#transaction-usage-tips[Transaction usage tips].",
      "related_topics": [
        "xref:develop:transactions#transaction-usage-tips[Transaction usage tips]"
      ],
      "config_scope": "cluster"
    },
    "message.timestamp.type": {
      "description": "The source of a message's timestamp: either the message's creation time or its log append time.\n\nWhen `message.timestamp.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]",
        "xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]"
      ],
      "config_scope": "topic"
    },
    "min.cleanable.dirty.ratio": {
      "description": "The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#min_cleanable_dirty_ratio[`min_cleanable_dirty_ratio`]"
      ],
      "config_scope": "topic"
    },
    "min.compaction.lag.ms": {
      "description": "The minimum amount of time (in ms) that a log segment must remain unaltered before it can be compacted in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`] for the topic.",
      "related_topics": [
        "xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]",
        "xref:./cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]",
        "xref:manage:cluster-maintenance/compaction-settings.adoc#configure-min-compaction-lag[Configure minimum compaction lag]"
      ],
      "config_scope": "topic"
    },
    "min_compaction_lag_ms": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#min.compaction.lag.ms[`min.compaction.lag.ms`]"
      ],
      "config_scope": "cluster"
    },
    "node_id_overrides": {
      "description": "List of node ID and UUID overrides applied at broker startup. Each entry includes the current UUID, the desired new ID and UUID, and an ignore flag. An entry applies only if `current_uuid` matches the broker's actual UUID.\n\nRemove this property after the cluster restarts successfully and operates normally. This prevents reapplication and maintains consistent configuration across brokers.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  node_id_overrides:",
        "    - current_uuid: \"<current-broker-uuid>\"",
        "      new_id: <new-broker-id>",
        "      new_uuid: \"<new-broker-uuid>\"",
        "      ignore_existing_node_id: <ignore-existing-flag>",
        "    - current_uuid: \"<another-current-uuid>\"",
        "      new_id: <another-new-broker-id>",
        "      new_uuid: \"<another-new-uuid>\"",
        "      ignore_existing_node_id: <another-ignore-flag>",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "oidc_discovery_url": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nThe URL pointing to the well-known discovery endpoint for the OIDC provider.",
      "config_scope": "cluster"
    },
    "oidc_principal_mapping": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nRule for mapping JWT payload claim to a Redpanda user principal.",
      "related_topics": [
        "xref:manage:security/authentication.adoc#oidc[OpenID Connect authentication]",
        "xref:manage:kubernetes/security/authentication/k-authentication.adoc[OpenID Connect authentication in Kubernetes]"
      ],
      "config_scope": "cluster"
    },
    "oidc_token_audience": {
      "description": "ifdef::env-cloud[]\nNOTE: This property is read-only in Redpanda Cloud.\nendif::[]\n\nA string representing the intended recipient of the token.",
      "config_scope": "cluster"
    },
    "partition_autobalancing_max_disk_usage_percent": {
      "related_topics": [
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "config_scope": "cluster"
    },
    "partition_autobalancing_mode": {
      "related_topics": [
        "xref:manage:cluster-maintenance/cluster-balancing.adoc[partition balancing]",
        "xref:get-started:licensing/index.adoc[enterprise license]",
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "description": "Mode of partition balancing for a cluster. * `node_add`: partition balancing happens when a node is added. * `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an Enterprise license, and it is customized by `partition_autobalancing_node_availability_timeout_sec` and `partition_autobalancing_max_disk_usage_percent` properties. * `off`: partition balancing is disabled. This option is not recommended for production clusters.",
      "config_scope": "cluster"
    },
    "partition_autobalancing_node_availability_timeout_sec": {
      "related_topics": [
        "xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
      ],
      "config_scope": "cluster"
    },
    "pp_sr_smp_max_non_local_requests": {
      "description": "Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the `seastar::smp_service_group` documentation).\n\nSee https://docs.seastar.io/master/[Seastar documentation^]",
      "config_scope": "cluster"
    },
    "rack": {
      "related_topics": [
        "xref:./cluster-properties.adoc#enable_rack_awareness[enable_rack_awareness]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "raft_recovery_throttle_disable_dynamic_mode": {
      "description": "include::reference:partial$internal-use-property.adoc[]\n\nDisables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems.",
      "config_scope": "cluster"
    },
    "raft_smp_max_non_local_requests": {
      "description": "Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the `seastar::smp_service_group` documentation).\n\nSee https://docs.seastar.io/master/[Seastar documentation^]",
      "config_scope": "cluster"
    },
    "recovery_mode_enabled": {
      "description": "If `true`, start Redpanda in xref:manage:recovery-mode.adoc[recovery mode], where user partitions are not loaded and only administrative operations are allowed.",
      "related_topics": [
        "xref:manage:recovery-mode.adoc[recovery mode]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "redpanda.iceberg.delete": {
      "description": "Whether the corresponding Iceberg table is deleted upon deleting the topic.",
      "config_scope": "topic"
    },
    "redpanda.iceberg.invalid.record.action": {
      "description": "Whether to write invalid records to a dead-letter queue (DLQ).",
      "related_topics": [
        "xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.mode": {
      "description": "Enable the Iceberg integration for the topic. You can choose one of four modes.",
      "related_topics": [
        "xref:manage:iceberg/choose-iceberg-mode.adoc#override-value-schema-latest-default[Choose an Iceberg Mode]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.partition.spec": {
      "description": "The link:https://iceberg.apache.org/docs/nightly/partitioning/[partitioning^] specification for the Iceberg table.",
      "related_topics": [
        "xref:manage:iceberg/about-iceberg-topics.adoc#use-custom-partitioning[Use custom partitioning]"
      ],
      "config_scope": "topic"
    },
    "redpanda.iceberg.target.lag.ms": {
      "description": "Controls how often the data in the Iceberg table is refreshed with new data from the topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.",
      "config_scope": "topic"
    },
    "redpanda.leaders.preference": {
      "description": "The preferred location (rack) for partition leaders of a topic.\n\nThis property inherits the value from the config_ref:default_leaders_preference,true,properties/cluster-properties[] cluster configuration property. You may override the cluster-wide setting by specifying the value for individual topics.\n\nIf the cluster configuration property config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, Leader Pinning is disabled across the cluster.",
      "related_topics": [
        "xref:develop:produce-data/leader-pinning.adoc[Leader pinning]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.allowgaps": {
      "exclude_from_docs": true,
      "config_scope": "topic"
    },
    "redpanda.remote.delete": {
      "description": "A flag that enables deletion of data from object storage for Tiered Storage when it's deleted from local storage for a topic.\n\nNOTE: `redpanda.remote.delete` doesn't apply to Remote Read Replica topics: a Remote Read Replica topic isn't deleted from object storage when this flag is `true`.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.read": {
      "description": "A flag for enabling Redpanda to fetch data for a topic from object storage to local storage. When set to `true` together with <<redpandaremotewrite, `redpanda.remote.write`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.readreplica": {
      "description": "The name of the object storage bucket for a Remote Read Replica topic.\n\nCAUTION: Setting `redpanda.remote.readreplica` together with either `redpanda.remote.read` or `redpanda.remote.write` results in an error.",
      "related_topics": [
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.recovery": {
      "description": "A flag that enables the recovery or reproduction of a topic from object storage for Tiered Storage. The recovered data is saved in local storage, and the maximum amount of recovered data is determined by the local storage retention limits of the topic.\n\nTIP: You can only configure `redpanda.remote.recovery` when you create a topic. You cannot apply this setting to existing topics.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.remote.write": {
      "description": "A flag for enabling Redpanda to upload data for a topic from local storage to object storage. When set to `true` together with <<redpandaremoteread, `redpanda.remote.read`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.",
      "related_topics": [
        "xref:manage:tiered-storage.adoc[Tiered Storage]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "redpanda.virtual.cluster.id": {
      "exclude_from_docs": true,
      "config_scope": "topic"
    },
    "replication.factor": {
      "description": "The number of replicas of a topic to save in different nodes (brokers) of a cluster.\n\nIf `replication.factor` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication] for the topic.\n\nNOTE: Although `replication.factor` isn't returned or displayed by xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] as a valid Kafka property, you can set it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]. When the `replication.factor` of a topic is altered, it isn't simply a property value that's updated, but rather the actual replica sets of topic partitions that are changed.",
      "related_topics": [
        "xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication]",
        "xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`]",
        "xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]",
        "xref:./cluster-properties.adoc#default_topic_replication[`default_topic_replication`]",
        "xref:develop:config-topics.adoc#choose-the-replication-factor[Choose the replication factor]",
        "xref:develop:config-topics.adoc#change-the-replication-factor[Change the replication factor]"
      ],
      "config_scope": "topic"
    },
    "retention.bytes": {
      "description": "A size-based retention limit that configures the maximum size that a topic partition can grow before becoming eligible for cleanup.\n\nIf `retention.bytes` is set to a positive value, it overrides the cluster property xref:cluster-properties.adoc#retention_bytes[`retention_bytes`] for the topic, and the total retained size for the topic is `retention.bytes` multiplied by the number of partitions for the topic.\n\nWhen both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, cleanup occurs when either limit is reached.",
      "related_topics": [
        "xref:cluster-properties.adoc#retention_bytes[`retention_bytes`]",
        "xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "topic"
    },
    "retention.local.target.bytes": {
      "description": "A size-based retention limit for Tiered Storage that configures the maximum size that a topic partition in local storage can grow before becoming eligible for cleanup. It applies per partition and is equivalent to <<retentionbytes, `retention.bytes`>> without Tiered Storage.",
      "related_topics": [
        "xref:./cluster-properties.adoc#retention_local_target_bytes[`retention_local_target_bytes`]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "retention.local.target.ms": {
      "description": "A time-based retention limit for Tiered Storage that sets the maximum duration that a log's segment file for a topic is retained in local storage before it's eligible for cleanup. This property is equivalent to <<retentionms, `retention.ms`>> without Tiered Storage.",
      "related_topics": [
        "xref:./cluster-properties.adoc#retention_local_target_ms[`retention_local_target_ms`]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]",
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic"
    },
    "retention.ms": {
      "description": "A time-based retention limit that configures the maximum duration that a log's segment file for a topic is retained before it becomes eligible to be cleaned up. To consume all data, a consumer of the topic must read from a segment before its `retention.ms` elapses, otherwise the segment may be compacted and/or deleted. If a non-positive value, no per-topic limit is applied.\n\nIf `retention.ms` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`] for the topic.\n\nWhen both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, the earliest occurring limit applies.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]",
        "xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "topic"
    },
    "retention_bytes": {
      "description": "Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied.\n\nThe topic property xref:./topic-properties.adoc#retentionbytes[`retention.bytes`] overrides the value of `retention_bytes` at the topic level.",
      "related_topics": [
        "xref:./topic-properties.adoc#retentionbytes[`retention.bytes`]"
      ],
      "config_scope": "cluster"
    },
    "retention_local_target_bytes_default": {
      "related_topics": [
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "cluster"
    },
    "retention_local_target_capacity_bytes": {
      "description": "The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.",
      "config_scope": "cluster"
    },
    "retention_local_target_capacity_percent": {
      "description": "The target capacity in percent of unreserved space (<<disk_reservation_percent,`disk_reservation_percent`>>) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.",
      "config_scope": "cluster"
    },
    "retention_local_target_ms_default": {
      "related_topics": [
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]"
      ],
      "config_scope": "cluster"
    },
    "rpc_server_tls": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  rpc_server_tls:",
        "    enabled: true",
        "    cert_file: \"<path-to-cert-file>\"",
        "    key_file: \"<path-to-key-file>\"",
        "    truststore_file: \"<path-to-truststore-file>\"",
        "    require_client_auth: true",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "sasl_mechanisms": {
      "description": "A list of supported SASL mechanisms.\n\n*Accepted values:*\n\n* `SCRAM`\n* `GSSAPI`\n* `OAUTHBEARER`\n* `PLAIN`\n\nNote that in order to enable PLAIN, you must also enable SCRAM.",
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "schema_registry_enable_authorization": {
      "description": "Enables ACL-based authorization for Schema Registry requests. When `true`, Schema Registry\nuses ACL-based authorization instead of the default `public/user/superuser` authorization model. \nifdef::env-cloud[]\nRequires authentication to be enabled using the `authentication_method` property in the `schema_registry_api` broker configuration.\nendif::[]",
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "seed_servers": {
      "description": "List of the seed servers used to join current cluster. If the `seed_servers` list is empty the broker will be a cluster root and it will form a new cluster.\n\n* When `empty_seed_starts_cluster` is `true`, Redpanda enables one broker with an empty `seed_servers` list to initiate a new cluster. The broker with an empty `seed_servers` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their `seed_servers` populated with the cluster root's address, facilitating their connection to the cluster.\n+\n[IMPORTANT]\n====\nOnly one broker, the designated cluster root, should have an empty `seed_servers` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation.\n====\n\n* When `empty_seed_starts_cluster` is `false`, Redpanda requires all brokers to start with a known set of brokers listed in `seed_servers`. The `seed_servers` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the `seed_servers` list use it to discover and join the cluster, allowing for expansion beyond the foundational members.\n+\n[NOTE]\n====\nThe `seed_servers` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.\n====",
      "example": [
        ".Example with `empty_seed_starts_cluster: true`\n[,yaml]\n----\n# Cluster root broker (seed starter)\nredpanda:\n  empty_seed_starts_cluster: true\n  seed_servers: []\n----\n\n[,yaml]\n----\n# Additional brokers joining the cluster\nredpanda:\n  empty_seed_starts_cluster: true\n  seed_servers:\n    - host:\n        address: <seed-broker-ip>\n        port: <rpc-port>\n----\n\n.Example with `empty_seed_starts_cluster: false`\n[,yaml]\n----\n# All initial seed brokers use the same configuration\nredpanda:\n  empty_seed_starts_cluster: false\n  seed_servers:\n    - host:\n        address: <seed-broker-1-ip>\n        port: <rpc-port>\n    - host:\n        address: <seed-broker-2-ip>\n        port: <rpc-port>\n    - host:\n        address: <seed-broker-3-ip>\n        port: <rpc-port>\n----\n\nReplace the following placeholders with your values:\n\n* `<seed-broker-ip>`: IP address of the cluster root broker\n* `<seed-broker-x-ip>`: IP addresses of each seed broker in the cluster\n* `<rpc-port>`: RPC port for brokers (default: `33145`)"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "segment.bytes": {
      "description": "The maximum size of an active log segment for a topic. When the size of an active segment exceeds `segment.bytes`, the segment is closed and a new active segment is created. The closed, inactive segment is then eligible to be cleaned up according to retention properties.\n\nWhen `segment.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`] for the topic.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]",
        "xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]",
        "xref:manage:remote-read-replicas.adoc[Remote Read Replicas]"
      ],
      "config_scope": "topic"
    },
    "segment.ms": {
      "description": "The maximum duration that a log segment of a topic is active (open for writes and not deletable). A periodic event, with `segment.ms` as its period, forcibly closes the active segment and transitions, or rolls, to a new active segment. The closed (inactive) segment is then eligible to be cleaned up according to cleanup and retention properties.\n\nIf set to a positive duration, `segment.ms` overrides the cluster property xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]. Values are automatically clamped between the cluster bounds set by xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`] (default: 10 minutes) and xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`] (default: 1 year). If your configured value exceeds these bounds, Redpanda uses the bound value and logs a warning. Check current cluster bounds with `rpk cluster config get log_segment_ms_min log_segment_ms_max`.",
      "related_topics": [
        "xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]",
        "xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`]",
        "xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`]",
        "xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]",
        "xref:manage:cluster-maintenance/disk-utilization.adoc#log-rolling[Log rolling]"
      ],
      "config_scope": "topic"
    },
    "storage_compaction_key_map_memory": {
      "description": "Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.",
      "config_scope": "cluster"
    },
    "storage_compaction_key_map_memory_limit_percent": {
      "description": "Limit on <<storage_compaction_key_map_memory,`storage_compaction_key_map_memory`>>, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. \n\nNOTE: Memory per shard is computed after <<data_transforms_per_core_memory_reservation,`data_transforms_per_core_memory_reservation`>>, and only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.",
      "config_scope": "cluster"
    },
    "storage_strict_data_init": {
      "description": "Requires that an empty file named `.redpanda_data_dir` be present in the xref:reference:properties/broker-properties.adoc#data_directory[`data_ directory`]. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.",
      "related_topics": [
        "xref:reference:properties/broker-properties.adoc#data_directory[`data_ directory`]"
      ],
      "config_scope": "cluster"
    },
    "tombstone_retention_ms": {
      "description": "The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`. A typical default setting is `86400000`, or 24 hours.",
      "related_topics": [
        "xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]"
      ],
      "config_scope": "cluster"
    },
    "topic_partitions_memory_allocation_percent": {
      "description": "Percentage of total memory to reserve for topic partitions. See <<topic_memory_per_partition, `topic_memory_per_partition`>> for details.",
      "config_scope": "cluster"
    },
    "verbose_logging_timeout_sec_max": {
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "schema_registry:",
        "  schema_registry_api:",
        "    address: 0.0.0.0",
        "    port: 8081",
        "    authentication_method: http_basic",
        "  schema_registry_replication_factor: 3",
        "  mode_mutability: true",
        "----"
      ],
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#http_authentication[`http_authentication`]",
        "xref:reference:properties/cluster-properties.adoc#http_authentication[`http_authentication`]"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "write.caching": {
      "description": "The write caching mode to apply to a topic.\n\nWhen `write.caching` is set, it overrides the cluster property xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.",
      "related_topics": [
        "xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]",
        "xref:./cluster-properties.adoc#write_caching_default[`write_caching_default`]",
        "xref:develop:config-topics.adoc#configure-write-caching[Write caching]",
        "xref:manage:tiered-storage.adoc[Tiered Storage]"
      ],
      "config_scope": "topic"
    },
    "confluent.key.schema.validation": {
      "description": "Enable validation of the schema ID for keys on a record. This is a compatibility alias for `redpanda.key.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's key is registered in the Schema Registry according to the configured subject name strategy.",
      "config_scope": "topic"
    },
    "confluent.key.subject.name.strategy": {
      "description": "The subject name strategy for keys when `confluent.key.schema.validation` is enabled. This is a compatibility alias for `redpanda.key.subject.name.strategy` that determines how the topic and schema are mapped to a subject name in the Schema Registry.",
      "config_scope": "topic"
    },
    "confluent.value.schema.validation": {
      "description": "Enable validation of the schema ID for values on a record. This is a compatibility alias for `redpanda.value.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's value is registered in the Schema Registry according to the configured subject name strategy.",
      "config_scope": "topic"
    },
    "confluent.value.subject.name.strategy": {
      "description": "The subject name strategy for values when `confluent.value.schema.validation` is enabled. This is a compatibility alias for `redpanda.value.subject.name.strategy`. This determines how the topic and schema are mapped to a subject name in the Schema Registry.",
      "config_scope": "topic"
    },
    "write_caching_default": {
      "related_topics": [
        "xref:reference:properties/topic-properties.adoc#writecaching[`write.caching`]",
        "xref:develop:config-topics.adoc#configure-write-caching[Write caching]"
      ],
      "config_scope": "cluster"
    },
    "advertised_kafka_api": {
      "description": "Address of the Kafka API published to the clients. If not set, the <<kafka_api, `kafka_api`>> broker property is used. When behind a load balancer or in containerized environments, this should be the externally-accessible address that clients use to connect.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  advertised_kafka_api:",
        "    - name: <kafka-api-name>",
        "      address: <external-broker-hostname>",
        "      port: <kafka-port>",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "advertised_rpc_api": {
      "description": "Address of RPC endpoint published to other cluster members. If not set, the <<rpc_server, `rpc_server`>> broker property is used. This should be the address other brokers can use to communicate with this broker.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  advertised_rpc_api:",
        "    address: <external-broker-hostname>",
        "    port: <rpc-port>",
        "----"
      ],
      "config_scope": "broker",
      "category": "redpanda"
    },
    "api_doc_dir": {
      "description": "Path to the API specifications directory. This directory contains API documentation for both the HTTP Proxy API and Schema Registry API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "audit_enabled": {
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "client_keep_alive": {
      "description": "Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "cloud_storage_access_key": {
      "description": "AWS or GCP access key. This access key is part of the credentials that Redpanda requires to authenticate with object storage services for Tiered Storage. This access key is used with the <<cloud_storage_secret_key,`cloud_storage_secret_key`>> to form the complete credentials required for authentication.\nTo authenticate using IAM roles, see <<cloud_storage_credentials_source,`cloud_storage_credentials_source`>>.",
      "config_scope": "object-storage"
    },
    "cloud_storage_client_lease_timeout_ms": {
      "description": "Maximum time to hold a cloud storage client lease (ms), after which any outstanding connection is immediately closed.",
      "config_scope": "cluster"
    },
    "cloud_storage_disable_archiver_manager": {
      "description": "Use legacy upload mode and do not start archiver_manager.",
      "config_scope": "cluster"
    },
    "cloud_storage_inventory_hash_path_directory": {
      "description": "Directory to store inventory report hashes for use by cloud storage scrubber. If not specified, Redpanda uses a default path within the data directory.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "redpanda:",
        "  cloud_storage_inventory_hash_store: <inventory-hash-directory-path>",
        "----"
      ],
      "config_scope": "object-storage"
    },
    "consumer_heartbeat_interval_ms": {
      "description": "Interval (in milliseconds) for consumer heartbeats.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_instance_timeout_ms": {
      "description": "How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "consumer_offsets_topic_batch_cache_enabled": {
      "description": "This property lets you enable the batch cache for the consumer offsets topic. By default, the cache for consumer offsets topic is disabled. Changing this property is not recommended in production systems, as it may affect performance. The change is applied only after the restart.",
      "config_scope": "cluster"
    },
    "consumer_rebalance_timeout_ms": {
      "description": "Timeout (in milliseconds) for consumer rebalance.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_max_bytes": {
      "description": "Maximum bytes to fetch per request.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_min_bytes": {
      "description": "Minimum bytes to fetch per request.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_request_timeout_ms": {
      "description": "Interval (in milliseconds) for consumer request timeout.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "consumer_session_timeout_ms": {
      "description": "Timeout (in milliseconds) for consumer session.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "development_enable_cluster_link": {
      "description": "Enable cluster linking.",
      "config_scope": "cluster"
    },
    "development_feature_property_testing_only": {
      "description": "Development feature property for testing only.",
      "config_scope": "cluster"
    },
    "disable_cluster_recovery_loop_for_tests": {
      "description": "Disables the cluster recovery loop. This property is used to simplify testing and should not be set in production.",
      "config_scope": "cluster"
    },
    "enable_developmental_unrecoverable_data_corrupting_features": {
      "description": "Development features should never be enabled in a production cluster, or any cluster where stability, data loss, or the ability to upgrade are a concern. To enable experimental features, set the value of this configuration option to the current unix epoch expressed in seconds. The value must be within one hour of the current time on the broker.Once experimental features are enabled they cannot be disabled",
      "config_scope": "cluster"
    },
    "iceberg_delete": {
      "description": "Default value for the redpanda.iceberg.delete topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.",
      "config_scope": "cluster"
    },
    "iceberg_disable_snapshot_tagging": {
      "description": "Whether to disable tagging of Iceberg snapshots. These tags are used to ensure that the snapshots that Redpanda writes are retained during snapshot removal, which in turn, helps Redpanda ensure exactly once delivery of records. Disabling tags is therefore not recommended, but may be useful if the Iceberg catalog does not support tags.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_authentication_mode": {
      "description": "The authentication mode for client requests made to the Iceberg catalog. Choose from: `none`, `bearer`, `oauth2`, and `aws_sigv4`. In `bearer` mode, the token specified in `iceberg_rest_catalog_token` is used unconditonally, and no attempts are made to refresh the token. In `oauth2` mode, the credentials specified in `iceberg_rest_catalog_client_id` and `iceberg_rest_catalog_client_secret` are used to obtain a bearer token from the URI defined by `iceberg_rest_catalog_oauth2_server_uri`. In `aws_sigv4` mode, the same AWS credentials used for cloud storage (see `cloud_storage_region`, `cloud_storage_access_key`, `cloud_storage_secret_key`, and `cloud_storage_credentials_source`) are used to sign requests to AWS Glue catalog with SigV4.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_client_id": {
      "description": "Iceberg REST catalog user ID. This ID is used to query the catalog API for the OAuth token. Required if catalog type is set to `rest` and `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_credentials_source": {
      "description": "ifndef::env-cloud[]\nSource of AWS credentials for Iceberg REST catalog SigV4 authentication. If not set, falls back to xref:reference:properties/object-storage-properties.adoc#cloud_storage_credentials_source[`cloud_storage_credentials_source`] when using aws_sigv4 authentication mode.\nendif::[]\n\nifdef::env-cloud[]\nSource of AWS credentials for Iceberg REST catalog SigV4 authentication. If providing explicit credentials using `iceberg_rest_catalog_aws_access_key` and `iceberg_rest_catalog_aws_secret_key` for Glue catalog authentication, you must set this property to `config_file`.\nendif::[]\n\n*Accepted values*: `aws_instance_metadata`, `azure_aks_oidc_federation`, `azure_vm_instance_metadata`, `config_file`, `gcp_instance_metadata`, `sts`.",
      "related_topics": [
        "xref:reference:properties/object-storage-properties.adoc#cloud_storage_credentials_source[`cloud_storage_credentials_source`]"
      ],
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_crl": {
      "description": "The contents of a certificate revocation list for `iceberg_rest_catalog_trust`. Takes precedence over `iceberg_rest_catalog_crl_file`.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_oauth2_scope": {
      "description": "The OAuth scope used to retrieve access tokens for Iceberg catalog authentication. Only meaningful when `iceberg_rest_catalog_authentication_mode` is set to `oauth2`",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_oauth2_server_uri": {
      "description": "The OAuth URI used to retrieve access tokens for Iceberg catalog authentication. If left undefined, the deprecated Iceberg catalog endpoint `/v1/oauth/tokens` is used instead.",
      "config_scope": "cluster"
    },
    "iceberg_rest_catalog_request_timeout_ms": {
      "description": "Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request",
      "config_scope": "cluster"
    },
    "iceberg_topic_name_dot_replacement": {
      "description": "Optional replacement string for dots in topic names when deriving Iceberg table names, useful when downstream systems do not permit dots in table names. The replacement string cannot contain dots. Be careful to avoid table name collisions caused by the replacement.If an Iceberg topic with dots in the name exists in the cluster, the value of this property should not be changed.",
      "config_scope": "cluster"
    },
    "kafka_enable_authorization": {
      "description": "Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by enable_sasl. * `null`: Ignored. Authorization is enabled with `enable_sasl`: `true` * `true`: authorization is required. * `false`: authorization is disabled.",
      "config_scope": "cluster"
    },
    "kafka_produce_batch_validation": {
      "description": "Controls the level of validation performed on batches produced to Redpanda. When set to `legacy`, there is minimal validation performed on the produce path. When set to `relaxed`, full validation is performed on uncompressed batches and on compressed batches with the `max_timestamp` value left unset. When set to `strict`, full validation of uncompressed and compressed batches is performed. This should be the default in environments where producing clients are not trusted.",
      "config_scope": "cluster"
    },
    "mode_mutability": {
      "description": "Enable modifications to the read-only `mode` of the Schema Registry. When set to `true`, the entire Schema Registry or its subjects can be switched to `READONLY` or `READWRITE`. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.",
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "pandaproxy_api": {
      "description": "Rest API listener address and port.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "pandaproxy:",
        "  pandaproxy_api:",
        "    address: 0.0.0.0",
        "    port: 8082",
        "    authentication_method: http_basic",
        "----"
      ],
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "pandaproxy_api_tls": {
      "description": "TLS configuration for Pandaproxy API.",
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "produce_batch_delay_ms": {
      "description": "Delay (in milliseconds) to wait before sending batch.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_batch_size_bytes": {
      "description": "Number of bytes to batch before sending to broker.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_shutdown_delay_ms": {
      "description": "Delay (in milliseconds) to allow for final flush of buffers before shutting down.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "retry_base_backoff_ms": {
      "description": "Delay (in milliseconds) for initial retry backoff.",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "sasl_mechanism": {
      "description": "The SASL mechanism to use when the HTTP Proxy client connects to the Kafka API. These credentials are used when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\nThis property specifies which individual SASL mechanism the HTTP Proxy client should use, while the cluster-wide available mechanisms are configured using the xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`] cluster property.\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]",
      "related_topics": [
        "xref:reference:properties/cluster-properties.adoc#sasl_mechanisms[`sasl_mechanisms`]"
      ],
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "sasl_mechanisms_overrides": {
      "description": "A list of overrides for SASL mechanisms, defined by listener. SASL mechanisms defined here will replace the ones set in `sasl_mechanisms`. The same limitations apply as for `sasl_mechanisms`.",
      "related_topics": [
        "xref:get-started:licensing/index.adoc[Redpanda Licensing]"
      ],
      "config_scope": "cluster"
    },
    "schema_registry_api": {
      "description": "Schema Registry API listener address and port.",
      "example": [
        ".Example",
        "[,yaml]",
        "----",
        "schema_registry:",
        "  schema_registry_api:",
        "    address: 0.0.0.0",
        "    port: 8081",
        "    authentication_method: http_basic",
        "----"
      ],
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "schema_registry_replication_factor": {
      "description": "Replication factor for internal `_schemas` topic. If unset, defaults to the xref:../cluster-properties.adoc#default_topic_replication[`default_topic_replication`] cluster property.",
      "related_topics": [
        "xref:../cluster-properties.adoc#default_topic_replication[`default_topic_replication`]"
      ],
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "scram_password": {
      "description": "Password to use for SCRAM authentication mechanisms when the HTTP Proxy client connects to the Kafka API. This property is required when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "scram_username": {
      "description": "Username to use for SCRAM authentication mechanisms when the HTTP Proxy client connects to the Kafka API. This property is required when the HTTP Proxy API listener has <<http_proxy_auth_method, `authentication_method`>>: `none` but the cluster requires authenticated access to the Kafka API.\ninclude::shared:partial$http-proxy-ephemeral-credentials-breaking-change.adoc[]",
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "transaction_coordinator_cleanup_policy": {
      "description": "Cleanup policy for a transaction coordinator topic.\n\n*Accepted values:*\n\n* `compact`\n* `delete`\n* `[\"compact\",\"delete\"]`\n* `none`",
      "config_scope": "cluster"
    },
    "admin_api_doc_dir": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "crash_loop_limit": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "data_directory": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "fips_mode": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "memory_allocation_warning_threshold": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "node_id": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "openssl_config_file": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "openssl_module_directory": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "rpc_server": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "storage_failure_injection_config_path": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "storage_failure_injection_enabled": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "upgrade_override_checks": {
      "config_scope": "broker",
      "category": "redpanda"
    },
    "schema_registry_api_tls": {
      "config_scope": "broker",
      "category": "schema-registry"
    },
    "advertised_pandaproxy_api": {
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "client_cache_max_size": {
      "config_scope": "broker",
      "category": "pandaproxy"
    },
    "broker_tls": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "brokers": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "client_identifier": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_ack_level": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_batch_record_count": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "produce_compression_type": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "retries": {
      "config_scope": "broker",
      "category": "pandaproxy-client"
    },
    "redpanda.cloud_topic.enabled": {
      "config_scope": "topic",
      "category": "tiered-storage"
    }
  }
}