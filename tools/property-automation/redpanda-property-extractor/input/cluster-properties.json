{
    "properties": {
        "admin_api_require_auth": {
            "description": "Whether admin API clients must provide HTTP basic authentication headers."
        },
        "superusers": {
            "description": "List of superuser usernames."
        },
        "audit_enabled": {
            "description": "Enables or disables audit logging. When you set this to true, Redpanda checks for an existing topic named `_redpanda.audit_log`. If none is found, Redpanda automatically creates one for you."
        },
        "audit_enabled_event_types": {
            "description": "List of strings in JSON style identifying the event types to include in the audit log. This may include any of the following - `management, produce, consume, describe, heartbeat, authenticate, schema_registry, admin`."
        },
        "audit_exclude_principals": {
            "description": "List of strings in JSON style identifying the principals the audit logging system should ignore. Principals can be listed as `User:name` or `name`, both are accepted."
        },
        "audit_exclude_topics": {
            "description": "List of strings in JSON style identifying the topics the audit logging system should ignore. This list cannot include the `_redpanda.audit_log` topic. Redpanda will reject the command if you do attempt to include that topic."
        },
        "audit_client_max_buffer_size": {
            "description": "Defines the number of bytes allocated by the internal audit client for audit messages. When changing this, you must disable audit logging and then re-enable it for the change to take effect. Consider increasing this if your system generates a very large number of audit records in a short amount of time."
        },
        "audit_log_num_partitions": {
            "description": "Defines the number of partitions used by a newly created audit topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics."
        },
        "audit_log_replication_factor": {
            "description": "Defines the replication factor for a newly created audit log topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics. Setting this value is optional. If a value is not provided, Redpanda will use the `internal_topic_replication_factor` cluster config value."
        },
        "audit_queue_drain_interval_ms": {
            "description": "Internally, Redpanda batches audit log messages in memory and periodically writes them to the audit log topic. This defines the period in milliseconds between draining this queue to the audit log topic. Longer intervals may help prevent duplicate messages, especially in high throughput scenarios, but they also increase the risk of data loss during hard shutdowns where the queue is lost."
        },
        "audit_queue_max_buffer_size_per_shard": {
            "description": "Defines the maximum amount of memory in bytes used by the audit buffer in each shard. Once this size is reached, requests to log additional audit messages will return a non-retryable error."
        },
        "cloud_storage_azure_storage_account": {
            "description": "The name of the Azure storage account to use with Tiered Storage. If `null`, the property is disabled.\n\n*Type*: string\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "cloud_storage_azure_container": {
            "description": "The name of the Azure container to use with Tiered Storage. If `null`, the property is disabled.\n\nNOTE: The container must belong to <<cloud_storage_azure_storage_account,cloud_storage_azure_storage_account>>.\n\n*Type*: string\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "cloud_storage_azure_shared_key": {
            "description": "The shared key to be used for Azure Shared Key authentication with the Azure storage account configured by <<cloud_storage_azure_storage_account,cloud_storage_azure_storage_account>>.  If `null`, the property is disabled.\n\nNOTE: Redpanda expects this key string to be Base64 encoded.\n\n*Type*: string\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "cloud_storage_access_key": {
            "description": "AWS or GCP access key."
        },
        "cloud_storage_api_endpoint": {
            "description": "Optional API endpoint.\n\n- AWS: when left blank, it's automatically generated using <<cloud_storage_region,region>> property, and <<cloud_storage_bucket,bucket>> property. Otherwise uses the value assigned.\n- GCP: use `storage.googleapis.com`."
        },
        "cloud_storage_api_endpoint_port": {
            "description": "TLS port override."
        },
        "cloud_storage_bucket": {
            "description": "AWS or GCS bucket that should be used to store data."
        },
        "cloud_storage_cache_size": {
            "description": "Max size of object storage cache.\n\n*Units*: bytes"
        },
        "cloud_storage_cluster_metadata_upload_interval_ms": {
            "description": "Time interval to wait between cluster metadata uploads.\n\n*Units*: milliseconds"
        },
        "cloud_storage_credentials_source": {
            "description": "The source of credentials used to connect to cloud services.\n\n\n*Valid values*: config_file, aws_instance_metadata, sts, gcp_instance_metadata"
        },
        "cloud_storage_disable_tls": {
            "description": "Disable TLS for all S3 or GCS connections.\n\n*Type*: boolean"
        },
        "cloud_storage_enabled": {
            "description": "Enable object storage. Must be set to `true` to use xref:manage:tiered-storage.adoc[Tiered Storage] or Remote Read Replicas.\n\n*Type*: boolean"
        },
        "cloud_storage_max_connections": {
            "description": "Max number of simultaneous connections to S3 per shard. Includes connections used for both uploads and downloads.\n\n*Units*: number of simultaneous connections"
        },
        "cloud_storage_region": {
            "description": "AWS or GCP region that houses the bucket used for storage.\n\n*Type*: string"
        },
        "cloud_storage_secret_key": {
            "description": "AWS or GCP secret key.\n\n*Type*: string"
        },
        "cloud_storage_trust_file": {
            "description": "Path to certificate that should be used to validate server certificate during TLS handshake.\n\n*Type*: string"
        },
        "cluster_id": {
            "description": "Cluster identifier.\n\n*Type*: string"
        },
        "controller_snapshot_max_age_sec": {
            "description": "Max time that will pass before Redpanda attempts to create a controller snapshot after a new controller command appears.\n\n*Units*: seconds"
        },
        "enable_auto_rebalance_on_node_add\n{badge-deprecated}": {
            "description": "Enable automatic partition rebalancing when new nodes are added.\n\n*Type*: boolean"
        },
        "enable_cluster_metadata_upload_loop": {
            "description": "Enables cluster metadata uploads. Required for xref:manage:whole-cluster-restore.adoc[whole cluster restore].\n\n*Type*: boolean"
        },
        "enable_controller_log_rate_limiting": {
            "description": "Flag to enable limiting the write rate for the controller log.\n\n*Type*: boolean"
        },
        "enable_leader_balancer": {
            "description": "Enable automatic leadership rebalancing. Mode is set by <<leader_balancer_mode,`leader_balancer_mode`>>.\n\n*Type*: boolean"
        },
        "enable_rack_awareness": {
            "description": "Enable rack-aware replica assignment.\n\n*Type*: boolean"
        },
        "leader_balancer_mode": {
            "description": "Mode of the leader balancer for optimizing movements of leadership between shards (logical CPU cores). Enabled by <<enable_leader_balancer,`enable_leader_balancer`>>.\n\nValid modes:\n\n* `random_hill_climbing`: a shard is randomly chosen and leadership is moved to it if the load on the original shard is reduced.\n* `greedy_balanced_shards`: leadership movement is based on a greedy heuristic of moving leaders from the most loaded shard to the least loaded shard.\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "partition_autobalancing_mode": {
            "description": "Mode of xref:manage:cluster-maintenance/cluster-balancing.adoc[partition balancing] for a cluster.\n\nAvailable modes:\n\n* `node_add`: partition balancing happens when a node is added.\n* `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an xref:get-started:licenses.adoc[Enterprise license], and it is customized by <<partition_autobalancing_node_availability_timeout_sec,partition_autobalancing_node_availability_timeout_sec>> and <<partition_autobalancing_max_disk_usage_percent,partition_autobalancing_max_disk_usage_percent>> properties.\n* `off`: partition balancing is disabled. This option is not recommended for production clusters.\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
        },
        "partition_autobalancing_node_availability_timeout_sec": {
            "description": "NOTE: This property applies only when <<partition_autobalancing_mode,partition_autobalancing_mode>> is set to `continuous`.\n\nWhen a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node.\n\n*Units*: seconds\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
        },
        "partition_autobalancing_max_disk_usage_percent": {
            "description": "NOTE: This property applies only when <<partition_autobalancing_mode,partition_autobalancing_mode>> is set to `continuous`.\n\nWhen the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node.\n\n*Units*: percent of disk used\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]"
        },
        "kafka_admin_topic_api_rate": {
            "description": "Target quota rate for partition mutations per xref:./tunable-properties.adoc#default_window_sec[`default_window_sec`]. If `null`, the property is disabled, and no quota rate is applied.\n\n*Units*: partition mutations per default_window_second\n\n\n*Related properties*:\n\n* xref:./tunable-properties.adoc#default_window_sec[`default_window_sec`]"
        },
        "kafka_client_group_byte_rate_quota": {
            "description": "A map specifying the produce-rate quota per client group.\n\nThe configurable fields:\n\n* `group_name`: name of a client group\n* `clients_prefix`: prefix to prepend to the name of each client belonging to the group specified by `group_name`\n* `quota`: produce-rate quota of each client in bytes per second\n\nAn example: `([{'group_name': 'first_group','clients_prefix': 'group_1','quota': 10240}])`\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#client-group-throughput-limits[Client group throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_client_group_fetch_byte_rate_quota": {
            "description": "A map specifying the fetch-rate quota per client group.\n\nThe configurable fields:\n\n* `group_name`: name of a client group\n* `clients_prefix`: prefix to prepend to the name of each client belonging to the group specified by `group_name`\n* `quota`: fetch-rate quota of each client in bytes per second\n\nAn example: `([{'group_name': 'first_group','clients_prefix': 'group_1','quota': 10240}])`\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#client-group-throughput-limits[Client group throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "enable_idempotence": {
            "description": "Enable idempotent producers.\n\n*Type*: boolean"
        },
        "enable_sasl": {
            "description": "Enable SASL authentication for Kafka connections.\n\n*Type*: boolean"
        },
        "enable_schema_id_validation": {
            "description": "Mode to enable server-side schema ID validation.\n\n*Valid values*:\n\n* `none`: schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified.\n* `redpanda`: schema validation is enabled. Only Redpanda topic properties are accepted.\n* `compat`: schema validation is enabled. Both Redpanda and compatible topic properties are accepted.\n\n\n\n*Related topics*:\n\n* xref:manage:schema-id-validation.adoc[Server-Side Schema ID Validation]"
        },
        "fetch_max_bytes": {
            "description": "Maximum number of bytes returned in a fetch request.\n\n*Units*: bytes"
        },
        "group_max_session_timeout_ms": {
            "description": "The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.\n\n*Units*: milliseconds"
        },
        "group_min_session_timeout_ms": {
            "description": "The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating which can overwhelm broker resources.\n\n*Units*: milliseconds"
        },
        "kafka_connection_rate_limit": {
            "description": "Maximum connections per second for one core. If `null` (the default), the number of connections per second is unlimited.\n\n*Units*: number of connections per second, per core\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
        },
        "kafka_connection_rate_limit_overrides": {
            "description": "Overrides the maximum connections per second for one core for the specified IP addresses (for example, `['127.0.0.1:90', '50.20.1.1:40']`)\n\n*Type*: string\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
        },
        "kafka_connections_max": {
            "description": "Maximum number of Kafka client connections per broker. If `null`, the property is disabled.\n\n*Units*: number of Kafka client connections per broker\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
        },
        "kafka_connections_max_overrides": {
            "description": "A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
        },
        "kafka_connections_max_per_ip": {
            "description": "Maximum number of Kafka client connections per IP address, per broker. If `null`, the property is disabled.\n\n*Units*: number of Kafka client connections per IP address, per broker\n\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]"
        },
        "kafka_enable_authorization": {
            "description": "Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by <<enable_sasl,enable_sasl>>.\n\nValid values:\n\n* `null`: Ignored. Authorization is enabled with <<enable_sasl,`enable_sasl`>>: `true`\n* `true`: authorization is required.\n* `false`: authorization is disabled.\n\n*Type*: boolean\n\n\n*Related properties*:\n\n* <<enable_sasl,enable_sasl>>\n* `kafka_api[].authentication_method`"
        },
        "kafka_enable_partition_reassignment": {
            "description": "Enable the Kafka partition reassignment API.\n\n*Type*: boolean\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_group_recovery_timeout_ms": {
            "description": "Kafka group recovery timeout.\n\n*Units*: milliseconds"
        },
        "kafka_mtls_principal_mapping_rules": {
            "description": "Principal mapping rules for mTLS authentication on the Kafka API. If `null`, the property is disabled."
        },
        "kafka_nodelete_topics": {
            "description": "A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.\n\n\n\n*Related topics*:\n\n* xref:develop:consume-data/consumer-offsets.adoc[Consumer Offsets]\n* xref:manage:schema-registry.adoc[Schema Registry]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_noproduce_topics": {
            "description": "A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_qdc_enable": {
            "description": "Enable Kafka queue depth control.\n\n*Type*: boolean"
        },
        "kafka_qdc_max_latency_ms": {
            "description": "Maximum latency threshold for Kafka queue depth control depth tracking.\n\n*Units*: milliseconds"
        },
        "kafka_quota_balancer_node_period_ms": {
            "description": "The period at which the intra-node throughput quota balancer runs.\n\nIt may take longer for the balancer to complete a single balancing step than the period this property specifies, so the actual period may be more than configured here.\n\nIf `0`, the balancer is disabled and all throughput quotas are immutable.\n\n*Units*: milliseconds\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_quota_balancer_min_shard_throughput_ratio": {
            "description": "The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. While the value applies equally to ingress and egress traffic, the default shard quota can be different for ingress and egress and therefore result in different minimum throughput bytes-per-second (bps) values.\n\nBoth `kafka_quota_balancer_min_shard_throughput_ratio` and <<kafka_quota_balancer_min_shard_throughput_bps,kafka_quota_balancer_min_shard_throughput_bps>> can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties.\n\nIf set to `0.0`, the minimum is disabled. If set to `1.0`, then the balancer won't be able to rebalance quota without violating this ratio, consequently precluding the balancer from adjusting shards' quotas.\n\n*Type*: double\n\n*Units*: ratio of default shard quota\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_quota_balancer_min_shard_throughput_bps": {
            "description": "The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed in bytes per second. The value applies equally to ingress and egress traffic.\n\nkafka_quota_balancer_min_shard_throughput_bps doesn't override the limit settings, <<kafka_throughput_limit_node_in_bps,kafka_throughput_limit_node_in_bps>> and <<kafka_throughput_limit_node_out_bps,kafka_throughput_limit_node_out_bps>>. Consequently, the value of\n`kafka_throughput_limit_node_in_bps` or `kafka_throughput_limit_node_out_bps` can result in lesser throughput than kafka_quota_balancer_min_shard_throughput_bps.\n\nBoth <<kafka_quota_balancer_min_shard_throughput_ratio,kafka_quota_balancer_min_shard_throughput_ratio>> and kafka_quota_balancer_min_shard_throughput_bps can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties.\n\nIf set to `0`, no minimum is enforced.\n\n*Units*: bytes per second\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_quota_balancer_window_ms": {
            "description": "Time window used to average the current throughput measurement for the quota balancer.\n\n*Units*: milliseconds\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_rpc_server_tcp_recv_buf": {
            "description": "Size of the Kafka server TCP receive buffer. If `null`, the property is disabled.\n\n*Units*: bytes\n\n\n"
        },
        "kafka_rpc_server_tcp_send_buf": {
            "description": "Size of the Kafka server TCP transmit buffer. If `null`, the property is disabled.\n\n*Units*: bytes\n\n\n"
        },
        "kafka_throughput_limit_node_in_bps": {
            "description": "The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.).\n\nIf `null`, the property is disabled, and traffic is not limited.\n\n*Units*: bytes per second\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "kafka_throughput_limit_node_out_bps": {
            "description": "The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.).\n\nIf `null`, the property is disabled, and traffic is not limited.\n\n*Units*: bytes per second\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "log_segment_ms": {
            "description": "Default lifetime of log segments. If `null`, the property is disabled, and no default lifetime is set. Any value under 60 seconds (60000 ms) is rejected. This property can also be set in the Kafka API using the Kafka-compatible alias, `log.roll.ms`.\n\nThe topic property xref:./topic-properties.adoc#segmentms[`segment.ms`] overrides the value of `log_segment_ms` at the topic level.\n\n*Units*: milliseconds\n\n\n*Related properties*:\n\n* xref:./tunable-properties.adoc#log_segment_ms_min[log_segment_ms_min]\n* xref:./tunable-properties.adoc#log_segment_ms_max[log_segment_ms_max]\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "oidc_clock_skew_tolerance": {
            "description": "The amount of time (in seconds) to allow for when validating the expiry claim in the token.\n\n\n*Units*: seconds"
        },
        "oidc_discovery_url": {
            "description": "The URL pointing to the well-known discovery endpoint for the OIDC provider."
        },
        "oidc_token_audience": {
            "description": "A string representing the intended recipient of the token.\n\n*Type*: string"
        },
        "rm_sync_timeout_ms": {
            "description": "Resource manager's synchronization timeout. Maximum time for this node to wait for internal state machine to catch up with all events written by previous leaders before rejecting a request.\n\n*Units*: milliseconds"
        },
        "rpc_server_listen_backlog": {
            "description": "Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.\n\n*Units*: number of queue entries\n\n\n"
        },
        "rpc_server_tcp_recv_buf": {
            "description": "Internal RPC TCP receive buffer size. If `null` (the default value), no buffer size is set by Redpanda.\n\n*Units*: bytes\n\n\n"
        },
        "rpc_server_tcp_send_buf": {
            "description": "Internal RPC TCP send buffer size. If `null` (the default value), no buffer size is set by Redpanda.\n\n*Units*: bytes\n\n\n"
        },
        "sasl_kerberos_config": {
            "description": "The location of the Kerberos `krb5.conf` file for Redpanda.\n\n*Type*: string"
        },
        "sasl_kerberos_keytab": {
            "description": "The location of the Kerberos keytab file for Redpanda.\n\n*Type*: string"
        },
        "sasl_kerberos_principal": {
            "description": "The primary of the Kerberos Service Principal Name (SPN) for Redpanda.\n\n*Type*: string"
        },
        "sasl_kerberos_principal_mapping": {
            "description": "Rules for mapping Kerberos principal names to Redpanda user principals.\n\n*Type*: array of string"
        },
        "sasl_mechanisms": {
            "description": "A list of supported SASL mechanisms. `SCRAM` and `GSSAPI` are allowed.\n\n*Type*: array of string\n\n\n*Valid values*: `\"SCRAM\"`, `\"GSSAPI\"`"
        },
        "target_quota_byte_rate": {
            "description": "Target quota byte rate.\n\nThe `target_quota_byte_rate` property applies to a producer client that isn't a member of a client group configured by <<kafka_client_group_byte_rate_quota,`kafka_client_group_byte_rate_quota`>>. It sets the maximum throughput quota of a client sending to a Redpanda broker node.\n\n*Units*: bytes per second\n\n\n*Related topics*:\n\n* xref:manage:cluster-maintenance/manage-throughput.adoc#client-throughput-limits[Client throughput limits]"
        },
        "target_fetch_quota_byte_rate": {
            "description": "Target fetch-size quota byte rate. If `null`, the property is disabled, and no quota byte rate is applied.\n\n*Units*: bytes per second\n\n\n\n*Supported versions*: Redpanda v23.1 or later"
        },
        "aggregate_metrics": {
            "description": "Enable aggregation of metrics returned by the xref:./internal-metrics-reference.adoc[/metrics] endpoint. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.\n\n*Type*: boolean"
        },
        "disable_metrics": {
            "description": "Disable registering metrics exposed on the internal metrics endpoint.\n\n*Type*: boolean"
        },
        "disable_public_metrics": {
            "description": "Disable registering metrics exposed on the public metrics endpoint.\n\n*Type*: boolean"
        },
        "enable_metrics_reporter": {
            "description": "Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by xref:./tunable-properties.adoc#metrics_reporter_report_interval[metrics_reporter_report_interval].\n\n[NOTE]\n====\nThe cluster metrics of the metrics reporter are different from xref:manage:monitoring.adoc[monitoring metrics].\n\n* The metrics reporter exports customer usage metrics for consumption by Redpanda Data.\n* Monitoring metrics are exported for consumption by Redpanda users to monitor their system's health.\n====\n\n*Type*: boolean"
        },
        "raft_learner_recovery_rate": {
            "description": "Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders.\n\nThis rate limit is placed on a node sending data to a recovering node. Each sending node will be limited to this rate. The recovering node will accept data as fast as possible according to the combined limits of all healthy nodes in the cluster. For example, if two nodes are sending data to the recovering node, and `raft_learner_recovery_rate` is 100 MB/sec, then the recovering node will recover at a rate of 200 MB/sec.\n\n*Units*: bytes per second"
        },
        "cloud_storage_cache_size_percent": {
            "description": "Maximum size of the cloud cache.\n\nThe property <<cloud_storage_cache_size,`cloud_storage_cache_size`>> controls the same limit expressed as a fixed number of bytes.\n\nNOTE: `cloud_storage_cache_size_percent` overrides `cloud_storage_cache_size`.\n\n*Units*: percentage of total disk size"
        },
        "delete_retention_ms": {
            "description": "{badge-deprecated}\n\nIMPORTANT: Starting in version 23.3.1, `delete_retention_ms` is deprecated. Use <<log_retention_ms, `log_retention_ms`>> instead."
        },
        "disk_reservation_percent": {
            "description": "Amount of disk space to reserve for general system overhead.\n\n*Units*: percentage of total disk size"
        },
        "initial_retention_local_target_bytes_default": {
            "description": "Initial local retention size target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled.\n\n*Units*: bytes"
        },
        "initial_retention_local_target_ms_default": {
            "description": "Initial local retention time target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled.\n\n*Units*: milliseconds"
        },
        "log_cleanup_policy": {
            "description": "Default cleanup policy for topic logs.\n\nThe topic property xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`] overrides the value of `log_cleanup_policy` at the topic level.\n\n\n*Valid Values*: `compact`, `delete`, `compact,delete`, `none`"
        },
        "log_compaction_interval_ms": {
            "description": "How often to trigger background compaction.\n\n*Units*: milliseconds"
        },
        "log_compression_type": {
            "description": "Default topic compression type (gzip, snappy, lz4, zstd, producer, or none).\n\nThe topic property xref:./topic-properties.adoc#compressiontype[`compression.type`] overrides the value of `log_compression_type` at the topic level.\n\n\n*Valid values*: `gzip`, `snappy`, `lz4`, `zstd`, `producer`, `none`"
        },
        "log_message_timestamp_type": {
            "description": "Default timestamp type for topic messages (CreateTime or LogAppendTime).\n\nThe topic property xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`] overrides the value of `log_message_timestamp_type` at the topic level.\n\n\n*Valid values*: `CreateTime`, `LogAppendTime`"
        },
        "log_retention_ms": {
            "description": "The amount of time to keep a log file before deleting it (in milliseconds). If set to -1, no time limit is applied. This is a cluster-wide default when a topic does not set or disable xref:./topic-properties.adoc#retentionms[`retention.ms`].\n\n*Units*: milliseconds"
        },
        "retention_local_target_capacity_bytes": {
            "description": "The target capacity in bytes that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified, storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.\n\n*Units*: percentage of total disk size"
        },
        "retention_local_target_capacity_percent": {
            "description": "The target capacity in percent of unreserved space (<<disk_reservation_percent,`disk_reservation_percent`>>) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.\n\nNOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.\n\n*Units*: percentage of total disk size"
        },
        "retention_local_strict": {
            "description": "Flag to allow Tiered Storage topics to expand to consumable retention policy limits.\n\nWhen this flag is enabled, non-local retention settings are used, and local retention settings are used to inform data removal policies in low-disk space scenarios.\n\n*Type*: boolean"
        },
        "retention_local_target_bytes_default": {
            "description": "Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled.\n\nThis property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention].\n\n:::note\nBoth `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.\n:::\n\n*Units*: bytes\n\n\n\n*Related properties*:\n\n* <<retention_local_target_ms_default,retention_local_target_ms_default>>"
        },
        "retention_local_target_ms_default": {
            "description": "Local retention time target for partitions of topics with object storage write enabled.\n\nThis property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention].\n\nNOTE: Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.\n\n*Units*: milliseconds\n\n\n\n*Related properties*:\n\n* <<retention_local_target_bytes_default,retention_local_target_bytes_default>>"
        },
        "retention_local_trim_interval": {
            "description": "The period that disk usage is checked for disk pressure, and data is optionally trimmed to meet the target.\n\n*Units*: seconds"
        },
        "space_management_enable": {
            "description": "Option to explicitly disable automatic disk space management. If this was explicitly disabled while using v23.2, this will remain disabled following an upgrade.\n\n*Type*: boolean"
        },
        "storage_strict_data_init": {
            "description": "Requires that an empty file named `.redpanda_data_dir` be present in the xref:./node-properties.adoc#data_directory[data directory]. If set `true`, Redpanda will refuse to start if the file is not found in the data directory.\n\n*Type*: boolean"
        },
        "storage_ignore_timestamps_in_future_sec": {
            "description": "The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda.\n\nWhen determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime).\n\nBy default, `storage_ignore_timestamps_in_future_sec` is disabled (null).\n\n[TIP]\n====\nTo figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system:\n\n. Look for logs with segments that are unexpectedly large and not being cleaned up.\n. In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized.\n. If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed.\n. If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.\n====\n\n*Units*: seconds\n\n\n\n*Supported versions*: Redpanda v22.3 or later"
        },
        "legacy_permit_unsafe_log_operation": {
            "description": "Flag enabling a Redpanda cluster operator to use unsafe control characters within strings such as consumer group names or user names.\n\nThis flag applies only for Redpanda clusters v23.1 or earlier that have upgraded to v23.2 or later. Newly-created Redpanda clusters v23.2 or later ignore this property.\n\n*Type*: boolean\n\n\n\n*Related properties*: <<legacy_unsafe_log_warning_interval_sec,legacy_unsafe_log_warning_interval_sec>>"
        },
        "legacy_unsafe_log_warning_interval_sec": {
            "description": "Period at which to log a warning about using unsafe strings containing control characters.\n\nIf unsafe strings are permitted by <<legacy_permit_unsafe_log_operation,`legacy_permit_unsafe_log_operation`>>, a warning will be logged at an interval specified by this property.\n\n*Units*: seconds\n\n\n\n*Related properties*: <<legacy_permit_unsafe_log_operation,legacy_permit_unsafe_log_operation>>"
        },
        "metrics_reporter_url": {
            "description": "URL of the cluster metrics reporter."
        },
        "auto_create_topics_enabled": {
            "description": "Allow automatic topic creation. To prevent runaway topic creation, this property is not supported for Redpanda Cloud BYOC and dedicated clusters. You should explicitly manage topic creation for these Redpanda Cloud clusters.\n\n*Type*: boolean"
        },
        "default_topic_partitions": {
            "description": "Default number of partitions per topic.\n\n*Units*: number of partitions per topic"
        },
        "default_topic_replications": {
            "description": "Default replication factor for new topics.\n\nThe topic property xref:./topic-properties.adoc#replicationfactor[`replication.factor`] overrides the value of `default_topic_replications` at the topic level.\n\n*Units*: number of replicas per topic\n\n\n"
        },
        "internal_topic_replication_factor": {
            "description": "Target replication factor for internal topics.\n\n*Units*: number of replicas per topic"
        },
        "minimum_topic_replication": {
            "description": "Minimum allowable replication factor for topics in this cluster. The set value must be positive, odd, and equal to or less than the number of available brokers. Changing this parameter only restricts newly-created topics. Redpanda returns an `INVALID_REPLICATION_FACTOR` error on any attempt to create a topic with a replication factor less than this property.\n\nIf you change the `minimum_topic_replication` setting, the replication factor of existing topics remains unchanged. However, Redpanda will log a warning on start-up with a list of any topics that have fewer replicas than this minimum. For example, you might see a message such as `Topic X has a replication factor less than specified minimum: 1 < 3`.\n\n*Units*: minimum number of replicas per topic\n\n\n*Minimum*: 1\n\n"
        },
        "retention_bytes": {
            "description": "Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied.\n\nThe topic property xref:./topic-properties.adoc#retentionbytes[`retention.bytes`] overrides the value of `retention_bytes` at the topic level.\n\n*Units*: bytes per partition"
        },
        "rm_violation_recovery_policy\n{badge-deprecated}": {
            "description": "Describes how to recover from an invariant violation on the partition level."
        },
        "enable_transactions": {
            "description": "Enable transactions (atomic writes).\n\n*Type*: boolean"
        },
        "max_transactions_per_coordinator": {
            "description": "Specifies the maximum number of active transaction sessions per coordinator. For details, see xref:develop:transactions#transaction-usage-tips[Transaction usage tips]."
        },
        "seq_table_min_size": {
            "description": "The minimum threshold number of sessions to keep in the seq table. Not affected by compaction."
        },
        "tm_sync_timeout_ms": {
            "description": "Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.\n\n*Units*: milliseconds"
        },
        "tm_violation_recovery_policy\n{badge-deprecated}": {
            "description": "Describes how to recover from an invariant violation at the transaction coordinator level."
        },
        "transaction_coordinator_cleanup_policy": {
            "description": "Cleanup policy for a transaction coordinator topic.\n\n\n*Valid Values*: `compact`, `delete`, `compact,delete`, `none`"
        },
        "transaction_coordinator_delete_retention_ms": {
            "description": "Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than <<transactional_id_expiration_ms,transactional_id_expiration_ms>>.\n\n*Units*: milliseconds"
        },
        "transactional_id_expiration_ms": {
            "description": "Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.\n\n*Units*: milliseconds"
        },
        "tx_timeout_delay_ms": {
            "description": "Delay before scheduling the next check for timed out transactions.\n\n*Units*: milliseconds"
        }
    }
}