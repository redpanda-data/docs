:latest-data-transforms-version: 0.0.0-20230830git604fcce

Data transforms let you run common data streaming tasks, like filtering, scrubbing, and transcoding, within Redpanda. For example, you may have consumers that require you to redact credit card numbers or convert JSON to Avro. Data transforms can also interact with the Redpanda Schema Registry to work with encoded data types.

Data transforms use a WebAssembly (Wasm) engine inside a Redpanda broker. A Wasm function acts on a single record in an input topic. You can develop and manage data transforms with xref:reference:rpk/rpk-transform/rpk-transform.adoc[`rpk transform`] commands.

See also: xref:./how-transforms-work.adoc[] and <<Limitations>>

IMPORTANT: This Beta feature is not supported for production deployments. Runtime behavior may change in the GA release, and Beta data transforms may not be upgradeable without changes.

== Enable data transforms

. To enable data transforms, set the `data_transforms_enabled` cluster property to `true`:
+
```bash
rpk cluster config set data_transforms_enabled true
```
. Restart all brokers:
+
ifdef::env-kubernetes[]
[,bash]
----
kubectl rollout restart statefulset redpanda --namespace=<namespace>
----
endif::[]
ifndef::env-kubernetes[]
[,bash]
----
rpk redpanda stop
rpk redpanda start
----
endif::[]

[[configure-memory]]
== Configure memory for data transforms

Redpanda reserves memory for each transform function within the broker. You need enough memory for your input record and output record to be in memory at the same time, as well as the runtime overhead. For TinyGo, the overhead is 64 KB. For standard Golang, the overhead is 16 MB.

Set the following properties based on the number of functions you have and the amount of memory you anticipate needing.

- `wasm_per_core_memory_reservation`: Total amount of memory (in bytes) to reserve per shard for all Wasm VMs. Default = 20 MiB. Requires restart.
- `wasm_per_function_memory_limit`: Amount of memory (in bytes) to reserve per instance of a Wasm VM. Default = 2 MiB. Requires restart.

For example, to set `wasm_per_core_memory_reservation` to 40 MiB:

[,bash]
----
rpk cluster config set wasm_per_core_memory_reservation=41943040
----

The maximum number of functions that can be deployed to a cluster is equal to `wasm_per_core_memory_reservation` / `wasm_per_function_memory_limit`. When that limit is hit, Redpanda cannot allocate memory for the VM and the transforms stay in error states.

See also: xref:./how-transforms-work.adoc[]

== Create a data transforms project

ifdef::env-kubernetes[]
. Install `rpk` on your local machine, not on a Pod:
+
[tabs]
=====
Linux::
+
--
[loweralpha]
include::get-started:partial$install-rpk-linux.adoc[]

--
macOS::
+
--
[loweralpha]
include::get-started:partial$install-rpk-homebrew.adoc[]

--
=====

. Configure `rpk` to connect to your cluster using the xref:manage:kubernetes/networking/connect-to-redpanda.adoc#rpk-profile[pre-configured profile]:
+
[source,bash]
----
rpk profile create --from-profile <(kubectl get configmap --namespace <namespace> redpanda-rpk -o go-template='{{ .data.profile }}') <profile-name>
----
+
Replace `<profile-name>` with the name that you want to give this `rpk` profile.
endif::[]

. Create and initialize a data transforms project:
+
```bash
rpk transform init
```
+
A successful command generates project files in your current directory:
+
[.no-copy]
----
.
├── go.mod
├── go.sum
├── README.md
├── transform.go
└── transform.yaml
----
+
The `transform.go` file contains the transform logic, and the `transform.yaml` file specifies the transform's configuration.
+
TIP: When creating a custom data transform, initialization steps can be done either in `main` (because it's only run once at the start of the package) or in Go's standard predefined `init()` function. Although state can be cached in global variables, Redpanda may restart a Wasm module at any point, which causes the state to be lost.

. Implement your project by adding transform logic to `transform.go`.
+
The following examples show some basic transforms. Each example can be copied into the `transform.go` file.
+
[tabs]
====
Identity transform::
+
--
```go
package main

import (
	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows the basic usage of the package:
// This transform does nothing but copy the same data from an
// input topic to an output topic.
func main() {
	// Make sure to register your callback and perform other setup in main
	redpanda.OnRecordWritten(identityTransform)
}

// This will be called for each record in the source topic.
//
// The output records returned will be written to the destination topic.
func identityTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	return []redpanda.Record{e.Record()}, nil
}
```
--
Transcoder transform::
+
--

```go
package main

import (
	"bytes"
	"encoding/csv"
	"encoding/json"
	"errors"
	"io"
	"strconv"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a transform that converts CSV inputs into JSON outputs.
func main() {
	redpanda.OnRecordWritten(csvToJsonTransform)
}

type Foo struct {
	A string `json:"a"`
	B int    `json:"b"`
}

func csvToJsonTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	// The input data is a CSV (without a header row) that is the structure of:
	// key, a, b
	reader := csv.NewReader(bytes.NewReader(e.Record().Value))
	// Improve performance by reusing the result slice.
	reader.ReuseRecord = true
	output := []redpanda.Record{}
	for {
		row, err := reader.Read()
		if err == io.EOF {
			break
		} else if err != nil {
			return nil, err
		}
		if len(row) != 3 {
			return nil, errors.New("unexpected number of rows")
		}
		// Convert the last column into an int
		b, err := strconv.Atoi(row[2])
		if err != nil {
			return nil, err
		}
		// Marshal our JSON value
		f := Foo{
			A: row[1],
			B: b,
		}
		v, err := json.Marshal(&f)
		if err != nil {
			return nil, err
		}
		// Add our output record using the first column as the key.
		output = append(output, redpanda.Record{
			Key:   []byte(row[0]),
			Value: v,
		})

	}
	return output, nil
}
```

--
Validation filter transform::
+
--
```go
import (
	"encoding/json"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a filter that outputs only valid JSON into the
// output topic.
func main() {
	redpanda.OnRecordWritten(filterValidJson)
}

func filterValidJson(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	v := []redpanda.Record{}
	if json.Valid(e.Record().Value) {
		v = append(v, e.Record())
	}
	return v, nil
}
```

--
====

== Build and deploy the transform

. Build the transform into a Wasm module with metadata:
+
```bash
rpk transform build
```

. Deploy the Wasm module to your cluster. For example, with the identity transform:
+
```bash
rpk transform deploy --input-topic=demo-1 --output-topic=demo-2
```

. Validate that your transform is running. For example:
.. Produce a few records to the `demo-1` topic.
+
```bash
echo "foo\nbar" | rpk topic produce demo-1
```
.. Consume from the `demo-2` topic.
+
```bash
rpk topic consume demo-2
```
+
[,json,role="no-copy"]
----
{
  "topic": "demo-2",
  "value": "foo",
  "timestamp": 1687545891433,
  "partition": 0,
  "offset": 0
},
{
  "topic": "demo-2",
  "value": "bar",
  "timestamp": 1687545892434,
  "partition": 0,
  "offset": 1
}
----

NOTE: You can see `stdout` and `stderr` for your function in the broker's logs.

== Monitor data transforms

You can monitor your transforms with the following metrics:

* `transform_execution_latency_sec`
* `transform_execution_errors`
* `wasm_engine_cpu_seconds_total`
* `wasm_engine_memory_usage`
* `wasm_engine_max_memory`
* `wasm_binary_executable_memory_usage`
* `transform_processor_read_bytes`
* `transform_processor_write_bytes`
* `transform_processor_lag`
* `transform_processor_failures`
* `transform_processor_state`

See xref:reference:public-metrics-reference.adoc[]

== Limitations

- Transforms have no external access to disk or network resources.
- Only single record transforms is supported, but multiple output records from a single input record is supported. For aggregations, joins, or complex transformations, use Apache Flink.
- Only a single output topic is supported.
- Transforms have at-least-once delivery.
- When clients use the Kafka Transactions API on partitions of an input topic, transforms process only committed records.
- Because data transforms are powered by Wasm, transform functions can be authored in any language. However, a data transforms SDK currently is only available in xref:reference:data-transform-api.adoc[Golang].

== Suggested reading

- xref:./how-transforms-work.adoc[]
- xref:reference:data-transform-api.adoc[]
- xref:reference:rpk/rpk-transform/rpk-transform.adoc[`rpk transform` commands]
