= Data Transforms
:description: Learn how to run WebAssembly data transforms within Redpanda brokers.
:page-aliases: reference:rpk/rpk-wasm/rpk-wasm.adoc, reference:rpk/rpk-wasm.adoc, reference:rpk/rpk-wasm/rpk-wasm-deploy.adoc, reference:rpk/rpk-wasm/rpk-wasm-generate.adoc, reference:rpk/rpk-wasm/rpk-wasm-remove.adoc, data-management:data-transform.adoc
:latest-data-transforms-version: 0.0.0-20230830git604fcce

Redpanda provides the framework to build and deploy inline transformations on data written to Redpanda topics, delivering validated and processed data to consumers in the form they expect. Data transforms let you process common data streaming tasks, like filtering, scrubbing, and transcoding, within the Redpanda broker. For example, you may have consumers that require you to remove personal data like credit card numbers or convert JSON to Avro. Data transforms can also interact with the Redpanda Schema Registry to work with encoded data types. Redpanda does this directly inside the broker, eliminating the need for a separate stream processing environment or 3rd-party tools. 

image::shared:wasm1.png[Data transforms in a broker] 

Data transforms run asynchronously using a WebAssembly (Wasm) engine inside a Redpanda broker. You can develop and manage data transforms with `rpk transform` commands. 

== Limitations

- Transforms have no external access to disk or network resources. 
- Only single record transforms are supported, not aggregations or joins. In other words, a Wasm function acts on a single record in an input topic.
- Only a single output topic is supported.
- Transforms have at-least-once delivery.
- Because data transforms are powered by Wasm, transform functions can be authored in any language. However, libraries to author transforms currently are only available in https://go.dev/doc/install[Golang^]. 

== How data transforms work

Within Repdanda, there's a single Raft controller that manages cluster information, including data transforms. On every shard (core), Redpanda knows what data transforms exist in the cluster, as well as metadata about the transform, such as input and output topics and environment variables. Data transforms take an input topic and map it to an output topic, running virtual machines (VMs) on the same shard as the input topic. 

To run Wasm, Redpanda uses just-in-time (JIT) compilation, which compiles it in memory, writes it to executable space, then runs the directly translated machine code. When it becomes leader of a given partition on the input topic, Redpanda spins up Wasm VMs and fetches the function's executable code in memory. Redpanda writes the output, which may go to the same shard on the same machine, or to a different shard on the same machine, or to a different machine. Because all transform function metadata is in memory, Redpanda can stop and start transforms when partitions are moved. 

image::shared:wasm_architecture.png[Wasm architecture in Redpanda]

The transform function processes every record produced to an input topic and returns zero or more records that are then produced to an output topic. Data transforms are applied to all partitions on an input topic. A record is processed after it has been successfully written to disk on the input topic. The record is then copied into the Wasm runtime, transformed, and copied back out, and then the broker writes the resulting record to the output topic. 

In the following diagram, you see that Broker A has its own disk that it persistently writes to, and Broker B also has its own disk that it persistently writes to. A transform doesn't affect the original produced record: it happens in the background after the write finishes.

image::shared:wasm_flow.png[Flow of record in Redpanda data transform]

When you deploy a data transform, Redpanda pushes the generated `.wasm` files and metadata to the cluster, which then stores and replicates the bytecode and metadata using internal broker topics. Wasm is executed within a runtime in the broker. Each shard of Redpanda runs its own copy of the transform function. There is at most one runtime for a transform for any shard, and memory for each function must be reserved within the broker. This is achieved with the following cluster configuration properties: 

- `wasm_per_core_memory_reservation`: Total amount of memory to reserve per shard 
- `wasm_per_function_memory_limit`: Limit for an individual transform's memory

There may only be `wasm_per_core_memory_reservation`/ `wasm_per_function_memory_limit` transforms deployed to a cluster at once. CPU time is dynamically allocated to the Wasm runtime and ensures that the code does not run forever and cannot block the broker from handling traffic or doing other work, such as Tiered Storage uploads.

== Enable data transforms

To enable data transforms, set the `data_transforms_enabled` cluster configuration flag:

```bash
rpk cluster config set data_transforms_enabled true
```

== Create a data transforms project

. Create and initialize a data transforms project:
+
```bash
rpk transform init
```
+
A successful command generates project files in your current directory:
+
[.no-copy]
----
.
├── go.mod
├── go.sum
├── README.md
├── transform.go
└── transform.yaml
----
+
The `transform.go` file contains the transform logic, and the `transform.yaml` file configures the project.
+
TIP: When creating a custom data transform, initialization steps can be done either in `main` (because it's only run once at the start of the package) or in Go's standard predefined `init()` function. Although state can be cached in global variables, Redpanda may restart a Wasm module at any point, which causes the state to be lost.

. Implement your project by adding transform logic to `transform.go`. 
+
The following examples show some basic transforms. Each example can be copied into the `transform.go` file.
+
[tabs]
====
Identity transform::
+
--
```go
package main

import (
	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows the basic usage of the package:
// This transform does nothing but copy the same data from an
// input topic to an output topic.
func main() {
	// Make sure to register your callback and perform other setup in main
	redpanda.OnRecordWritten(identityTransform)
}

// This will be called for each record in the source topic.
//
// The output records returned will be written to the destination topic.
func identityTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	return []redpanda.Record{e.Record()}, nil
}
```
--
Transcoder transform::
+
--

```go
package main

import (
	"bytes"
	"encoding/csv"
	"encoding/json"
	"errors"
	"io"
	"strconv"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a transform that converts CSV inputs into JSON outputs.
func main() {
	redpanda.OnRecordWritten(csvToJsonTransform)
}

type Foo struct {
	A string `json:"a"`
	B int    `json:"b"`
}

func csvToJsonTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	// The input data is a CSV (without a header row) that is the structure of:
	// key, a, b
	reader := csv.NewReader(bytes.NewReader(e.Record().Value))
	// Improve performance by reusing the result slice.
	reader.ReuseRecord = true
	output := []redpanda.Record{}
	for {
		row, err := reader.Read()
		if err == io.EOF {
			break
		} else if err != nil {
			return nil, err
		}
		if len(row) != 3 {
			return nil, errors.New("unexpected number of rows")
		}
		// Convert the last column into an int
		b, err := strconv.Atoi(row[2])
		if err != nil {
			return nil, err
		}
		// Marshal our JSON value
		f := Foo{
			A: row[1],
			B: b,
		}
		v, err := json.Marshal(&f)
		if err != nil {
			return nil, err
		}
		// Add our output record using the first column as the key.
		output = append(output, redpanda.Record{
			Key:   []byte(row[0]),
			Value: v,
		})

	}
	return output, nil
}
```

--
Validation filter transform::
+
--
```go
import (
	"encoding/json"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a filter that outputs only valid JSON into the
// output topic.
func main() {
	redpanda.OnRecordWritten(filterValidJson)
}

func filterValidJson(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	v := []redpanda.Record{}
	if json.Valid(e.Record().Value) {
		v = append(v, e.Record())
	}
	return v, nil
}
```

--
====

== Build and deploy the transform

. Build the transform into a Wasm module with metadata: 
+
```bash
rpk transform build
```

. Deploy the Wasm module to your cluster. For example:
+
```bash
rpk transform deploy --input-topic=demo-1 --output-topic=demo-2
```

. Validate that your transform is running. For example:
.. Produce a few records to the `demo-1` topic.
+
```bash
echo "foo\nbar" | rpk topic produce demo-1
```
.. Consume from the `demo-2` topic.
+
```bash
rpk topic consume demo-2
```
+
[,json,role="no-copy"]
----
{
  "topic": "demo-2",
  "value": "foo",
  "timestamp": 1687545891433,
  "partition": 0,
  "offset": 0
}
{
  "topic": "demo-2",
  "value": "bar",
  "timestamp": 1687545892434,
  "partition": 0,
  "offset": 1
}
----

NOTE: You can see `stdout` and `stderr` for your function in the broker's logs. 

== Related topics

- xref:reference:data-transform-api.adoc[Data Transforms API]
- rpk transform **(link when we pull code include in Beta docs rpk section)**
