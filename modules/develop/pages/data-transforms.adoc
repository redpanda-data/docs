= Data Transforms
:description: Learn how to run WebAssembly data transforms within Redpanda brokers.
:page-aliases: reference:rpk/rpk-wasm/rpk-wasm.adoc, reference:rpk/rpk-wasm.adoc, reference:rpk/rpk-wasm/rpk-wasm-deploy.adoc, reference:rpk/rpk-wasm/rpk-wasm-generate.adoc, reference:rpk/rpk-wasm/rpk-wasm-remove.adoc, data-management:data-transform.adoc
:latest-data-transforms-version: 0.0.0-20230830git604fcce

With Redpanda data transforms, you can perform common data streaming tasks within the Redpanda broker, like filtering, scrubbing/cleaning/redacting, and normalizing and transcoding. For example, you may have consumers that require you to convert JSON to Avro or require you to remove all PII, like credit card numbers. Data transforms let you do this directly inside the broker without a separate stream processing environment or 3rd-party tools. Data transforms dramatically simplify the effort of delivering validated and pre-processed data to consumers in the form they expect. Data transforms can also interact with the Redpanda Schema Registry to work with encoded data types.

**(add Wasm image from roadmap)**

Redpanda provides the framework to create, build, and deploy inline transformations on data written to Redpanda topics. The data transforms run asynchronously using a WebAssembly (Wasm) engine inside a Redpanda broker. The transform function processes every record produced to an input topic and returns one or more records that are then produced to an output topic. Data transforms are applied to all partitions on an input topic. A records is processed after it has been successfully written to disk on the input topic. Each record in a partition is then copied into the Wasm runtime, transformed, and copied back out. The broker then writes the resulting records back out to the output topic.

You can develop and manage data transforms with `rpk transform` commands. When you deploy a data transform, Redpanda pushes the generated `.wasm` files and metadata to the cluster, which then stores and replicates the bytecode and metadata using internal broker topics. Wasm is executed within a runtime in the broker. Each shard (core) of Redpanda runs it's own copy of the transform function. There is at most one runtime for a transform for any shard, and memory for each function must be reserved within the broker. This is achieved with the cluster configuration properties: 

- `wasm_per_core_memory_reservation`: the total amount of memory to reserve per shard 
- `wasm_per_function_memory_limit`: the limit for an individual transform's memory

There may only be `wasm_per_core_memory_reservation`/ `wasm_per_function_memory_limit` transforms deployed to a cluster at once. CPU time is dynamically allocated to the Wasm runtime and ensures that the code does not run forever and cannot block the broker from handling traffic or doing other work (such as Tiered Storage uploads).

== How data transforms work

**(add architecture diagram)**

Within Repdanda, there's a single Raft controller that manages cluster information, including data transforms. So on every shard, Redpanda knows what transforms exist in the cluster, as well as metadata about the transform, such as input and output topics and environment variables. Data transforms take an input topic and map it to an output topic, running virtual machines (VMs) on the same shard as the input topic. 

To run Wasm, Redpanda uses just-in-time (JIT) compilation, which compiles it in memory, writes it to executable space, then runs the directly translated machine code. When it becomes leader of a given partition on the input topic, Redpanda spins up Wasm VMs and fetches the function's executable code in memory. Redpanda writes the output, which may go to the same shard on the same machine, or to a different shard on the same machine, or to a different machine. Because all transform function metadata is in memory, Redpanda can stop and start transforms when partitions are moved. 

In the following diagram, you can see Broker A has it's own disk that it persistently writes to, and Broker B has it's own disk too that it persistently writes to. The transform doesn't affect the original produced record: it happens in the background after the write finishes.

**(add sequence/mermaid diagram)**

== Limitations

- Transforms have no external access to disk or network resources. 
- Only single record transforms are supported, not aggregations or joins. In other words, a Wasm function acts on a single record in an input topic.
- The input and output topics must have the same number of partitions. Redpanda writes to the same output partition as the input partition. For example, all records on input Topic A partition 2 go to output Topic B partition 2. 
- Only a single output topic is supported.
- Transforms have at-least-once delivery.
- Because data transforms are powered by Wasm, transform functions can be authored in any language. However, libraries to author transforms currently are only available in https://go.dev/doc/install[Golang^]. 

== Enable data transforms

. Enable the data transforms cluster configuration flag:
+
`rpk cluster config set data_transforms_enabled true`

== Create a data transforms project

. Create and initialize a data transforms project:
+
```bash
rpk transform init
```
+
A successful command generates project files in your current directory:
+
[.no-copy]
----
.
├── go.mod
├── go.sum
├── README.md
├── transform.go
└── transform.yaml
----
+
The `transform.go` file contains the transform logic, and the `transform.yaml` file configures the project.
+
TIP: When creating a custom data transform, initialization steps can be done either in `main` (because it's only run once at the start of the package) or in Go's standard predefined `init()` function. Although state can be cached in global variables, Redpanda may restart a Wasm module at any point, which causes the state to be lost.

. Implement your project by adding transform logic to `transform.go`. The following examples show some basic transforms. Each example can be copied into the `transform.go` file.
+
[tabs]
====
Identity transform::
+
--
```go
package main

import (
	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows the basic usage of the package:
// This transform does nothing but copy the same data from an
// input topic to an output topic.
func main() {
	// Make sure to register your callback and perform other setup in main
	redpanda.OnRecordWritten(identityTransform)
}

// This will be called for each record in the source topic.
//
// The output records returned will be written to the destination topic.
func identityTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	return []redpanda.Record{e.Record()}, nil
}
```
--
Transcoder transform::
+
--

```go
package main

import (
	"bytes"
	"encoding/csv"
	"encoding/json"
	"errors"
	"io"
	"strconv"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a transform that converts CSV inputs into JSON outputs.
func main() {
	redpanda.OnRecordWritten(csvToJsonTransform)
}

type Foo struct {
	A string `json:"a"`
	B int    `json:"b"`
}

func csvToJsonTransform(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	// The input data is a CSV (without a header row) that is the structure of:
	// key, a, b
	reader := csv.NewReader(bytes.NewReader(e.Record().Value))
	// Improve performance by reusing the result slice.
	reader.ReuseRecord = true
	output := []redpanda.Record{}
	for {
		row, err := reader.Read()
		if err == io.EOF {
			break
		} else if err != nil {
			return nil, err
		}
		if len(row) != 3 {
			return nil, errors.New("unexpected number of rows")
		}
		// Convert the last column into an int
		b, err := strconv.Atoi(row[2])
		if err != nil {
			return nil, err
		}
		// Marshal our JSON value
		f := Foo{
			A: row[1],
			B: b,
		}
		v, err := json.Marshal(&f)
		if err != nil {
			return nil, err
		}
		// Add our output record using the first column as the key.
		output = append(output, redpanda.Record{
			Key:   []byte(row[0]),
			Value: v,
		})

	}
	return output, nil
}
```

--
Validation filter transform::
+
--
```go
import (
	"encoding/json"

	"github.com/redpanda-data/redpanda/src/go/transform-sdk"
)

// This example shows a filter that outputs only valid JSON into the
// output topic.
func main() {
	redpanda.OnRecordWritten(filterValidJson)
}

func filterValidJson(e redpanda.WriteEvent) ([]redpanda.Record, error) {
	v := []redpanda.Record{}
	if json.Valid(e.Record().Value) {
		v = append(v, e.Record())
	}
	return v, nil
}
```

--
====

== Build and deploy the transform

. Build the transform into a Wasm module and package it up with metadata. 
+
```bash
rpk transform build
```

. Deploy the Wasm module to your cluster.
+
```bash
rpk transform deploy --input-topic=demo-1 --output-topic=demo-2
```

. Validate that your transform is running:
.. Produce a few records to the `demo-1` topic.
+
```bash
echo "foo\nbar" | rpk topic produce demo-1
```
.. Consume from the `demo-2` topic.
+
```bash
rpk topic consume demo-2
```
+
[,json,role="no-copy"]
----
{
  "topic": "demo-2",
  "value": "foo",
  "timestamp": 1687545891433,
  "partition": 0,
  "offset": 0
}
{
  "topic": "demo-2",
  "value": "bar",
  "timestamp": 1687545892434,
  "partition": 0,
  "offset": 1
}
----

NOTE: You can see `stdout` and `stderr` from the broker's logs. 

== Next steps

- xref:reference:data-transform-api.adoc[Data Transforms API]
- rpk transform **(link when we pull code include in Beta docs rpk section)**
