= Deploy a Redpanda Cluster in Google Kubernetes Engine
:description: Deploy a secure Redpanda cluster and Redpanda Console in Google Kubernetes Engine (GKE).

Deploy a secure Redpanda cluster and Redpanda Console in Google Kubernetes Engine (GKE). Then, use rpk both as an internal client and an external client to interact with your Redpanda cluster from the command-line. Your Redpanda cluster will have the following security features:

* SASL for authenticating users' connections.
* TLS with self-signed certificates for secure communication between the cluster and clients.

== Prerequisites

Before you begin, you must have the following:

* Complete the 'Before you begin' steps and the 'Launch Cloud Shell' steps of the https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster#before-you-begin[GKE quickstart^]. Cloud Shell comes preinstalled with the Google Cloud CLI, the `kubectl` command-line tool, and the Helm package manager.
* https://kubernetes.io/docs/tasks/tools/[`kubectl`^]. Minimum required Kubernetes version: {supported-kubernetes-version}.
+
[,bash]
----
kubectl version --short --client
----

* https://helm.sh/docs/intro/install/[Helm^]. Minimum required Helm version: {supported-helm-version}
+
[,bash]
----
helm version
----

== Create a GKE cluster

Your GKE cluster must have one worker node available for each Redpanda broker that you plan to deploy in your Redpanda cluster. You also need to run the worker nodes on a machine type that supports the xref:./kubernetes-cluster-requirements.adoc[requirements and recommendations] for production deployments.

In this step, you create a GKE cluster with three nodes on https://cloud.google.com/compute/docs/compute-optimized-machines#c2d_series[`c2d-standard-8` machine types]. Deploying three nodes allows your GKE cluster to support a Redpanda cluster with three brokers. This `c2d-standard-8` instance type comes with:

- 2 cores per worker node, which is required.
- Local NVMe disks, which is recommended for best performance.

NOTE: The Helm chart configures `podAntiAffinity` rules to make sure that only one Pod running a Redpanda broker is scheduled on each worker node.

. Create a GKE cluster. Replace the `<region>` placeholder with your own region.
+
[,bash]
----
gcloud container clusters create <cluster-name> \
  --machine-type c2d-standard-8 \
  --num-nodes=3 \
  --local-nvme-ssd-block count=2
  --region=<region>
----
+
[TIP]
====
To see all options that you can specify when creating a cluster, see the https://cloud.google.com/sdk/gcloud/reference/container/clusters/create[Cloud SDK reference^].

Or, for help creating a GKE cluster, see the https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster#create_cluster[GKE documentation^].
====

=== Create a StorageClass for your local NVMe disks

When you provisioned the GKE cluster, you selected an instance type that comes with local NVMe disks. However, these disks are not automatically mounted or formatted upon creation. To use these local NVMe disks, you must mount and format them, and you must create the necessary PersistentVolumes (PVs). To automate this process, you can use a Container Storage Interface (CSI) driver.

In this step, you install the recommended https://github.com/metal-stack/csi-driver-lvm[local volume manager (LVM) CSI driver]. Then, you create a StorageClass that references the LVM CSI driver and specifies the recommended XFS filesystem.

. Install the LVM CSI driver:
+
[,yaml,lines=6]
----
helm repo add metal-stack https://helm.metal-stack.io
helm repo update
helm install csi-driver-lvm metal-stack/csi-driver-lvm \
  --namespace csi-driver-lvm \
  --create-namespace \
  --set lvm.devicePattern='/dev/nvme[0-9]n[0-9]'
----
+
The `lvm.devicePattern` property specifies the pattern that the CSI driver uses to identify available NVMe volumes on your worker nodes.

. Create the StorageClass:
+
.`csi-driver-lvm-linear-xfs.yaml`
[,yaml,lines=5-8+10-11]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-driver-lvm-linear-xfs
provisioner: lvm.csi.metal-stack.io
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: "linear"
  csi.storage.k8s.io/fstype: xfs
----
+
- `provisioner`: The LVM CSI driver responsible for provisioning the volume.
- `reclaimPolicy`: The `Retain` policy ensures that the underlying volume is not deleted when the corresponding PVC is deleted.
- `volumeBindingMode`: The `WaitForFirstConsumer` mode delays the binding and provisioning of a PersistentVolume until a Pod that uses the PVC is created. This mode is important for ensuring that the PV is created on the same node where the Pod will run because the PV will use the node's local NVMe volumes.
- `allowVolumeExpansion`: Allows the volume to be expanded after it has been provisioned.
- `parameters.type`: Combines multiple physical volumes to create a single logical volume. In a linear setup, data is written sequentially to the first physical volume until it is full, then to the next, and so on.
- `parameters.csi.storage.k8s.io/fstype`: Formats the volumes with the XFS filesystem. Redpanda Data recommends XFS for its enhanced performance with Redpanda workloads.

. Apply the StorageClass:
+
[,bash]
----
kubectl apply -f csi-driver-lvm-linear-xfs.yaml
----
+
After applying this StorageClass, any PVCs that references it will attempt to provision storage using the LVM CSI driver and the provided parameters.

=== Configure external access

Add inbound firewall rules to your instances so that external traffic can reach the following node ports on all Kubernetes worker nodes in the cluster:

* 31644
* 31092
* 30082
* 30081

For help creating firewall rules, see the https://cloud.google.com/vpc/docs/using-firewalls[Google VPC documentation^].

include::deploy:partial$kubernetes/guides/deploy-redpanda.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/verify-deployment.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/create-user.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/start-streaming.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/explore-topics-localhost.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/external-access-intro.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/external-access-steps.adoc[]

include::deploy:partial$kubernetes/default-components.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/troubleshoot.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/next-steps.adoc[leveloffset=+1]

include::shared:partial$suggested-reading.adoc[]

include::deploy:partial$kubernetes/guides/suggested-reading-content.adoc[leveloffset=+1]
