= Deploy Redpanda for Production in Kubernetes
:description: Deploy a Redpanda cluster in Kubernetes.
:tags: ["Kubernetes"]
:page-aliases: deploy:deployment-option/self-hosted/kubernetes/kubernetes-best-practices.adoc, deploy:deployment-option/self-hosted/kubernetes/redpanda-cluster-recommendations.adoc, deploy:deployment-option/self-hosted/kubernetes/kubernetes-deploy.adoc
:page-toclevels: 1
:page-context-links: [{"name": "Linux", "to": "deploy:deployment-option/self-hosted/manual/production/production-deployment.adoc" },{"name": "Kubernetes", "to": "deploy:deployment-option/self-hosted/kubernetes/k-production-deployment.adoc" } ]
:env-kubernetes: true
:page-categories: Deployment, GitOps

This topic describes how to configure and deploy one or more Redpanda clusters and Redpanda Console in Kubernetes.

== Prerequisites

Make sure that your Kubernetes cluster meets the xref:./k-requirements.adoc[requirements].

You must already have a ConfigMap that stores your `io-config.yaml` file. See xref:./k-tune-workers.adoc#io[Generate optimal I/O configuration settings].

== Deploy a Redpanda cluster

To deploy Redpanda and Redpanda Console, you can use the following tools:

- *Redpanda Operator*: The Redpanda Operator extends Kubernetes with custom resource definitions (CRDs), allowing you to define Redpanda clusters as native Kubernetes resources. The resource that the Redpanda Operator uses to represent a Redpanda cluster is the Redpanda resource.
- *Helm*: https://helm.sh/docs[Helm^] is a package manager for Kubernetes, which simplifies the process of defining, installing, and upgrading Kubernetes applications. Helm uses charts, a collection of files that describe a related set of Kubernetes resources, to deploy applications in a Kubernetes cluster.

TIP: For more details about the differences between these two methods, see xref:./k-deployment-overview.adoc[].

[tabs]
====
Operator::
+
--

NOTE: The Redpanda Operator is namespace scoped. You must install the Redpanda Operator in the same namespace as your Redpanda resource (Redpanda cluster).

. Make sure that you have permission to install custom resource definitions (CRDs):
+
```bash
kubectl auth can-i create CustomResourceDefinition --all-namespaces
```
+
You should see `yes` in the output.
+
You need these cluster-level permissions to install glossterm:cert-manager[^] and Redpanda Operator CRDs in the next steps.

. Install https://cert-manager.io/docs/installation/helm/[cert-manager^]:
+
```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
  --set crds.enabled=true \
  --namespace cert-manager  \
  --create-namespace
```
+
The Redpanda Helm chart enables TLS by default and uses cert-manager to manage TLS certificates.

. Install the Redpanda Operator CRDs:
+
include::deploy:partial$kubernetes/install-crds.adoc[]

. Deploy the Redpanda Operator.
+
[,bash,subs="attributes+"]
----
helm repo add redpanda https://charts.redpanda.com
helm upgrade --install redpanda-controller redpanda/operator \
  --namespace <namespace> \
  --create-namespace \
  --version {latest-operator-version} \ <1>
----
+
<1> This flag specifies the exact version of the Redpanda Operator Helm chart to use for deployment. By setting this value, you pin the chart to a specific version, which prevents automatic updates that might introduce breaking changes or new features that have not been tested in your environment.

. Ensure that the Deployment is successfully rolled out:
+
```bash
kubectl --namespace <namespace> rollout status --watch deployment/redpanda-controller-operator
```
+
[.no-copy]
----
deployment "redpanda-controller-operator" successfully rolled out
----

. Install a xref:reference:k-crd.adoc[Redpanda custom resource] to deploy a Redpanda cluster and Redpanda Console.
+
.`redpanda-cluster.yaml`
[,yaml,subs="attributes+",lines=4+6+7+8]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  clusterSpec:
    #enterprise:
      #licenseSecretRef:
        #name: <secret-name>
        #key: <secret-key>
    statefulset:
      extraVolumes: |-
        - name: redpanda-io-config
          configMap:
            name: redpanda-io-config
      extraVolumeMounts: |-
        - name: redpanda-io-config
          mountPath: /etc/redpanda-io-config
      additionalRedpandaCmdFlags:
        - "--io-properties-file=/etc/redpanda-io-config/io-config.yaml"
----
+
- `metadata.name`: Name to assign the Redpanda cluster.
- xref:reference:k-crd.adoc#k8s-api-github-com-redpanda-data-redpanda-operator-api-redpanda-v1alpha2-redpandaclusterspec[`spec.clusterSpec`]: This is where you can override default values in the Redpanda Helm chart. Here, you mount the <<prerequisites, I/O configuration file>> to the Pods that run Redpanda. For other configuration details, see <<Production considerations>>.
- `spec.clusterSpec.enterprise`: If you want to use enterprise features in Redpanda, uncomment this section and add the details of a Secret that stores your Enterprise Edition license key. For details, see xref:get-started:licenses.adoc[].
- `spec.clusterSpec.statefulset`: Here, you mount the <<prerequisites, I/O configuration file>> to the Pods that run Redpanda. For other configuration details, see <<Production considerations>>.

. Apply the Redpanda resource:
+
```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```
+
NOTE: The Redpanda resource must be deployed in the same namespace as the Redpanda Operator. Each new deployment of Redpanda requires a separate namespace.

. Wait for the Redpanda Operator to deploy Redpanda using the Helm chart:
+
```bash
kubectl get redpanda --namespace <namespace> --watch
```
+
[.no-copy]
----
NAME       READY   STATUS
redpanda   True    Redpanda reconciliation succeeded
----
+
This step may take a few minutes. You can watch for new Pods to make sure that the deployment is progressing:
+
```bash
kubectl get pod --namespace <namespace>
```
+
If it's taking too long, see xref:manage:kubernetes/troubleshooting/k-troubleshoot.adoc[Troubleshooting].

. Verify that each Redpanda broker is scheduled on only one Kubernetes node:
+
```bash
kubectl get pod --namespace <namespace>  \
  -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -l \
  app.kubernetes.io/component=redpanda-statefulset
```
+
Expected output:
+
[.no-copy]
----
example-worker3   redpanda-0
example-worker2   redpanda-1
example-worker    redpanda-2
----

--
Helm::
+
--

. Install cert-manager using Helm:
+
```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
  --set crds.enabled=true \
  --namespace cert-manager  \
  --create-namespace
```
+
The Redpanda Helm chart enables TLS by default and uses cert-manager to manage TLS certificates.

. Override the default values to mount your <<prerequisites, I/O configuration file>> onto each Pod that runs Redpanda.
+
.`redpanda-values.yaml`
[,yaml]
----
statefulset:
  extraVolumes: |-
    - name: redpanda-io-config
      configMap:
        name: redpanda-io-config
  extraVolumeMounts: |-
    - name: redpanda-io-config
      mountPath: /etc/redpanda-io-config
  additionalRedpandaCmdFlags:
    - "--io-properties-file=/etc/redpanda-io-config/io-config.yaml"
----
+
Redpanda reads from this file at startup to optimize itself for the given I/O parameters.
+
If you want to use enterprise features in Redpanda, add the details of a Secret that stores your Enterprise Edition license key.
+
.`redpanda-values.yaml`
[,yaml]
----
enterprise:
  licenseSecretRef:
    name: <secret-name>
    key: <secret-key>
----
+
For details, see xref:get-started:licensing/add-license-redpanda/kubernetes.adoc[].

. Install the Redpanda Helm chart to deploy a Redpanda cluster and Redpanda Console.
+
[source,bash,subs="attributes+"]
----
helm repo add redpanda https://charts.redpanda.com
helm repo update
helm install redpanda redpanda/redpanda \
  --version {latest-redpanda-helm-chart-version} \ <1>
  --namespace <namespace> \ <2>
  --create-namespace \
  --values redpanda-values.yaml
----
+
<1> This flag specifies the exact version of the Redpanda Helm chart to use for deployment. By setting this value, you <<version-pinning, pin the chart to a specific version>>, which prevents automatic updates that might introduce breaking changes or new features that have not been tested in your environment.
<2> Each deployment of the Redpanda Helm chart requires a separate namespace. Ensure you choose a unique namespace for each deployment.

. Wait for the Redpanda cluster to be ready:
+
```bash
kubectl --namespace <namespace> rollout status statefulset redpanda --watch
```
+
When the Redpanda cluster is ready, the output should look similar to the following:
+
[.no-copy]
----
statefulset rolling update complete 3 pods at revision redpanda-8654f645b4...
----

. Verify that each Redpanda broker is scheduled on only one Kubernetes node:
+
```bash
kubectl get pod --namespace <namespace> \
-o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -l \
app.kubernetes.io/component=redpanda-statefulset
```
+
Expected output:
+
[.no-copy]
----
example-worker3   redpanda-0
example-worker2   redpanda-1
example-worker    redpanda-2
----

--
====


[[config]]
== Production considerations

This section provides advice for configuring the Redpanda in Kubernetes for production.

If you're using the Redpanda Operator, see: xref:reference:k-crd.adoc[] for all available settings.

If you're using the Redpanda Helm chart, see: xref:reference:k-redpanda-helm-spec.adoc[] for all available settings.

[[version-pinning]]
=== Version pinning (Helm)

If you use the Redpanda Helm chart to deploy Redpanda, it's important to pin the version of the Helm chart to ensure that you have control over the version of Redpanda that you deploy.

The Redpanda Helm chart version is independent of the Redpanda application version. The Redpanda application version can change even in patch releases of the Helm chart. This means that updates to the chart may roll out new versions of Redpanda.

To avoid unexpected changes to your deployments, pin the version of the Helm chart. Pinning refers to the practice of specifying an exact version to use during deployment, rather than using the latest or unspecified version. When you pin the Helm chart version, you maintain consistent, predictable environments, especially in production. Using a specific version helps to:

- *Ensure compatibility*: Guarantee that the deployed application behaves as tested, regardless of new chart versions being released.
- *Avoid unexpected updates*: Prevent automatic updates that may introduce changes incompatible with the current deployment or operational practices.

[source,bash,subs="attributes+"]
----
helm repo add redpanda https://charts.redpanda.com
helm repo update
helm install redpanda redpanda/redpanda \
  --version {latest-redpanda-helm-chart-version} \
  --namespace <namespace> \
  --create-namespace
----

xref:reference:releases/index.adoc[Review the release notes] to understand any significant changes, bug fixes, or potential disruptions that could affect your existing deployment.

=== Name overrides (Helm)

Deploying multiple instances of the same Helm chart in a Kubernetes cluster can lead to naming conflicts. Using `nameOverride` and `fullnameOverride` helps differentiate between them. If you have a production and staging environment for Redpanda, different names help to avoid confusion.

- Use `nameOverride` to customize the labels `app.kubernetes.io/component=<nameOverride>-statefulset` and `app.kubernetes.io/name=<nameOverride>`.
- Use `fullnameOverride` to customize the name of the StatefulSet and Services.

[source,yaml]
----
nameOverride: 'redpanda-production'
fullnameOverride: 'redpanda-instance-prod'
----

=== Labels

Kubernetes labels help you to organize, query, and manage your resources. Use labels to categorize Kubernetes resources in different deployments by environment, purpose, or team.

[source,yaml]
----
commonLabels:
  env: 'production'
----

=== Tolerations

Tolerations and taints allow Pods to be scheduled onto nodes where they otherwise wouldn't. If you have nodes dedicated to Redpanda with a taint `dedicated=redpanda:NoSchedule`, the following toleration allows the Redpanda brokers to be scheduled on them.

[source,yaml]
----
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "redpanda"
  effect: "NoSchedule"
----

=== Docker image

You can specify the image tag to deploy a known version of the Redpanda Docker image. By default, the image tag is set in `Chart.appVersion`. Avoid using the `latest` tag, which can lead to unexpected changes.

If you're using a private repository, always ensure your nodes have the necessary credentials to pull the image.

[source,yaml,subs="attributes+"]
----
image:
  repository: docker.redpanda.com/redpandadata/redpanda
  tag: "{latest-redpanda-tag}"
imagePullSecrets: []
----

=== Number of Redpanda brokers

The number of Redpanda brokers you deploy depends on your use case and the level of redundancy you require. For production, deploy at least three Redpanda brokers. Always deploy an odd number of brokers to avoid split-brain scenarios.

[,yaml]
----
statefulset:
  replicas: 3
----

NOTE: You must provision one dedicated worker node for each Redpanda broker that you plan to deploy in your Redpanda cluster. The default <<affinity-rules, `podAntiAffinity` rules>> make sure that each Redpanda broker runs on its own worker node.

See also:

- xref:./k-high-availability.adoc[]
- xref:./k-requirements.adoc#number-of-worker-nodes[Kubernetes Cluster Requirements]

=== TLS

By default, TLS (Transport Layer Security) is enabled for encrypted communication. Internal (`default`) and external (`external`) self-signed certificates are generated using cert-manager. See <<TLS Certificates>>.

[source,yaml]
----
tls:
  enabled: true
  certs:
    # This key represents the name of the certificate.
    default:
      caEnabled: true
    # This key represents the name of the certificate.
    external:
      caEnabled: true
----

See also: xref:manage:kubernetes/security/kubernetes-tls.adoc[]

=== Authentication

If you want to authenticate clients connections to the Redpanda cluster, you can enable SASL authentication.

[source,yaml]
----
auth:
  sasl:
    enabled: true
    mechanism: "SCRAM-SHA-512"
    secretRef: "sasl-password-secret"
    users: []
----

See also: xref:manage:kubernetes/security/authentication/k-authentication.adoc[]

=== Resources

By default, the resources allocated to Redpanda are for a development environment. In a production cluster, the resources you allocate should be proportionate to your machine type. You should determine and set these values before deploying the cluster.

[source,yaml]
----
resources:
  cpu:
    cores: 4
  memory:
    enable_memory_locking: true
    container:
      max: 10Gi
----

See also:

- xref:manage:kubernetes/k-manage-resources.adoc[]
- xref:deploy:deployment-option/self-hosted/kubernetes/k-requirements.adoc[]

=== Storage

In production, it's best to use local PersistentVolumes (PVs) that are backed by NVMe devices to store the Redpanda data directory. NVMe devices outperform traditional SSDs or HDDs.

Redpanda Data recommends creating StorageClasses that use the https://github.com/metal-stack/csi-driver-lvm[local volume manager (LVM) CSI driver] to automatically provision PVs. The LVM allows you to group physical storage devices into a logical volume group. Allocating logical volumes from a logical volume group provides greater flexibility in terms of storage expansion and management. The LVM supports features such as resizing, snapshots, and striping, which are not available with the other drivers such as the local volume static provisioner.

[,yaml]
----
storage:
  persistentVolume:
    enabled: true
    size: 100Gi
    storageClass: csi-driver-lvm-striped-xfs
----

For an example of configuring local PersistentVolumes backed by NVMe disks, see one of the following guides:

* xref:./aks-guide.adoc#create-sc[Azure Kubernetes Service] (AKS)
* xref:./eks-guide.adoc#create-sc[Elastic Kubernetes Service] (EKS)
* xref:./gke-guide.adoc#create-sc[Google Kubernetes Engine] (GKE)

See also:

- xref:manage:kubernetes/storage/k-volume-types.adoc[]
- xref:./cloud-instance-local-storage.adoc[].
- xref:manage:kubernetes/storage/k-configure-storage.adoc[]

=== External access

To make the Redpanda cluster accessible from outside the Kubernetes cluster, you can use NodePort or LoadBalancer Services.

The default NodePort Service provides the lowest latency of all the Kubernetes Services because it does not include any unnecessary routing or middleware. Client connections go to the Redpanda brokers in the most direct way possible, through the worker nodes.

By default, the fully qualified domain names (FQDNs) that brokers advertise are their internal addresses within the Kubernetes cluster, which are not reachable from outside the cluster. To make the cluster accessible from outside, each broker must advertise a domain that can be reached from outside the cluster.

[,yaml]
----
external:
  enabled: true
  type: NodePort
----

See also:

- xref:manage:kubernetes/networking/k-networking-and-connectivity.adoc[]
- xref:manage:kubernetes/networking/k-configure-listeners.adoc[]

=== ExternalDNS

You should use ExternalDNS to manage DNS records for your Pods' domains. ExternalDNS synchronizes exposed Kubernetes Services with various DNS providers, rendering Kubernetes resources accessible through DNS servers.

Benefits of ExternalDNS include:

* *Automation*: ExternalDNS automatically configures public DNS records when you create, update, or delete Kubernetes Services or Ingresses. This eliminates the need for manual DNS configuration, which can be error-prone.
* *Compatibility*: ExternalDNS is compatible with a wide range of DNS providers, including major cloud providers such as AWS, Google Cloud, and Azure, and DNS servers like CoreDNS and PowerDNS.
* *Integration with other tools*: ExternalDNS can be used with other Kubernetes tools, such as ingress controllers or cert-manager for managing TLS certificates.

[,yaml]
----
external:
  enabled: true
  type: LoadBalancer
  externalDns:
    enabled: true
----

See also:

- xref:manage:kubernetes/networking/external/k-nodeport.adoc#externaldns[ExternalDNS with a NodePort Service]
- xref:manage:kubernetes/networking/external/k-loadbalancer.adoc#externaldns[ExternalDNS with LoadBalancer Services]

=== Logging

By default, the log-level is set to `info`. In production, use the `info` logging level to avoid overwhelming the storage. For debugging purposes, temporarily change the logging level to `debug`.

[source,yaml]
----
logging:
  level: "info"
----

=== Monitoring

By default, monitoring is disabled. If you have the https://prometheus-operator.dev/[Prometheus Operator], enable monitoring to deploy a ServiceMonitor resource for Redpanda. Observability is essential in production environments.

[source,yaml]
----
monitoring:
  enabled: true
----

See also: xref:manage:kubernetes/monitor.adoc[]

=== StatefulSet update strategy

For smooth and uninterrupted updates, use the default `RollingUpdate` strategy. Additionally, set a PodDisruptionBudget to ensure that at least one Pod is available during updates.

[source,yaml]
----
statefulset:
  updateStrategy:
    type: "RollingUpdate"
  budget:
    maxUnavailable: 1
----

See also: xref:upgrade:k-rolling-upgrade.adoc[]

=== Affinity rules

By default, `podAntiAffinity` rules stop the Kubernetes scheduler from placing multiple Redpanda brokers on the same node. These rules offer two benefits:

- Minimize the risk of data loss by ensuring that a node's failure results in the loss of only one Redpanda broker.
- Prevent resource contention between brokers by ensuring they are never co-located on the same node.

Affinities control Pod placement in the cluster based on various conditions. Set these according to your high availability and infrastructure needs. For example, this is a soft rule that tries to ensure the Kubernetes scheduler doesn't place two Pods with the same `app: redpanda` label in the same zone. However, if it's not possible, the scheduler can still place the Pods in the same zone.

[source,yaml,lines=7]
----
statefulset:
  podAntiAffinity:
    topologyKey: kubernetes.io/hostname
    type: hard
    weight: 100
    custom:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: "app"
              operator: "In"
              values:
              - "redpanda"
          topologyKey: "kubernetes.io/zone"
----

See also: xref:./k-high-availability.adoc[]

=== Graceful shutdown

By default, Pods are given 90 seconds to shut down gracefully. If your brokers require additional time for a graceful shutdown, modify the `terminationGracePeriodSeconds`.

[source,yaml]
----
statefulset:
  terminationGracePeriodSeconds: 100
----

See also: xref:upgrade:k-rolling-upgrade.adoc[]

=== Service account

Restricting permissions is a best practice. Create a dedicated ServiceAccount for each Pod. To assign roles to this ServiceAccount, see <<RBAC>>.

[source,yaml]
----
serviceAccount:
  create: true
  name: "redpanda-service-account"
----

[[RBAC]]
=== Role-based access control (RBAC)

RBAC is a method for providing permissions to ServiceAccounts based on roles. Some features such as rack awareness require both a ServiceAccount and RBAC to access resources using the Kubernetes API.

[source,yaml]
----
rbac:
  enabled: true
  annotations: {}
----

NOTE: If you use the Redpanda Operator, you must also deploy the Redpanda Operator Helm chart with `rbac.createRPKBundleCRs` set to `true` to give it the required roles.

See also: xref:manage:kubernetes/k-rack-awareness.adoc[]

include::deploy:partial$self-test.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/default-components.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/uninstall.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/troubleshoot.adoc[leveloffset=+1]

== Next steps

See the xref:manage:kubernetes/index.adoc[Manage Kubernetes topics] to learn how to customize your deployment to meet your needs.

include::shared:partial$suggested-reading.adoc[]

- xref:./k-high-availability.adoc[]
- xref:reference:k-redpanda-helm-spec.adoc[Redpanda Helm Specification]
- xref:reference:k-crd.adoc[Redpanda CRD Reference]
