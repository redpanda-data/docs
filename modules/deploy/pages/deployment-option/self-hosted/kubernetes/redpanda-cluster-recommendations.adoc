= Redpanda Cluster Recommendations
:description: A list of recommendations for deploying the Redpanda Helm chart in production.
:tags: ["Kubernetes"]
:page-aliases: deploy:deployment-option/self-hosted/kubernetes/kubernetes-best-practices.adoc

This topic provides recommendations for deploying Redpanda in production.

== Deploy at least three Pod replicas

Redpanda Data recommends at least three Pod replicas (Redpanda brokers) to use as _seed servers_. Seed servers are used to bootstrap the gossip process for new brokers joining a cluster. When a new broker joins, it connects to the seed servers to find out the topology of the Redpanda cluster. A larger number of seed servers makes consensus more robust and minimizes the chance of unwanted clusters forming when brokers are restarted without any data.

By default, the Redpanda Helm chart deploys a StatefulSet with three Redpanda brokers. You can specify the number of Redpanda brokers in the xref:reference:redpanda-helm-spec.adoc#statefulset-replicas[`statefulset.replicas`] configuration.

== Set resource requests and limits for memory and CPU

In a production cluster, the resources you allocate to Redpanda should be proportionate to your machine type. Redpanda Data recommends that you determine and set these values before deploying the cluster. For instructions on setting Pod resources, see xref:manage:kubernetes/manage-resources.adoc[Manage Pod Resources in Kubernetes].

== Use local PersistentVolumes backed by NVMe disks

Redpanda Data recommends using PersistentVolumes (PVs) that are backed by locally attached NVMe devices to store the Redpanda data directory. NVMe devices outperform traditional SSDs or HDDs.

When working with local NVMe disks, Redpanda Data recommends creating StorageClasses that use the https://github.com/metal-stack/csi-driver-lvm[local volume manager (LVM) CSI driver] to automatically provision PVs. The LVM allows you to group physical storage devices into a logical volume group. Allocating logical volumes from a logical volume group provides greater flexibility in terms of storage expansion and management. The LVM supports features such as resizing, snapshots, and striping, which are not available with the other drivers such as the local volume static provisioner.

By default, the Redpanda Helm chart uses the default StorageClass in your Kubernetes cluster to create one PersistentVolumeClaim (PVC) for each Redpanda broker. To learn how to configure a different StorageClass, see xref:manage:kubernetes/storage/configure-persistent-storage.adoc[].

For details about StorageClasses, see the https://kubernetes.io/docs/concepts/storage/storage-classes/[Kubernetes documentation^].

== Use a NodePort Service for external access

The NodePort Service provides the lowest latency of all the Kubernetes Services because it does not include any unnecessary routing or middleware. Client connections go to the Redpanda brokers in the most direct way possible, through the worker nodes.

By default, the Redpanda Helm chart creates a NodePort Service with the following ports:

|===
| Listener|Node Port

| Schema registry
| 30081

| HTTP Proxy
| 30082

| Kafka API
| 31092

| Admin API
| 31644
|===

To change these ports, see xref:manage:kubernetes/networking/configure-listeners.adoc[].

Depending on your deployment and security policies, you may not be able to access worker nodes through a NodePort Service.
If you choose to use another Service, consider the impact on the cost and performance of your deployment:

* *LoadBalancer Service*: To make each Redpanda broker accessible with LoadBalancer Services, you need one LoadBalancer Service for each Redpanda broker so that requests can be routed to specific brokers rather than balancing requests across all brokers. Load balancers are expensive, add latency and occasional packet loss, and add an unnecessary layer of complexity.
* *Ingress*: To make each Redpanda broker accessible with Ingress, you must run an Ingress controller and set up routing to each Redpanda broker. Routing adds latency and can be a throughput bottleneck.

For more details, see xref:manage:kubernetes/networking/networking-and-connectivity.adoc[Networking and Connectivity].

== Use ExternalDNS for external access

Redpanda Data recommends using ExternalDNS to manage DNS records for your Pods' domains. ExternalDNS synchronizes exposed Kubernetes Services with various DNS providers, rendering Kubernetes resources accessible through DNS servers.

Benefits of ExternalDNS include:

* *Automation*: ExternalDNS automatically configures public DNS records when you create, update, or delete Kubernetes Services or Ingresses. This eliminates the need for manual DNS configuration, which can be error-prone.
* *Compatibility*: ExternalDNS is compatible with a wide range of DNS providers, including major cloud providers such as AWS, Google Cloud, and Azure, and DNS servers like CoreDNS and PowerDNS.
* *Integration with other tools*: ExternalDNS can be used in conjunction with other Kubernetes tools, such as ingress controllers or cert-manager for managing TLS certificates.

You can use ExternalDNS with the default xref:manage:kubernetes/networking/configure-external-access-nodeport.adoc#externaldns[NodePort Service] or with xref:manage:kubernetes/networking/configure-external-access-loadbalancer.adoc#externaldns[LoadBalancer Services].

== Secure your Redpanda cluster

Deploy Redpanda in a separate namespace to protect your data from other resources in your Kubernetes cluster.

To protect your Redpanda cluster, enable and configure the following:

* xref:manage:kubernetes/security/sasl-kubernetes.adoc[Authentication through SASL]
* xref:manage:kubernetes/security/kubernetes-tls.adoc[Network encryption through TLS]

By default, the Redpanda Helm chart enables TLS using cert-manager.

include::shared:partial$suggested-reading.adoc[]

- xref:reference:redpanda-helm-spec.adoc#rackawareness[Redpanda Helm Specification]
- xref:reference:crd.adoc[Redpanda CRD Reference]

== Next steps

- xref:./kubernetes-tune-workers.adoc[].
- xref:manage:kubernetes/index.adoc[].