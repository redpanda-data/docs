= Deploy a Redpanda Cluster in Amazon Elastic Kubernetes Service
:description: Deploy a secure Redpanda cluster and Redpanda Console in Amazon Elastic Kubernetes Service (EKS).

Deploy a secure Redpanda cluster and Redpanda Console in Amazon Elastic Kubernetes Service (EKS).
Then, use rpk both as an internal client and an external client to interact with your Redpanda cluster from the command-line. Your Redpanda cluster will have the following security features:

* SASL for authenticating users' connections.
* TLS with self-signed certificates for secure communication between the cluster and clients.

== Prerequisites

Before you begin, you must have the following prerequisites.

=== IAM user

You need an IAM user with at least the following policies. See the AWS documentation for help https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html[creating IAM users^] or for help https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot.html[troubleshooting IAM^].

.Policies
[%collapsible]
====
Replace `<account-id>` with your own https://console.aws.amazon.com/iamv2/home[account ID^].

.AmazonEC2FullAccess
[,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
     "Action": "ec2:*",
     "Effect": "Allow",
     "Resource": "*"
    },
    {
     "Effect": "Allow",
     "Action": "elasticloadbalancing:*",
     "Resource": "*"
    },
    {
     "Effect": "Allow",
     "Action": "cloudwatch:*",
     "Resource": "*"
    },
    {
     "Effect": "Allow",
     "Action": "autoscaling:*",
     "Resource": "*"
    },
    {
     "Effect": "Allow",
     "Action": "iam:CreateServiceLinkedRole",
     "Resource": "*",
     "Condition": {
      "StringEquals": {
        "iam:AWSServiceName": [
           "autoscaling.amazonaws.com",
           "ec2scheduled.amazonaws.com",
           "elasticloadbalancing.amazonaws.com",
           "spot.amazonaws.com",
           "spotfleet.amazonaws.com",
           "transitgateway.amazonaws.com"
        ]
      }
     }
    }
  ]
}
----

.AWSCloudFormationFullAccess
[,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
     "Effect": "Allow",
     "Action": [
      "cloudformation:*"
     ],
     "Resource": "*"
    }
  ]
}
----

.EksAllAccess
[,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
     "Effect": "Allow",
     "Action": "eks:*",
     "Resource": "*"
    },
    {
     "Action": [
      "ssm:GetParameter",
      "ssm:GetParameters"
     ],
     "Resource": [
      "arn:aws:ssm:*:<account-id>:parameter/aws/*",
      "arn:aws:ssm:*::parameter/aws/*"
     ],
     "Effect": "Allow"
    },
    {
     "Action": [
       "kms:CreateGrant",
       "kms:DescribeKey"
     ],
     "Resource": "*",
     "Effect": "Allow"
    },
    {
     "Action": [
       "logs:PutRetentionPolicy"
     ],
     "Resource": "*",
     "Effect": "Allow"
    }
  ]
}
----

.IamLimitedAccess
[,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
     "Effect": "Allow",
     "Action": [
      "iam:CreateInstanceProfile",
      "iam:DeleteInstanceProfile",
      "iam:GetInstanceProfile",
      "iam:RemoveRoleFromInstanceProfile",
      "iam:GetRole",
      "iam:CreateRole",
      "iam:DeleteRole",
      "iam:AttachRolePolicy",
      "iam:PutRolePolicy",
      "iam:ListInstanceProfiles",
      "iam:AddRoleToInstanceProfile",
      "iam:ListInstanceProfilesForRole",
      "iam:PassRole",
      "iam:DetachRolePolicy",
      "iam:DeleteRolePolicy",
      "iam:GetRolePolicy",
      "iam:GetOpenIDConnectProvider",
      "iam:CreateOpenIDConnectProvider",
      "iam:DeleteOpenIDConnectProvider",
      "iam:TagOpenIDConnectProvider",
      "iam:ListAttachedRolePolicies",
      "iam:TagRole",
      "iam:GetPolicy",
      "iam:CreatePolicy",
      "iam:DeletePolicy",
      "iam:ListPolicyVersions"
     ],
     "Resource": [
      "arn:aws:iam::<account-id>:instance-profile/eksctl-*",
      "arn:aws:iam::<account-id>:role/eksctl-*",
      "arn:aws:iam::<account-id>:policy/eksctl-*",
      "arn:aws:iam::<account-id>:oidc-provider/*",
      "arn:aws:iam::<account-id>:role/aws-service-role/eks-nodegroup.amazonaws.com/AWSServiceRoleForAmazonEKSNodegroup",
      "arn:aws:iam::<account-id>:role/eksctl-managed-*",
      "arn:aws:iam::<account-id>:role/AmazonEKS_EBS_CSI_DriverRole"
     ]
    },
    {
     "Effect": "Allow",
     "Action": [
      "iam:GetRole"
     ],
     "Resource": [
      "arn:aws:iam::<account-id>:role/*"
     ]
    },
    {
     "Effect": "Allow",
     "Action": [
      "iam:CreateServiceLinkedRole"
     ],
     "Resource": "*",
     "Condition": {
      "StringEquals": {
        "iam:AWSServiceName": [
           "eks.amazonaws.com",
           "eks-nodegroup.amazonaws.com",
           "eks-fargate.amazonaws.com"
        ]
      }
     }
    }
  ]
}
----
====

==== AWS CLI

You need the https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[AWS CLI^] to configure `kubeconfig` and get information about your EC2 instances.

After you've installed the AWS CLI, make sure to https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html[configure it^] with credentials for your IAM user.

NOTE: If your account uses an identity provider in the IAM Identity Center (previously https://aws.amazon.com/about-aws/whats-new/2022/07/aws-single-sign-on-aws-sso-now-aws-iam-identity-center/[AWS SSO^]), https://docs.aws.amazon.com/cli/latest/userguide/sso-configure-profile-token.html[authenticate with the IAM Identity Center^] (`aws sso login`).

For troubleshooting, see the https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#install-tshoot[AWS CLI documentation^].

=== eksctl

You need https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html[`eksctl`^] to create an EKS cluster from the command-line.

=== jq

You need https://stedolan.github.io/jq/download/[jq^] to parse JSON results and store the value in environment variables..

=== kubectl

You must have https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html[`kubectl`^] with the following minimum required Kubernetes version: {supported-kubernetes-version}

To check if you have `kubectl` installed:

```bash
kubectl version --short --client
```

=== Helm

You must have the following minimum required version of https://helm.sh/docs/intro/install/[Helm^]: {supported-helm-version}

To check if you have Helm installed:

```bash
helm version
```

== Create an EKS cluster

Your EKS cluster must have one worker node available for each Redpanda broker that you plan to deploy in your Redpanda cluster. You also need to run the worker nodes on an EC2 instance type that supports the xref:./kubernetes-cluster-requirements.adoc[requirements and recommendations] for production deployments.

In this step, you create an EKS cluster with three nodes on https://aws.amazon.com/ec2/instance-types/c5/[`c5d.2xlarge` instance types]. Deploying three nodes allows your EKS cluster to support a Redpanda cluster with three brokers. The `c5d.2xlarge` instance type comes with:

- 2 cores per worker node, which is a requirement for production.
- Local NVMe disks, which is recommended for best performance.

NOTE: The Helm chart configures `podAntiAffinity` rules to make sure that only one Pod running a Redpanda broker is scheduled on each worker node.

. Create an EKS cluster and give it a unique name. If your account is configured with OIDC, add the `--with-oidc` flag to the `create cluster` command.
+
[,bash,lines=4-6]
----
eksctl create cluster \
  --name <cluster-name> \
  --nodegroup-name nvme-workers \
  --node-type c5d.2xlarge \
  --nodes 3 \
  --external-dns-access
----
+
[TIP]
====
To see all options:

```bash
eksctl create cluster --help
```

Or, for help creating an EKS cluster, see the https://eksctl.io/usage/creating-and-managing-clusters/[Creating and managing clusters^] in the `eksctl` documentation.
====

. Make sure that your local `kubeconfig` file points to your EKS cluster:
+
```bash
kubectl get service
```
+
You should see the a ClusterIP Service called `kubernetes`.
+
If the `kubectl` command cannot connect to your cluster, update your local `kubeconfig` file to point to your EKS cluster.
+
Your default region is in the `~/.aws/credentials` file.
+
```bash
aws eks update-kubeconfig --region <region> --name <cluster-name>
```

=== Create a StorageClass for your local NVMe disks

When you provisioned the EKS cluster, you selected an instance type that comes with local NVMe disks. However, these disks are not automatically mounted or formatted upon creation. To use these local NVMe disks, you must mount and format them, and you must create the necessary PersistentVolumes (PVs). To automate this process, you can use a Container Storage Interface (CSI) driver.

In this step, you install the recommended https://github.com/metal-stack/csi-driver-lvm[local volume manager (LVM) CSI driver]. Then, you create a StorageClass that references the LVM CSI driver and specifies the recommended XFS filesystem.

. Install the LVM CSI driver:
+
[,yaml,lines=6]
----
helm repo add metal-stack https://helm.metal-stack.io
helm repo update
helm install csi-driver-lvm metal-stack/csi-driver-lvm \
  --namespace csi-driver-lvm \
  --create-namespace \
  --set lvm.devicePattern='/dev/nvme[1-9]n[0-9]'
----
+
The `lvm.devicePattern` property specifies the pattern that the CSI driver uses to identify available NVMe volumes on your worker nodes.

. Create the StorageClass:
+
.`csi-driver-lvm-linear-xfs.yaml`
[,yaml,lines=5-8+10-11]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-driver-lvm-linear-xfs
provisioner: lvm.csi.metal-stack.io
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: "linear"
  csi.storage.k8s.io/fstype: xfs
----
+
- `provisioner`: The LVM CSI driver responsible for provisioning the volume.
- `reclaimPolicy`: The `Retain` policy ensures that the underlying volume is not deleted when the corresponding PVC is deleted.
- `volumeBindingMode`: The `WaitForFirstConsumer` mode delays the binding and provisioning of a PersistentVolume until a Pod that uses the PVC is created. This mode is important for ensuring that the PV is created on the same node where the Pod will run because the PV will use the node's local NVMe volumes.
- `allowVolumeExpansion`: Allows the volume to be expanded after it has been provisioned.
- `parameters.type`: Combines multiple physical volumes to create a single logical volume. In a linear setup, data is written sequentially to the first physical volume until it is full, then to the next, and so on.
- `parameters.csi.storage.k8s.io/fstype`: Formats the volumes with the XFS filesystem. Redpanda Data recommends XFS for its enhanced performance with Redpanda workloads.

. Apply the StorageClass:
+
[,bash]
----
kubectl apply -f csi-driver-lvm-linear-xfs.yaml
----
+
After applying this StorageClass, any PVCs that references it will attempt to provision storage using the LVM CSI driver and the provided parameters.

=== Configure external access

In this step, you configure your EKS cluster to allow external access to the node ports on which the Redpanda deployment will be exposed. You'll use these node ports in later steps to configure external access to your Redpanda cluster.

. Get the ID of the security group that's associated with the nodes in your EKS cluster:
+
```bash
AWS_SECURITY_GROUP_ID=`aws eks describe-cluster --name <cluster-name> | jq -r '.cluster.resourcesVpcConfig.clusterSecurityGroupId'`
```

. Add inbound firewall rules to your EC2 instances so that external traffic can reach the node ports exposed on all Kubernetes worker nodes in the cluster:
+
```bash
aws ec2 authorize-security-group-ingress \
  --group-id ${AWS_SECURITY_GROUP_ID} \
  --ip-permissions '[
  {
    "IpProtocol": "tcp",
    "FromPort": 30081,
    "ToPort": 30081,
    "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
  },
  {
    "IpProtocol": "tcp",
    "FromPort": 30082,
    "ToPort": 30082,
    "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
  },
  {
    "IpProtocol": "tcp",
    "FromPort": 31644,
    "ToPort": 31644,
    "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
  },
  {
    "IpProtocol": "tcp",
    "FromPort": 31092,
    "ToPort": 31092,
    "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
  }
  ]'
```
+
CAUTION: If you use `0.0.0.0/0`, you enable all IPv4 addresses to access your instances on those node ports. In production, you should authorize only a specific IP address or range of addresses to access your instances.
+
For help creating firewall rules, see the https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html[Amazon EC2 documentation^].

include::deploy:partial$kubernetes/guides/deploy-redpanda.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/verify-deployment.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/create-user.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/start-streaming.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/explore-topics-localhost.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/external-access-intro.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/external-access-steps.adoc[]

include::deploy:partial$kubernetes/default-components.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/troubleshoot.adoc[leveloffset=+1]

include::deploy:partial$kubernetes/guides/next-steps.adoc[leveloffset=+1]

include::shared:partial$suggested-reading.adoc[]

include::deploy:partial$kubernetes/guides/suggested-reading-content.adoc[leveloffset=+1]
