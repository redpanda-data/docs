= Kubernetes Cluster Requirements and Recommendations
:description: A list of requirements and recommendations for provisioning Kubernetes clusters and worker nodes for running Redpanda in production.
:tags: ["Kubernetes"]

This topic provides the requirements and recommendations for provisioning Kubernetes clusters and worker nodes for running Redpanda in production.

== Operating system

- Minimum recommended version of RHEL/CentOS: {supported-rhel-version}

- Minimum version of Ubuntu: {supported-ubuntu-version}

== Kubernetes version

Minimum required Kubernetes version: {supported-kubernetes-version}

Make sure to do the following:

. https://kubernetes.io/docs/tasks/tools/[Install kubectl^].
. https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[Configure the `kubeconfig` file for your cluster^].

== Helm version

Minimum required Helm version: {supported-helm-version}

https://helm.sh/docs/intro/install/[Install Helm^].

== Number of worker nodes

Provision one dedicated worker node for each Redpanda broker that you plan to deploy in your Redpanda cluster.
Each Pod replica that runs a Redpanda broker requires its own dedicated worker node for the following reasons:

- *Resource isolation*: Redpanda brokers are designed to make full use of available system resources, including CPU and memory. By dedicating a worker node to each broker, you ensure that these resources aren't shared with other applications or processes, avoiding potential performance bottlenecks or contention.
- *External networking*: External clients should connect directly to the broker that owns the partition they're interested in. This means that each broker must be individually addressable. As clients must connect to the specific broker that is the leader of the partition, they need a mechanism to directly address each broker in the cluster. Assigning each broker to its own dedicated worker node makes this direct addressing feasible, since each worker node will have a unique address. See <<External networking>>.
- *Fault tolerance*: Ensuring each broker operates on a separate node enhances fault tolerance. If one node experiences issues, it won't directly impact the other brokers.

NOTE: The Redpanda Helm chart configures xref:reference:redpanda-helm-spec.adoc#statefulset-podantiaffinity[`podAntiAffinity` rules] to make sure that each Redpanda broker runs on its own worker node.

*Recommendations*: xref:./redpanda-cluster-recommendations.adoc#deploy-at-least-three-pod-replicas[Deploy at least three Pod replicas].

== CPU

- Two physical, not virtual, cores for each worker node.

- x86_64 (Westmere or newer) and AWS Graviton family processors are supported.

*Recommendations*:

- Four physical cores for each worker node are strongly recommended.

- xref:./redpanda-cluster-recommendations.adoc#set-resource-requests-and-limits-for-memory-and-cpu[Set resource requests and limits for memory and CPU].

== Memory

2 GB or more of memory per core.

== Storage

- An XFS or ext4 file system.
+
The Redpanda data directory (`/var/lib/redpanda/data`) and the Tiered Storage cache must be mounted on an XFS or ext4 file system. For information about supported volume types for different data in Redpanda, see xref:manage:kubernetes/storage/volume-types.adoc[].
+
CAUTION: Avoid using NFS (Network File System) for the Redpanda data directory or the Tiered Storage cache.

- A default StorageClass that can provision PersistentVolumes with at least 20Gi of storage.
+
By default, the Redpanda Helm chart uses the default StorageClass in your Kubernetes cluster to create one PersistentVolumeClaim (PVC) for each Redpanda broker. The default PVC size is 20Gi. To learn how to configure a different StorageClass, see xref:manage:kubernetes/storage/configure-persistent-storage.adoc[].

*Recommendations*: xref:./redpanda-cluster-recommendations.adoc#use-local-persistentvolumes-backed-by-nvme-disks[Use local PersistentVolumes backed by NVMe disks].

*Recommendations*:

- Use an XFS file system for its enhanced performance with Redpanda workloads.

- For setups with multiple disks, use a RAID-0 (striped) array. It boosts speed but lacks redundancy. A disk failure can lead to data loss.

== Security

*Recommendations*:

- If you're using a cloud platform, use xref:manage:security/iam-roles.adoc[IAM roles] to restrict access to resources in your cluster.

- xref:./redpanda-cluster-recommendations.adoc#secure-your-redpanda-cluster[Secure your Redpanda cluster].

== External networking

- For external access, each worker node in your cluster must have a static, externally accessible IP address.

- Minimum 10 GigE (10 Gigabit Ethernet) connection to ensure:

* High data throughput
* Reduced data transfer latency
* Scalability for increased network traffic

*Recommendations*: xref:./redpanda-cluster-recommendations.adoc#use-a-nodeport-service-for-external-access[Use a NodePort Service for external access].

== Tuning

Before deploying Redpanda to production, each worker node that runs Redpanda must be tuned to optimize the Linux kernel for Redpanda processes. See xref:./kubernetes-tune-workers.adoc[Tuning Kubernetes Worker Nodes for Production].

== Object storage providers for Tiered Storage

Redpanda supports the following storage providers for Tiered Storage:

- Amazon Simple Storage Service (S3)
- Google Cloud Storage (GCS), using the Google Cloud Platform S3 API
- Azure Blob Storage (ABS)

include::shared:partial$suggested-reading.adoc[]

- xref:./redpanda-cluster-recommendations.adoc[]
- xref:reference:redpanda-helm-spec.adoc[Redpanda Helm Specification]
- xref:reference:crd.adoc[Redpanda CRD Reference]

== Next steps

xref:./kubernetes-deploy.adoc[].
