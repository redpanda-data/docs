:node: node
ifdef::env-kubernetes[]
:node: worker node
endif::[]

== Operating system

- Minimum version required of RHEL/CentOS: {supported-rhel-required}. *Recommended*: {supported-rhel-recommended}

- Minimum version required of Ubuntu: {supported-ubuntu-required}. *Recommended*: {supported-ubuntu-recommended}

NOTE: Enabling SELinux (Security-Enhanced Linux) can result in latency issues. If you wish to avoid such latency issues, do not use this mechanism.

*Recommendation*: Linux kernel 4.19 or later for better performance.

ifdef::env-kubernetes[]
== Kubernetes version

Minimum required Kubernetes version: {supported-kubernetes-version}

Make sure to do the following:

. https://kubernetes.io/docs/tasks/tools/[Install kubectl^].
. https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[Configure the `kubeconfig` file for your cluster^].

== Helm version

Minimum required Helm version: {supported-helm-version}

https://helm.sh/docs/intro/install/[Install Helm^].
endif::[]

[[number-of-workers]]
== Number of nodes

Provision one physical node or virtual machine (VM) for each Redpanda broker that you plan to deploy in your Redpanda cluster.
Each Redpanda broker requires its own dedicated node for the following reasons:

- *Resource isolation*: Redpanda brokers are designed to make full use of available system resources, including CPU and memory. By dedicating a node to each broker, you ensure that these resources aren't shared with other applications or processes, avoiding potential performance bottlenecks or contention.
- *External networking*: External clients should connect directly to the broker that owns the partition they're interested in. This means that each broker must be individually addressable. As clients must connect to the specific broker that is the leader of the partition, they need a mechanism to directly address each broker in the cluster. Assigning each broker to its own dedicated node makes this direct addressing feasible, since each node will have a unique address. See <<External networking>>.
- *Fault tolerance*: Ensuring each broker operates on a separate node enhances fault tolerance. If one node experiences issues, it won't directly impact the other brokers.

ifdef::env-kubernetes[]
NOTE: The Redpanda Helm chart configures xref:reference:k-redpanda-helm-spec.adoc#statefulset-podantiaffinity[`podAntiAffinity` rules] to make sure that each Redpanda broker runs on its own node.


*Recommendations*: xref:./kubernetes-deploy.adoc#pod-replicas[Deploy at least three Pod replicas].
endif::[]

ifndef::env-kubernetes[]
*Recommendations*: Deploy at least three Redpanda brokers.
endif::[]

[[node-updates]]
== Prevent automatic node upgrades

Ensure that node and operating system (OS) upgrades are manually managed when running Redpanda in production. Manual control avoids unplanned reboots or replacements that disrupt Redpanda brokers, causing service downtime, data loss, or quorum instability.

Common issues with automatic node upgrades include:

- Hard timeouts for graceful shutdowns that do not allow Redpanda brokers enough time to complete decommissioning or leadership transitions.
- Replacements or reboots without ensuring data has been safely migrated or replicated, risking data loss.
- Parallel upgrades across multiple nodes, which can disrupt quorum or reduce cluster availability.

*Requirements*:

- Disable automatic node maintenance or upgrades.
ifdef::env-kubernetes[]
To prevent managed Kubernetes services from automatically rebooting or upgrading nodes:
** **Azure AKS**: https://learn.microsoft.com/en-us/azure/aks/auto-upgrade-node-os-image[Set the OS upgrade channel to `None`^].
** **Google GKE**: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades[Disable GKE auto-upgrades for node pools^].
** **Amazon EKS**: https://docs.aws.amazon.com/eks/latest/userguide/automode.html[Disable EKS node auto-upgrades^].

See also: xref:upgrade:k-upgrade-kubernetes.adoc[How to manually manage node upgrades].
endif::[]

== CPU and memory

*Requirements*:

- Each node must have at least two physical CPU cores.
- x86_64 (Westmere or newer) and AWS Graviton processors are supported.
ifdef::env-kubernetes[]
- Each Redpanda Pod requires at least 2 GiB of memory per core.
+
** Request a minimum of 2.22 GiB per core to meet Redpanda's memory allocation strategy.
+
See xref:manage:kubernetes/k-manage-resources.adoc[] for detailed guidance and examples.
endif::[]

- Each Redpanda broker must have at least 2 GB of memory per core.

- Each Redpanda broker must have at least 2 MB of memory for each topic partition replica.
+
The total memory available for partition replicas is determined as a percentage of the cluster's total memory, which is controlled by the xref:reference:tunable-properties.adoc#topic_partitions_memory_allocation_percent[`topic_partitions_memory_allocation_percent`] setting. Each partition replica consumes xref:reference:tunable-properties.adoc#topic_memory_per_partition[`topic_memory_per_partition`] bytes from this pool. If insufficient memory is available, topic operations will fail. You can adjust the allocation ratio using `topic_partitions_memory_allocation_percent`, but doing so is not recommended, as lowering it may lead to instability or degraded performance.

*Recommendations*:

- Four physical cores for each node are strongly recommended.

ifdef::env-kubernetes[]
== Pod resource configuration

To ensure stable performance and predictable scheduling in Kubernetes, configure Redpanda Pods with appropriate CPU and memory requests and limits:

* Set `resources.requests.memory` and `resources.limits.memory` to the same value.
** Request at least 2.22 GiB of memory per core to meet Redpanda's heap and overhead requirements.
* Set `resources.cpu.cores` to an even integer (for example, `4`, `6`, or `8`) to align with the Kubernetes static CPU manager policy.
* Match CPU and memory resource settings for all containers in the Pod, including init containers and sidecars, to receive the `Guaranteed` QoS class.
* Enable memory locking with the `--lock-memory` flag to prevent paging and improve performance.

This configuration:

* Grants Redpanda exclusive access to CPU cores and memory
* Reduces the risk of throttling, eviction, and OOM kills
* Provides predictable and isolated runtime performance

See xref:manage:kubernetes/k-manage-resources.adoc[Manage Pod Resources in Kubernetes] for configuration examples using both Helm and the Redpanda Operator.
endif::[]

== Storage

*Requirements*:

- An XFS or ext4 file system.
+
The Redpanda data directory (`/var/lib/redpanda/data`) and the Tiered Storage cache must be mounted on an XFS or ext4 file system.
ifdef::env-kubernetes[]
+
For information about supported volume types for different data in Redpanda, see xref:manage:kubernetes/storage/k-volume-types.adoc[].
endif::[]
+
CAUTION: The Network File System (NFS) is unsupported for use as the storage mechanism for the Redpanda data directory or for the Tiered Storage cache.

ifdef::env-kubernetes[- A default StorageClass that can provision PersistentVolumes with at least 20Gi of storage.]

*Recommendations*:

- Use an XFS file system for its enhanced performance with Redpanda workloads.

- For setups with multiple disks, use a RAID-0 (striped) array. It boosts speed but lacks redundancy. A disk failure can lead to data loss.
ifdef::env-kubernetes[]
- xref:./kubernetes-deploy.adoc#storage[Use local PersistentVolumes backed by NVMe disks].
endif::[]

== Security

*Recommendations*:

- If you're using a cloud platform, use xref:manage:security/iam-roles.adoc[IAM roles] to restrict access to resources in your cluster.

- Secure your Redpanda cluster with TLS encryption and SASL authentication.

== External networking

- For external access, each node in your cluster must have a static, externally accessible IP address.

- Minimum 10 GigE (10 Gigabit Ethernet) connection to ensure:

* High data throughput
* Reduced data transfer latency
* Scalability for increased network traffic

ifdef::env-kubernetes[]
*Recommendations*: xref:deploy:deployment-option/self-hosted/kubernetes/kubernetes-deploy.adoc#external-access[Use a NodePort Service for external access].
endif::[]

== Tuning

Before deploying Redpanda to production, each node that runs Redpanda must be tuned to optimize the Linux kernel for Redpanda processes.

ifdef::env-kubernetes[]
See xref:deploy:deployment-option/self-hosted/kubernetes/k-tune-workers.adoc[].
endif::[]
ifndef::env-kubernetes[]
See xref:deploy:deployment-option/self-hosted/manual/production/production-deployment.adoc[].
endif::[]

== Object storage providers for Tiered Storage

Redpanda supports the following storage providers for Tiered Storage:

- Amazon Simple Storage Service (S3)
- Google Cloud Storage (GCS), using the Google Cloud Platform S3 API
- Azure Blob Storage (ABS)

== Cloud instance types

*Recommendations*:

- Use a cloud instance type that supports locally attached NVMe devices with an XFS file system. NVMe devices offer high I/O operations per second (IOPS) and minimal latency, while XFS offers enhanced performance with Redpanda workloads.

=== Amazon

ifdef::env-kubernetes[EKS defaults to the ext4 file system. Use XFS instead where possible.]

- General purpose: General-purpose instances provide a balance of compute, memory, and networking resources, and they can be used for a variety of diverse workloads.
+
[.two-column]
** https://aws.amazon.com/ec2/instance-types/m5/[M5d^]
** https://aws.amazon.com/ec2/instance-types/m5/[M5ad^]
** https://aws.amazon.com/ec2/instance-types/m5/[M5dn^]
** https://aws.amazon.com/ec2/instance-types/m6g/[M6gd^]
** https://aws.amazon.com/ec2/instance-types/m7g/[M7gd^]

- Memory optimized: Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.
+
[.two-column]
** https://aws.amazon.com/ec2/instance-types/r5/[R5ad^]
** https://aws.amazon.com/ec2/instance-types/r5/[R5d^]
** https://aws.amazon.com/ec2/instance-types/r5/[R5dn^]
** https://aws.amazon.com/ec2/instance-types/r6g/[R6gd^]
** https://aws.amazon.com/ec2/instance-types/r6i/[R6id^]
** https://aws.amazon.com/ec2/instance-types/r6i/[R6idn^]
** https://aws.amazon.com/ec2/instance-types/r7g/[R7gd^]
** https://aws.amazon.com/ec2/instance-types/x2/[X2gd^]
** https://aws.amazon.com/ec2/instance-types/x2i/[X2idn^]
** https://aws.amazon.com/ec2/instance-types/x2i/[X2iedn^]
** https://aws.amazon.com/ec2/instance-types/z1d/[z1d^]

- Storage optimized: Storage-optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency, random IOPS to applications.

** https://aws.amazon.com/ec2/instance-types/i4g/[I4g, Is4gen, Im4gn^]
** https://aws.amazon.com/ec2/instance-types/i4i/[I4i^]
** https://aws.amazon.com/ec2/instance-types/i3/[I3^]
** https://aws.amazon.com/ec2/instance-types/i3en/[I3en^]

- Compute optimized: Compute-optimized instances deliver cost-effective high performance at a low price per compute ratio for running advanced compute-intensive workloads.

** https://aws.amazon.com/ec2/instance-types/c5/[C5d^]
** https://aws.amazon.com/ec2/instance-types/c5/[C5ad^]

=== Azure

ifdef::env-kubernetes[AKS often defaults to the ext4 file system. Use XFS instead where possible.]

- General purpose: General purpose VM sizes provide balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers.	

** https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/general-purpose/ddv5-series?tabs=sizebasic[Standard_D2d_v5^]
** https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/general-purpose/ddv5-series?tabs=sizebasic[Standard_D4d_v5^]	
** https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/general-purpose/ddv5-series?tabs=sizebasic[Standard_D32d_v5^]	

=== Google

ifdef::env-kubernetes[GKE often defaults to the ext4 file system. Use XFS instead where possible.]

- General purpose: The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads.

** https://cloud.google.com/compute/docs/general-purpose-machines#c3-with-local-ssd[C3 machine series with local SSD^]
** https://cloud.google.com/compute/docs/general-purpose-machines#n2_series[N2 machine series^]
** https://cloud.google.com/compute/docs/general-purpose-machines#n2d_machines[N2D machine series^]

- Memory optimized: The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose N1 machine series.

** https://cloud.google.com/compute/docs/memory-optimized-machines#m3_series[M3 machine series^]

- Compute optimized: Compute-optimized VM instances are ideal for compute-intensive and high-performance computing (HPC) workloads.

** https://cloud.google.com/compute/docs/compute-optimized-machines#c2d_series[C2D machine series^]
** https://cloud.google.com/compute/docs/compute-optimized-machines#c2_machine_types[C2 machine series^]
