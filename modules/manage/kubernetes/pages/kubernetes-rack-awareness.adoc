= Enable Rack Awareness in Kubernetes
:description: Enable rack awareness to place partition replicas across different failure zones.
:contextLinks: [{"name"=>"Linux", "to"=>"manage/rack-awareness"}, {"name"=>"Kubernetes", "to"=>"manage/kubernetes/kubernetes-rack-awareness"}]
:deployment: Kubernetes
:description: Enable rack awareness to place partition replicas across different failure zones.
:linkRoot: ../../../
:tags: ["Kubernetes", "Helm configuration"]

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import AboutRackAwareness from '../../manage/shared/_rack-awareness.mdx';
import ValuesYaml from './shared/_values-yaml.mdx'

import ContextLink from '@site/src/components/ContextButton.js'

<ContextLink frontmatter=\{frontMatter}
/>+++<AboutRackAwareness>++++++</AboutRackAwareness>+++

When you enable rack awareness in the Redpanda Helm chart, Kubernetes failure zones are treated as racks. Redpanda maps each rack to a failure zone and places partition replicas across them. For more details about Kubernetes failure zones, see the https://kubernetes.io/docs/setup/best-practices/multiple-zones/[Kubernetes documentation].

== Prerequisites

Make sure that each of your Node resources is annotated with a key/value pair that corresponds to a failure zone. The Helm chart assigns each Redpanda broker to a particular rack, according to the failure zone of the Kubernetes node on which the broker is running.

////
[tabs]
=====
Managed::
+
--
Managed Kubernetes platforms in the cloud usually annotate Node resources with the availability zone in which the node instance is hosted. For example `topology.kubernetes.io/zone=us-east-1a`.

To check the value of the `topology.kubernetes.io/zone` key, run the following:

[,bash]
----
kubectl get node \
-o=custom-columns=NODE:.metadata.name,ZONE:.metadata.annotations."topology\.kubernetes\.io/zone"
----+++<details>++++++<summary>+++Example output+++</summary>+++ ``` NODE ZONE example-worker us-east-1a example-worker2 us-east-1b example-worker3 us-east-1c ``` If you don't see any values in the Zone column, see the **Bare-metal** tab.+++</details>+++

--
Bare-metal::
+
--
Make sure to annotate your Node resources with key/value pairs that correspond to your fault-tolerance requirements. For example:

[,bash]
----
kubectl annotate node example-worker topology.kubernetes.io/zone=rack1
kubectl annotate node example-worker2 topology.kubernetes.io/zone=rack2
kubectl annotate node example-worker3 topology.kubernetes.io/zone=rack3
----

--
=====
////

== Configure rack awareness

To enable rack awareness in your Redpanda cluster, you need to override the default configuration in the Helm chart depending on the key you used to annotate Node resources with the availability zone.

=== topology.kubernetes.io/zone annotations

By default, the Helm chart sets the rack to the value in the https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone[well-known] `topology.kubernetes.io/zone` key.

If your Node resources are annotated with the `topology.kubernetes.io/zone` key, do the following:+++<Tabs groupId="helm-config">++++++<TabItem value="values" label="--values">+++```yaml title="rack-awareness.yaml" rackAwareness: enabled: true serviceAccount: create: true rbac: enabled: true ``` ```bash helm upgrade --install redpanda redpanda/redpanda -n redpanda --create-namespace \ --values rack-awareness.yaml --reuse-values ```+++</TabItem>+++ +++<TabItem value="flags" label="--set">+++```bash helm upgrade --install redpanda redpanda/redpanda -n redpanda --create-namespace \ --set rackAwareness.enabled=true \ --set serviceAccount.create=true \ --set rbac.enabled=true ```+++</TabItem>++++++</Tabs>+++

:::note+++<ValuesYaml path="rackAwareness">++++++</ValuesYaml>+++

:::

:::note
The `serviceAccount` and `rbac` configurations are required. These configurations allow the initialization container to securely read the node annotations using the Kubernetes API.
:::

=== Other annotations

If your Node resources are annotated with a different key, set `rackAwareness.nodeAnnotation` to match your annotation key.+++<Tabs groupId="helm-config">++++++<TabItem value="values" label="--values">+++```yaml title="rack-awareness.yaml" rackAwareness: enabled: true nodeAnnotation: 'example' serviceAccount: create: true rbac: enabled: true ``` ```bash helm upgrade --install redpanda redpanda/redpanda -n redpanda --create-namespace \ --values rack-awareness.yaml --reuse-values ```+++</TabItem>+++ +++<TabItem value="flags" label="--set">+++```bash helm upgrade --install redpanda redpanda/redpanda -n redpanda --create-namespace \ --set rackAwareness.enabled=true \ --set rackAwareness.nodeAnnotation='example' \ --set serviceAccount.create=true \ --set rbac.enabled=true ```+++</TabItem>++++++</Tabs>+++

:::note+++<ValuesYaml path="rackAwareness">++++++</ValuesYaml>+++

:::

== Verify that rack awareness is enabled

After deploying Redpanda, make sure that rack awareness is enabled and configured on your Redpanda brokers.

. Make sure that your Node resources include the correct annotations. For example, to check the value of the `topology.kubernetes.io/zone` key, run the following:
+
[,bash]
----
 kubectl get node \
 -o=custom-columns=NODE:.metadata.name,ZONE:.metadata.annotations."topology\.kubernetes\.io/zone"
----+++<details>++++++<summary>+++Example output+++</summary>+++ ``` NODE ZONE example-worker us-east-1a example-worker2 us-east-1b example-worker3 us-east-1c ```+++</details>+++

. Make sure that rack awareness has been enabled and configured on your Redpanda brokers:
+
[,bash]
----
 kubectl -ti -n redpanda exec redpanda-0 -c redpanda -- \
   grep 'rack' etc/redpanda/redpanda.yaml
----+++<details>++++++<summary>+++Example output+++</summary>+++ ``` rack: us-east-1a enable_rack_awareness: true ```+++</details>+++

== Next steps

Use rack awareness with xref:cluster-maintenance:continuous-data-balancing:.adoc[Continuous Data Balancing] to continually maintain the configured replication level, even after a rack failure. For a given partition, Redpanda tries to move excess replicas from racks that have more than one replica to racks that have no replicas.
