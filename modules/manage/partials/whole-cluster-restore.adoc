ifdef::env-kubernetes[]
:link-tiered-storage: manage:kubernetes/storage/tiered-storage/k-tiered-storage.adoc
:link-recovery-mode: manage:kubernetes/k-recovery-mode.adoc
endif::[]
ifndef::env-kubernetes[]
:link-tiered-storage: manage:tiered-storage.adoc
:link-recovery-mode: manage:recovery-mode.adoc
endif::[]

[NOTE]
====
include::shared:partial$enterprise-license.adoc[]
====

With xref:{link-tiered-storage}[Tiered Storage] enabled, you can use Whole Cluster Restore to restore data from a failed cluster (source cluster), including its metadata, onto a new cluster (target cluster). This is a simpler and cheaper alternative to active-active replication, for example with xref:migrate:data-migration.adoc[MirrorMaker 2]. Use this recovery method to restore your application to the latest functional state as quickly as possible.

[CAUTION]
====
Whole Cluster Restore is not a fully-functional disaster recovery solution. It does not provide snapshot-style consistency. Some partitions in some topics will be more up-to-date than others. Committed transactions are not guaranteed to be atomic.
====

TIP: If you need to restore only a subset of topic data, consider using xref:deploy:redpanda/manual/disaster-recovery/topic-recovery.adoc[topic recovery] instead of a Whole Cluster Restore.

The following metadata is included in a Whole Cluster Restore:

* Topic definitions. If you have enabled Tiered Storage only for specific topics, topics without Tiered Storage enabled will be restored empty.
* Users and access control lists (ACLs).
* xref:manage:schema-reg/schema-reg-overview.adoc[Schemas]. To ensure that your schemas are also archived and restored, you must also enable Tiered Storage for the `_schemas` topic.
* The xref:develop:consume-data/consumer-offsets.adoc[consumer offsets topic]. Some restored committed consumer offsets may be truncated to a lower value than in the original cluster, to keep offsets at or below the highest restored offset in the partition.
* Transaction metadata, up to the highest committed transaction. In-flight transactions are treated as aborted and will not be included in the restore.
* xref:reference:cluster-properties.adoc[Cluster configurations], including your Redpanda license key, with the exception of the following properties:
** `cloud_storage_cache_size`
** `cluster_id`
** `cloud_storage_access_key`
** `cloud_storage_secret_key`
** `cloud_storage_region`
** `cloud_storage_bucket`
** `cloud_storage_api_endpoint`
** `cloud_storage_credentials_source`
** `cloud_storage_trust_file`
** `cloud_storage_backend`
** `cloud_storage_credentials_host`
** `cloud_storage_azure_storage_account`
** `cloud_storage_azure_container`
** `cloud_storage_azure_shared_key`
** `cloud_storage_azure_adls_endpoint`
** `cloud_storage_azure_adls_port`

== Manage source metadata uploads

By default, Redpanda uploads cluster metadata to object storage periodically. You can manage metadata uploads for your source cluster, or disable them entirely, with the following cluster configuration properties:

* xref:reference:cluster-properties.adoc#enable_cluster_metadata_upload_loop[`enable_cluster_metadata_upload_loop`]: Enable metadata uploads. This property is enabled by default and is required for Whole Cluster Restore.
* xref:reference:properties/object-storage-properties.adoc#cloud_storage_cluster_metadata_upload_interval_ms[`cloud_storage_cluster_metadata_upload_interval_ms`]: Set the time interval to wait between metadata uploads.
* xref:reference:cluster-properties.adoc#controller_snapshot_max_age_sec[`controller_snapshot_max_age_sec`]: Maximum amount of time that can pass before Redpanda attempts to take a controller snapshot after a new controller command appears. This property affects how current the uploaded metadata can be.
* xref:reference:properties/object-storage-properties.adoc#cloud_storage_cluster_name[`cloud_storage_cluster_name`]: *Advanced: This is an internal-only configuration and should be enabled only after consulting with Redpanda support.* Specify a custom name for cluster's metadata in object storage. For use when multiple clusters share the same storage bucket (for example, for Whole Cluster Restore).

NOTE: You can monitor the xref:reference:public-metrics-reference.adoc#redpanda_cluster_latest_cluster_metadata_manifest_age[redpanda_cluster_latest_cluster_metadata_manifest_age] metric to track the age of the most recent metadata upload.

== Restore data from a source cluster

To restore data from a source cluster:

ifdef::env-kubernetes[. <<Start a target cluster>> (new cluster) with cluster restore enabled.]
ifndef::env-kubernetes[. <<Start a target cluster>> (new cluster).]
ifndef::env-kubernetes[. <<restore-to-target-cluster, Restore data from a failed source cluster to the new cluster>>.]
ifdef::env-kubernetes[. <<Verify that the cluster restore is complete>>.]

=== Prerequisites

You must have the following:

- Redpanda v23.3 or later on both source and target clusters.
- xref:{link-tiered-storage}[Tiered Storage] enabled on the source cluster.
- Physical or virtual machines on which to deploy the target cluster.

=== Limitations

- You cannot use Whole Cluster Restore if the target cluster is in xref:{link-recovery-mode}[recovery mode].

- Whole Cluster Restore supports only one source cluster. It is not possible to consolidate multiple clusters onto the target cluster.

- If a duplicate cluster configuration is found in the target cluster, it will be overwritten by the restore.

- The target cluster should not contain user-managed or application-managed topic data, schemas, users, ACLs, or ongoing transactions.

=== Start a target cluster

ifdef::env-kubernetes[]

Deploy the target Redpanda cluster.

[tabs]
======
Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    storage:
      tiered:
        <tiered-storage-settings>
    config:
      cluster:
        cloud_storage_attempt_cluster_restore_on_bootstrap: true
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```
--
Helm::
+
--
[tabs]
====
--values::
+
.`cluster-restore.yaml`
[,yaml]
----
storage:
  tiered:
    <tiered-storage-settings>
config:
  cluster:
    cloud_storage_attempt_cluster_restore_on_bootstrap: true
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
--values cluster-restore.yaml
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set storage.tiered.<tiered-storage-settings> \
  --set config.cluster.cloud_storage_attempt_cluster_restore_on_bootstrap=true
```
====
--
======

- `storage.tiered`: Make sure to configure the target cluster with the same Tiered Storage settings as the failed source cluster.
- `config.cluster.cloud_storage_attempt_cluster_restore_on_bootstrap`: Automate cluster restore in Kubernetes. Setting to `true` is recommended when using an automated method for deployment. When bootstrapping a cluster with a given bucket, make sure that any previous cluster using the bucket is fully destroyed, otherwise Tiered Storage subsystems may interfere with each other.

include::manage:partial$kubernetes/extraclusterconfig.adoc[]
endif::[]

ifndef::env-kubernetes[]
Follow the steps to xref:deploy:redpanda/manual/production/index.adoc[deploy a new cluster].

NOTE: Make sure to configure the target cluster with the same Tiered Storage settings as the source cluster.
endif::[]

ifndef::env-kubernetes[]
=== Restore to target cluster

You can restore data from a source cluster to a target cluster using the xref:reference:rpk/rpk-cluster/rpk-cluster-storage-restore.adoc[`rpk cluster storage restore`] command.

. Restore data from the source cluster:
+
[,bash]
----
rpk cluster storage restore start -w
----
+
The wait flag (`-w`) tells the command to poll the status of the restore process and then exit when completed.

. Check if a rolling restart is required:
+
[,bash]
----
rpk cluster config status
----
+
Example output when a restart is required:
+
[,bash]
----
NODE  CONFIG-VERSION  NEEDS-RESTART  INVALID  UNKNOWN
1     4               true           []       []
----

. If a restart is required, perform a xref:manage:cluster-maintenance/rolling-restart.adoc[rolling restart].

endif::[]

ifdef::env-kubernetes[]
=== Verify that the cluster restore is complete

. Run the following command until it returns `inactive`:
+
[,bash]
----
rpk cluster storage restore status
----

. Check if a rolling restart is required:
+
[,bash]
----
rpk cluster config status
----
+
Example output when a restart is required:
+
[,bash]
----
NODE  CONFIG-VERSION  NEEDS-RESTART  INVALID  UNKNOWN
1     4               true           []       []
----

. If a restart is required, perform a xref:manage:kubernetes/k-rolling-restart.adoc[rolling restart].

endif::[]

When the cluster restore is successfully completed, you can redirect your application workload to the new cluster. Make sure to update your application code to use the new addresses of your brokers.

== (Advanced) Restore data when multiple clusters share data 

[CAUTION]
====
This is an advanced use case and should be performed only after consulting with Redpanda support.
====

Typically, there is a one-to-one mapping between a Redpanda cluster and its object storage bucket. However, you can also run multiple clusters that share the same bucket. This allows you to move tenants between clusters without moving data, as the data remains in the same bucket. For example, you can mount topics to multiple clusters in the same bucket.

Running multiple clusters that share the same storage bucket presents unique challenges during Whole Cluster Restore operations. To manage these challenges, you must first understand how Redpanda uses <<the-role-of-cluster-uuids-in-whole-cluster-restore,UUIDs>> (universal unique identifiers) to identify clusters during Whole Cluster Restore.

=== The role of cluster UUIDs in Whole Cluster Restore

Every time a Redpanda cluster (single node or more) starts, it is automatically assigned a random UUID. From that moment forward, all entities created by the cluster are identifiable using that cluster UUID. Such entities include:

- Topic data
- Topic metadata
- Whole Cluster Restore manifests
- Controller log snapshots for Whole Cluster Restore
- Consumer offsets for Whole Cluster Restore

However, not all entities _managed_ by the cluster are identifiable using this cluster UUID. In fact, Redpanda can recover a different cluster in lieu of the existing cluster, or mount topics from different clusters. For a cluster that has been running for some time, your object storage may look like this:

[source,bash]
----
/
+- cluster_metadata/
   +- <uuid-a>/manifests/
   |  +- 0/cluster_manifest.json
   |  +- 1/cluster_manifest.json
   |  +- 2/cluster_manifest.json
   + <uuid-b>/manifests/
   |  +- 3/cluster_manifest.json
   |  +- 4/cluster_manifest.json
   + <uuid-c>/manifests/ # Previously active but not restored.
   |                     # Still, the manifest number starts at
   |                     # highest found in the bucket plus one.
   |  +- 5/cluster_manifest.json
   |  +- 6/cluster_manifest.json
   + <uuid-d>/manifests/ # active cluster (not restored)
      +- 7/cluster_manifest.json
      +- 8/cluster_manifest.json
----

Redpanda's algorithm lists all objects (cluster manifests) from object storage and during a Whole Cluster Restore, picks the object with the _highest ID available_, not the current UUID. In this case, if you attempt to restore you would recover `/cluster_metadata/<uuid-c>/manifests/6/cluster_manifest.json`, even though the active cluster is `<uuid-d>`.

However, this algorithm does not work if you have multiple clusters sharing the same object storage bucket. For example, your object storage might look like:

[source,bash]
----
/
+- cluster_metadata/
   + <uuid-a>/manifests/
   | +- 0/cluster_manifest.json
   | +- 1/cluster_manifest.json
   | +- 2/cluster_manifest.json
   + <uuid-b>/manifests/
     +- 0/cluster_manifest.json
     +- 1/cluster_manifest.json (lost cluster)
----

Here, if you've lost the cluster `uuid-b` and wish to recover it, the recovery process will select the metadata for `uuid-a`, which will lead to a split-brain/data corruption scenario. For troubleshooting details, see <<resolve-repeated-recovery-failures,Resolve repeated recovery failures>>

=== Configure cluster names for multiple source clusters

To disambiguate cluster metadata from multiple clusters, use the xref:reference:properties/object-storage-properties.adoc#cloud_storage_cluster_name[`cloud_storage_cluster_name`] property (off by default), which allows you to assign a unique name to each cluster sharing the same object storage bucket. This name must be unique within the bucket, 1-64 characters, and use only letters, numbers, underscores, and hyphens. Do not change this value once set. Once set, your object storage bucket may look like this:

[source,bash]
----
/
+- cluster_metadata/
|  + <uuid-a>/manifests/
|  | +- 0/cluster_manifest.json
|  | +- 1/cluster_manifest.json
|  | +- 2/cluster_manifest.json
|  + <uuid-b>/manifests/
|    +- 0/cluster_manifest.json
|    +- 1/cluster_manifest.json # lost cluster
+- cluster_name/
   +- rp-foo/uuid/<uuid-a>
   +- rp-qux/uuid/<uuid-b>
----

When a new cluster is created, and you have specified its `cloud_storage_cluster_name` (here, `rp-qux`), your object storage bucket may look like this:

[source,bash]
----
+- cluster_metadata/
|  + <uuid-a>/manifests/
|  | +- 0/cluster_manifest.json
|  | +- 1/cluster_manifest.json
|  | +- 2/cluster_manifest.json
|  + <uuid-b>/manifests/
|  | +- 0/cluster_manifest.json
|  | +- 1/cluster_manifest.json # lost cluster
|  + <uuid-c>/manifests/
|    +- 3/cluster_manifest.json # new cluster
|     # ^- next highest sequence number globally
+- cluster_name/
   +- rp-foo/uuid/<uuid-a>
   +- rp-qux/uuid/
      +- <uuid-b>
      +- <uuid-c> # reference to new cluster
----

During a Whole Cluster Restore, Redpanda will look for the cluster name specified in `cloud_storage_cluster_name` and only consider manifests associated with that name. In this example, if you start a cluster with `cloud_storage_cluster_name` set to `rp-qux`, Redpanda will only consider manifests under `<uuid-b>` and `<uuid-c>`, ignoring `<uuid-a>` entirely.

Redpanda uses this name to organize the cluster metadata within the shared object storage bucket. This ensures that each cluster's data remains distinct and prevents conflicts during recovery operations.

=== Resolve repeated recovery failures

If you are experiencing repeated failures when a cluster is lost and recreated, the automated recovery algorithm may have selected the manifest with the highest sequence number, which might be the most recent one with no data, instead of the original one that contains the data. Your object storage bucket might look like this:

[source,bash]
----
/
+- cluster_metadata/
   + <uuid-a>/manifests/
   | +- 0/cluster_manifest.json
   | +- 1/cluster_manifest.json #lost cluster
   + <uuid-b>/manifests/
     +- 3/cluster_manifest.json # lost again (not recovered)
   + <uuid-d>/manifests/
      +- 7/cluster_manifest.json # new attempt to recover uuid-b
                                 # it does not have the data
----

In such cases, you can explicitly run a POST request using the Admin API:

[source,bash]
----
curl -XPOST \
     --data '{"cluster_uuid_override":  "<uuid-a>"}'
     http://localhost:9644/v1/cloud_storage/automated_recovery
----

For details, see the Admin API.