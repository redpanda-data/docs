= Topics as Iceberg Tables
:description: Learn how to integrate Redpanda topics with Apache Iceberg.
:page-context-links: [{"name": "Linux", "to": "manage:topic-iceberg-integration.adoc" } ]
:page-categories: Management, High Availability, Data Replication, Integration


The Redpanda integration with Apache Iceberg allows your topic data to be stored in the cloud in Iceberg's table format. This opens up immediate access to your streaming data for analytical systems such as data warehouses like RedShift, Snowflake, and Clickhouse, and big data processing platforms such as Apache Spark and Flink, without setting up and maintaining additional ETL pipelines. 

The Iceberg integration uses xref:manage:tiered-storage.adoc[Tiered Storage]. When a cluster or topic is enabled with Tiered Storage, Redpanda stores the Iceberg files in the configured Tiered Storage bucket or container.  

== Prerequisites

include::shared:partial$enterprise-license.adoc[]

. To check if you already have a license key applied to your cluster:
+
[,bash]
----
rpk cluster license info
----

. Enable xref:manage:tiered-storage.adoc#set-up-tiered-storage[Tiered Storage] for the topics for which you want to generate Iceberg tables.
. Install `rpk`.

== Limitations

* You can only enable the topic integration for new Redpanda topics. 
* It is not possible to append or backfill data from Redpanda topics to an existing Iceberg table.
* JSON schemas are not currently supported. For Avro schemas, records cannot contain fields greater than 128KB.
* You can only use one schema per topic. Schema versioning as well as upcasting (where a value is cast into its more generic data type) are not supported. 

== Architecture 

Apache Iceberg is an open source format specification for defining structured tables in a glossterm:data lake[]. The table format enables you to easily and quickly manage, query, and process huge amounts of structured and unstructured data, like you would manage and run SQL queries against relational data in a database or data warehouse. The open format allows many different languages, tools, and applications to process the same data in a consistent way, so you can avoid vendor lock-in. This data management system is also referred to as a _data lakehouse_.

In the Iceberg specification, tables comprise the following:

* Data layer
** Data files: Store the actual data. The Redpanda integration currently supports the Parquet file format. Parquet files are column-based and suitable for analytical workloads at scale, and come with compression capabilities that optimize files for object storage. 
* Metadata layer
+
--
** Manifest files: Track data files and contain metadata about those files, such as record count, partition membership, and file paths.
** Manifest list: Tracks all the manifest files belonging to a table, including file paths and upper and lower bounds for partition fields. 
** Metadata file: Stores metadata about the table, including its schema, partition information, and snapshots. Whenever a change is made to the table, a new metadata file is created and becomes the latest version of the metadata in the catalog.
--
+
In the Redpanda Iceberg integration, the manifest files are in JSON format.
* Catalog: Contains the current metadata pointer for the table. Clients reading and writing data to the table see the same version of the current state of the table. You'll configure your Iceberg catalog to point to your object storage bucket or container where the Redpanda data in Iceberg format is located. Redpanda uses the https://iceberg.apache.org/concepts/catalog/#catalog-implementations[Iceberg REST catalog^] endpoint to update your catalog when there are changes to the Iceberg data and metadata.

image::shared:iceberg-integration.png[]

When you enable the Iceberg integration for a Redpanda topic, Redpanda brokers store streaming data in the Iceberg-compatible format in Parquet files in object storage, in addition to writing events stored in segment files on disk. Storing the streaming data in Iceberg tables in the cloud allows you to derive real-time insights through business intelligence and machine learning tools that work with Iceberg. 


== Enable Iceberg integration

To use the Iceberg integration for a topic, you must set Redpanda cluster configuration `enable_datalake` to true, and also explicitly set the topic configuration to `true`.

. Enable the Iceberg integration for the cluster by setting the cluster configuration `enable_datalake` to `true`. 
+
[,bash]
----
rpk cluster config set enable_datalake true 
----
+
[,bash,role=no-copy]
----
Successfully updated configuration. New configuration version is 2.
----

. Create a new topic.
+
[,bash,]
----
rpk topic create <new-topic-name> --partitions 1 --replicas 1
----
+
[,bash,role=no-copy]
----
TOPIC            STATUS
new-topic-name   OK
----

. Enable the integration for the topic. 
+
[,bash]
----
rpk topic alter-config <new-topic-name> --set iceberg_topic=true 
----
+
[,bash,role=no-copy]
----
TOPIC            STATUS
new-topic-name   OK
----

. Optional: Register a schema for the topic. Redpanda uses xref:manage:schema-reg/schema-id-validation.adoc#set-subject-name-strategy-per-topic[`TopicNameStrategy`] (`<topic-name>-value` or `<topic-name>-key`) to find the matching schema based on the subject name. Redpanda derives the Iceberg table format from the schema and writes data files to object storage in this format. Protobuf and Avro schemas are supported.
+
If there is no registered schema for the topic, Redpanda by default uses a simple schema consisting of the recordâ€™s key, value, and the original event timestamp. If you don't have a need for a strict schema and can use the data in a more semi-structured format, consider the schemaless approach.
+
[,bash]
----
rpk registry schema create <subject-name> --schema </path-to-schema> --type <format>
----
+
[,bash,role=no-copy]
----
SUBJECT                VERSION   ID   TYPE
new-topic-name-value   1         1    PROTOBUF
----

As you produce records to the topic, the data also becomes available to consume from object storage by Iceberg-compatible clients. 

== Consume data in Iceberg tables

You can use the same analytical tools to access table data just like you would in a relational database or data lake.

For example, suppose you are producing the same stream of events to a topic `test_topic`, and another topic `test_topic_schemaless`. The topics have Tiered Storage configured to an AWS S3 bucket. A record looks like the following:

[,bash,role=no-copy]
----

----

When you point your Iceberg-compatible tool or framework to the object storage location of the Iceberg tables, how you consume the data depends on whether you've registered a schema on which to base the table format. In either approach, you do not need to rely on complex ETL jobs or pipelines to consume real-time data from Redpanda.

=== Structured approach with schema

If you register the following schema for `test_topic` under the `test_topic-value` subject:

[,bash,role=no-copy]
----
syntax = "proto3";

message twitter_record {
  string Topic = 1;
  string Sentiment = 2;
  int64 TweetId = 3;
  string TweetText = 4;
  string TweetDate = 5;
}
----

Many platforms such as Clickhouse provide Iceberg integrations that allow you to easily access existing Iceberg tables in object storage. This example reads the data from `test_topic` into Clickhouse: 

[,bash]
----
CREATE TABLE test_topic ENGINE=IcebergS3('<s3-bucket-url>', <aws-access-key-id>', '<aws-secret-access-key>', 'Parquet')
----

The table structure is derived from the schema, and in this case, the `test-topic` table contains columns based on the  values:

[,bash]
----
SELECT
  Topic,
  Sentiment,
  TweetId,
  left(TweetText, 30)
FROM test_topic
----

[,bash,role=no-copy]
----

----

=== Schemaless approach

You can also forgo using a schema, which means the data in Iceberg will be semi-structured. 

This example reads the semi-structured data in `test_topic_schemaless` into Clickhouse:

[,bash]
----
CREATE TABLE test_topic_schemaless ENGINE=IcebergS3('<s3-bucket-url>', <aws-access-key-id>', '<aws-secret-access-key>', 'Parquet')
----

The table consists of columns containing the record key, value, and the original timestamp.

[,bash]
----
SELECT
  Timestamp,
  Key,
  left(Value, 30)
FROM test_topic_schemaless
----

[,bash,role=no-copy]
----

----

== Suggested reading

* https://www.redpanda.com/blog/apache-iceberg-topics-streaming-data[Apache Iceberg Topics: Stream directly into your data lake
^]