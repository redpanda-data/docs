= Topics as Iceberg Tables
:description: Learn how to integrate Redpanda topics with Apache Iceberg.
:page-context-links: [{"name": "Linux", "to": "manage:topic-iceberg-integration.adoc" } ]
:page-categories: Management, High Availability, Data Replication, Integration
:page-beta: true


The Apache Iceberg integration for Redpanda allows you to store topic data in the cloud in the Iceberg open table format. This makes your streaming data immediately available for analytical systems, such as data warehouses like RedShift, Snowflake, and Clickhouse, and big data processing platforms, such as Apache Spark and Flink, without setting up and maintaining additional ETL pipelines. 

The Iceberg integration uses xref:manage:tiered-storage.adoc[Tiered Storage]. When a cluster or topic has Tiered Storage enabled, Redpanda stores the Iceberg files in the configured Tiered Storage bucket or container.  

== Prerequisites

* Install `rpk`.

*  {empty}
include::shared:partial$enterprise-license.adoc[]
+
To check if you already have a license key applied to your cluster:
+
[,bash]
----
rpk cluster license info
----

* Enable xref:manage:tiered-storage.adoc#set-up-tiered-storage[Tiered Storage] for the topics for which you want to generate Iceberg tables.

== Limitations

* It is not possible to append data from Redpanda topics to an existing Iceberg table.
* If you enable the Iceberg integration on an existing Redpanda topic, Redpanda does not backfill the generated Iceberg table with topic data.
* JSON schemas are not currently supported. For Avro schemas, records cannot contain fields greater than 128 KB.
* You can only use one schema per topic. Schema versioning as well as upcasting (where a value is cast into its more generic data type) are not supported. See <<schema-types-translation,Schema types translation>> for more details.
* xref:manage:remote-read-replicas.adoc[Remote read replicas] and xref:manage:topic-recovery.adoc[topic recovery] are not supported for Iceberg-enabled topics.

== Architecture 

Apache Iceberg is an open source format specification for defining structured tables in a data lake. The table format lets you quickly and easily manage, query, and process huge amounts of structured and unstructured data. This is similar to the way in which you would manage and run SQL queries against relational data in a database or data warehouse. The open format lets you use many different languages, tools, and applications to process the same data in a consistent way, so you can avoid vendor lock-in. This data management system is referred to as a _data lakehouse_.

In the Iceberg specification, tables consist of the following layers:

* Data layer
** Data files: Store the data. The Iceberg integration currently supports the Parquet file format. Parquet files are column-based and suitable for analytical workloads at scale. They come with compression capabilities that optimize files for object storage. 
* Metadata layer
+
--
** Manifest files: Track data files and contain metadata about these files, such as record count, partition membership, and file paths.
** Manifest list: Tracks all the manifest files belonging to a table, including file paths and upper and lower bounds for partition fields. 
** Metadata file: Stores metadata about the table, including its schema, partition information, and snapshots. Whenever a change is made to the table, a new metadata file is created and becomes the latest version of the metadata in the catalog.
--
+
In the Redpanda Iceberg integration, the manifest files are in JSON format.
* Catalog: Contains the current metadata pointer for the table. Clients reading and writing data to the table see the same version of the current state of the table. The Iceberg integration supports two <<set-up-catalog-integration,catalog integration>> types. You can configure Redpanda to catalog files stored in the same object storage bucket or container where the Iceberg data files are located, or you can configure Redpanda to use an https://iceberg.apache.org/concepts/catalog/#catalog-implementations[Iceberg REST catalog^] endpoint to update an externally-managed catalog when there are changes to the Iceberg data and metadata.

image::shared:iceberg-integration.png[]

When you enable the Iceberg integration for a Redpanda topic, Redpanda brokers store streaming data in the Iceberg-compatible format in Parquet files in object storage. The brokers also write events to segment files stored on disk. Storing the streaming data in Iceberg tables in the cloud allows you to derive real-time insights through many compatible https://iceberg.apache.org/vendors/[data lakehouse, data engineering, and business intelligence tools]. 

== Enable Iceberg integration

To create an Iceberg table for a Redpanda topic, you must set the cluster configuration property `iceberg_enabled` to `true`, and also configure the topic property `redpanda.iceberg.mode`. You can choose to provide a schema if you need the Iceberg table to be structured with defined columns.

. Set the `iceberg_enabled` configuration option on your cluster to `true`. 
+
[,bash]
----
rpk cluster config set iceberg_enabled true 
----
+
[,bash,role=no-copy]
----
Successfully updated configuration. New configuration version is 2.
----

. Create a new topic.
+
[,bash,]
----
rpk topic create <new-topic-name> --partitions 1 --replicas 1
----
+
[,bash,role=no-copy]
----
TOPIC            STATUS
new-topic-name   OK
----

. Enable the integration for the topic by configuring `redpanda.iceberg.mode`. You can choose one of the following modes:
+
--
* `key_value`: Creates an Iceberg table with a `Key` column and a `Value` column. Redpanda stores the raw topic content in the `Value` column. We also refer to this as the "schemaless" mode.
* `value_schema_id_prefix`: Creates an Iceberg table whose structure matches the Redpanda schema for this topic, with columns corresponding to each field. Redpanda parses the topic values per field and stores them in the corresponding table columns. 
* `disabled` (default): Disables writing to an Iceberg table for this topic.
--
+
[,bash]
----
rpk topic alter-config <new-topic-name> --set redpanda.iceberg.mode=<topic-iceberg-mode>
----
+
[,bash,role=no-copy]
----
TOPIC            STATUS
new-topic-name   OK
----

. Register a schema for the topic (optional). You must also set the `redpanda.iceberg.mode` topic property to `value_schema_id_prefix`.
+
[,bash]
----
rpk registry schema create <subject-name> --schema </path-to-schema> --type <format>
----
+
[,bash,role=no-copy]
----
SUBJECT                VERSION   ID   TYPE
new-topic-name-value   1         1    PROTOBUF
----
+
If you don't use a schema for the topic, select `key_value` for the topic Iceberg mode. Redpanda uses a simple schema for the Iceberg table, consisting of a column that stores the recordâ€™s metadata, key, and value.

The Iceberg table name is the same as the Redpanda topic name. As you produce records to the topic, the data also becomes available in object storage for consumption by Iceberg-compatible clients. 

== Schema support and mapping

The `redpanda.iceberg.mode` property determines how Redpanda maps the topic data to the Iceberg table structure. You can either have the generated Iceberg table match the stucture of a Avro or Protobuf schema in the Schema Registry, or use the schemaless mode where Redpanda stores the record values as-is in the table. The JSON Schema format is not supported in this beta release. If your topic data is in JSON, it is recommended to use the `key_value` (schemaless) mode.

=== Iceberg table modes

For both `key_value` and `value_schema_id_prefix` modes, Redpanda writes to a `redpanda` table column that stores a single struct per record, containing nested columns of the metadata from each record, including the record timestamp, the partition it belongs to, and its offset. 

In the `key_value` ("schemaless") mode, the `redpanda` metadata structs also contain both the record key and value. If you are associating a schema with the topic using the `value_schema_id_prefix` mode, the `redpanda` structs contain the record key only. Redpanda uses the matching schema to define table columns based on schema fields and then maps the record values to the corresponding columns.

For example, if you produce to a topic according to the following Avro schema:

```
{
    "type": "record",
    "name": "ClickEvent",
    "fields": [
        {
            "name": "user_id",
            "type": "int"
        },
        {
            "name": "event_type",
            "type": "string"
        },
        {
            "name": "ts",
            "type": "string"
        }
    ]
}

```

The `key_value` mode writes to the following table format:

```
CREATE TABLE ClickEvent (
  redpanda OBJECT(
    partition INT,
    timestamp TIMESTAMP,
    offset    LONG,
    headers   ARRAY(OBJECT(KEY BINARY, VALUE BINARY NULLABLE)),
    key       BINARY NULLABLE,
    data      BINARY NULLABLE
  )
)
```

Consider this schemaless approach if the topic data is in JSON, or if you can use the Iceberg data in its semi-structured format.

The `value_schema_id_prefix` mode translates to the following table format:

```
CREATE TABLE ClickEvent (
  redpanda OBJECT(
    partition INT,
    timestamp TIMESTAMP,
    offset    LONG,
    headers   ARRAY(OBJECT(KEY BINARY, VALUE BINARY NULLABLE)),
    key       BINARY NULLABLE
  ),
   user_id INT,
   event_type STRING NULLABLE,
   ts STRING NULLABLE
);

```

With schema integration, Redpanda uses the schema ID prefix embedded in each record to find the matching schema in the Schema Registry. Producers to the topic must use the schema ID prefix in the serialization process so Redpanda can determine the schema used for each record, parse the record according to that schema, and use the schema for the Iceberg table as well.

=== Schema types translation

[tabs]
======
Avro::
+
--
* Redpanda supports direct translations of the following Avro types to Iceberg value domains:
** boolean
** int
** long
** float
** double
** bytes 
** string
** record
** array
** maps
** fixed
** decimal
** uuid
** date
** time
** timestamp
* Different flavors of timestamp and time types are all represented by the same Iceberg type, conversion will be done.
* Avro unions are flattened to structs with optional fields, out of which only one is present, or none in case there is a field with null type:
** For example, the union `["int", "long", "float"]` is represented as an Iceberg struct `OBJECT(0 INT NULLABLE, 1 LONG  NULLABLE, 2 FLOAT NULLABLE)`.
** The union `["int", null, "float"]` is represented as an Iceberg struct `OBJECT(0 INT NULLABLE, 1 FLOAT NULLABLE)`.
* All fields are required by default (Avro always sets a default in binary representation).
* Duration logical type is ignored.
* Null type is ignored and not represented in Iceberg schema.
* Recursive types are not supported.
--

Protobuf::
+
--
* Redpanda supports direct translations of the following Avro types to Iceberg value domains: 
** BOOL
** DOUBLE
** FLOAT
** INT32
** SINT32
** INT64
** SINT64
** SFIXED32
** SFIXED64
** STRING
** BYTES
** MAP
* Repeated values are translated into Iceberg ARRAY types.
* Enums are translated into Iceberg INT types based on the integer value of the enumerated type.
* UINT32 and FIXED32 are translated into Iceberg LONG types as that is the existing semantic for unsigned 32bit values in Iceberg.
* UINT64 and FIXED64 values are translated into their base 10 string representation.
* Timestamp well known protos are translated into TIMESTAMP Iceberg types.
* Messages are converted into STRUCT types.
* Recursive types are not supported.
--
======

== Set up catalog integration

You can configure the Iceberg integration to either create a file in the same object storage bucket or container to serve as the catalog, or connect to a REST-based catalog. 

Set the cluster configuration property `iceberg_catalog_type` with one of the following values:

* `object_storage`: Write catalog files to the same object storage bucket as the data files. Use the object storage URL to access the catalog for your Redpanda Iceberg tables.
* `rest`: Connect to and update an Iceberg catalog hosted on a REST server.

For an Iceberg REST catalog, set the following additional cluster configuration properties:

* `iceberg_rest_catalog_endpoint`: The endpoint URL for your Iceberg catalog, which you are either managing directly, or is managed by an external catalog service.
* `iceberg_rest_catalog_prefix`: Append a prefix to the catalog path, e.g. `/v1/{prefix}/namespaces`.
* `iceberg_rest_catalog_client_id`: The ID to connect to the REST server.
* `iceberg_rest_catalog_client_secret`: The secret data to connect to the REST server.

// update xref when PR for extracted properties is ready
See xref:reference:properties/cluster-properties.adoc[Cluster Configuration Properties] for the full list of cluster properties to configure for a catalog integration.

=== Example catalog configuration

You'll be able to use to the catalog to load, query, or refresh the Iceberg data as you produce to the Redpanda topic. Refer to the official documentation of your query engine or Iceberg-compatible tool for guidance on integrating the REST-based catalog. 

For example, if you have Redpanda cluster configuration properties set to connect to a REST catalog named `demo`:

```
iceberg_rest_catalog_type: rest 
iceberg_rest_catalog_endpoint: http://catalog-service:8181 
iceberg_rest_catalog_client_id: <rest-connection-id>
iceberg_rest_catalog_client_secret: <rest-connection-password>
```

And you have a Spark configured to use the `demo` catalog:

```
spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.demo.type = rest
spark.sql.catalog.demo.uri = http://catalog-service:8181
spark.sql.catalog.demo.warehouse = s3://redpanda/
```

Using Spark SQL, you can query the Iceberg table directly by specifying the catalog name:

```
SELECT * FROM demo.redpanda.ClickEvent;
```

== Access data in Iceberg tables

You can use the same analytical tools to access table data in a data lake as you would for a relational database.

For example, suppose you produce the same stream of events to a topic `ClickEvent`, and another topic `ClickEvent_schemaless`. The topics have Tiered Storage configured to an AWS S3 bucket. A sample record contains the following data:

[,bash,role=no-copy]
----
{"user_id": 2324, "event_type": "BUTTON_CLICK", "ts": "2024-11-25T20:23:59.380Z"}
----

When you point your Iceberg-compatible tool or framework to the object storage location of the Iceberg tables, how you consume the data depends on whether you've registered a schema on which to derive the table format. In either approach, you do not need to rely on complex ETL jobs or pipelines to consume real-time data from Redpanda.

=== Structured approach with schema

Register the following schema for `ClickEvent` under the `ClickEvent-value` subject:

[,avro]
----
{
    "type" : "record",
    "namespace" : "com.redpanda.examples.avro",
    "name" : "ClickEvent",
    "fields" : [
       { "name": "user_id", "type" : "int" },
       { "name": "event_type", "type" : "string" },
       { "name": "ts", "type": "string" }
    ]
 }
----

[,bash]
----
rpk registry schema create ClickEvent-value --schema path/to/schema.avsc --type avro
----

Query engines such as Spark SQL provide Iceberg integrations to allow easy access to existing Iceberg tables in object storage. The table structure is derived from the schema. In this example, the query returns values from columns in the `ClickEvent` table, with the column names matching the schema fields:

[,sql]
----
SELECT user_id, 
  event_type, 
  ts 
FROM ClickEvent;
----

[,bash,role=no-copy]
----
+---------+--------------+--------------------------+
| user_id | event_type   | ts                       |
+---------+--------------+--------------------------+
| 2324    | BUTTON_CLICK | 2024-11-25T20:23:59.380Z |
+---------+--------------+--------------------------+
----

=== Schemaless approach

You can also forgo using a schema, which means using semi-structured data in Iceberg. 

This example queries the semi-structured data in the `ClickEvent_schemaless` table, which consists of a column `redpanda` containing the record key, value, and metadata:

[,sql]
----
SELECT 
  redpanda.data
FROM ClickEvent_schemaless;
----

[,bash,role=no-copy]
----
+------------------------------------------------------------------------------+
| redpanda.data                                                                |
+------------------------------------------------------------------------------+
| {"user_id":2324,"event_type":"BUTTON_CLICK","ts":"2024-11-25T20:23:59.380Z"} |
+------------------------------------------------------------------------------+
----

== Suggested reading
