= Node-wise Partition Recovery
:description: Feature to recover partitions that have lost a majority of replicas.

Multi-node or entire-AZ failures (especially in cloud environments), along with some forms of human error, can result in ‘stuck’ partitions where there are fewer replicas than required to make a quorum. In such failure scenarios, some data loss may be unavoidable. Node-wise partition recovery provides a way to unsafely recover at least a portion of your data using remaining replicas, which are moved off of target nodes and allocated to healthy nodes. In one step, this process repairs partitions while draining the target nodes of all partition replicas. This topic helps admins understand what they can or cannot recover using node-wise partition recovery.

IMPORTANT: Only use this operation as a last-resort measure when all other recovery options have failed. In some cases, there may be no remaining replicas for the partitions on the dead nodes. This recovery method is intended for scenarios where you have already experienced data loss, with the goal being to stop the loss of additional data.

== Perform the recovery operation

Launch node-wise partition recovery with `rpk cluster partitions unsafe-recover`. This command includes an interactive prompt to confirm execution of the generated recovery plan, as it is a destructive operation. When you trigger node-wise partition recovery, the partitions on the node are rebuilt on a best-effort basis. In cases where there are zero surviving partition replicas, such as a topic with a replication factor of 1 (`RF=1`), partition recovery rebuilds empty partitions with no data, allowing Producers to continue writing to the partition despite the fact that no data can be recovered in such situations.

The syntax for this command is as follows:

 rpk cluster partitions unsafe-recover --from-nodes 1,3,5

The `--from-nodes` flag accepts a comma-separated list of the dead node IDs you wish to recover the data from. This example performs recovery operations on nodes 1, 3, and 5. Redpanda assesses these nodes to identify which partitions lack majority. It then creates a plan to recover the impacted partitions and prompts you for confirmation. You must respond `yes` to continue with recovery.

The `--dry` flag performs a dry run and allows you to view the recovery plan with no risk to your cluster.

[NOTE] 
====
When running node-wise partition recovery, it's possible that there may be more recent data (a higher offset) available in Tiered Storage if:

* Raft replication was stuck or slow before the node failure
* Zero live replicas remain in the cluster (because the partition had a replication factor of one, `RF=1`)

For topics configured to use Tiered Storage, Redpanda also attempts to recover partition data from cloud storage, recovering the latest offset available for a partition in either storage tier (local or cloud storage). This allows for the maximum amount of data to be recovered in all cases, even for topics with a replication factor of 1, where no replicas remain in local storage.
====

After the recovery operation is started, you can monitor the status in real-time with `rpk cluster partitions balancer-status`. The recovery operation can take some time to complete, especially when a large amount of data is involved.

== Example recovery operation
The following example shows the node-wise partition recovery process in action.

----
$ rpk cluster partitions unsafe-recover --from-nodes 1
NAMESPACE  TOPIC  PARTITION  REPLICA-CORE  DEAD-NODES
kafka      bar    0          [1-1]         [1]
? Confirm recovery from these nodes? Yes
Executing recovery plan...
Successfully queued the recovery plan, you may check the status by running 'rpk cluster partitions balancer-status'

$ rpk cluster partitions balancer-status
Status:                               ready
Seconds Since Last Tick:              26
Current Reassignment Count:           0
Partitions Pending Recovery (1):      [kafka/bar/0]
----