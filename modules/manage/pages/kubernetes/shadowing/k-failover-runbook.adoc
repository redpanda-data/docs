= Kubernetes Failover Runbook
:description: Step-by-step emergency guide for failing over Redpanda shadow links in Kubernetes during disasters.
:page-categories: Management, High Availability, Disaster Recovery, Emergency Response
:env-kubernetes: true

[NOTE]
====
include::shared:partial$enterprise-license.adoc[]
====

This guide provides step-by-step procedures for emergency failover when your primary Redpanda cluster becomes unavailable. Follow these procedures only during active disasters when immediate failover is required.

[IMPORTANT]
====
This is an emergency procedure. For planned failover testing or day-to-day shadow link management, see xref:manage:disaster-recovery/shadowing/failover.adoc[]. Ensure you have completed the xref:manage:disaster-recovery/shadowing/overview.adoc#disaster-readiness-checklist[disaster readiness checklist] before an emergency occurs.
====

== Emergency failover procedure

Follow these steps during an active disaster:

1. <<assess-situation,Assess the situation>>
2. <<verify-shadow-status,Verify shadow cluster status>>
3. <<document-state,Document current state>>
4. <<initiate-failover,Initiate failover>>
5. <<monitor-progress,Monitor failover progress>>
6. <<update-applications,Update application configuration>>
7. <<verify-functionality,Verify application functionality>>
8. <<cleanup-stabilize,Clean up and stabilize>>

[[assess-situation]]
=== Assess the situation

Confirm that failover is necessary:

[tabs]
======
Operator::
+
--
[,bash]
----
# Check if source cluster is responding
kubectl exec --namespace <source-namespace> <source-pod-name> --container redpanda -- \
  rpk cluster info

# If source cluster is down, check shadow cluster health
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk cluster info
----
--

Helm::
+
--
[,bash]
----
# Check if source cluster is responding
kubectl exec --namespace <source-namespace> <source-pod-name> --container redpanda -- \
  rpk cluster info

# If source cluster is down, check shadow cluster health
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk cluster info
----
--
======

**Decision point**: If the primary cluster is responsive, consider whether failover is actually needed. Partial outages may not require full disaster recovery.

include::manage:partial$shadowing/failover-decision-examples.adoc[]

[[verify-shadow-status]]
=== Verify shadow cluster status

Check the health of your shadow links:

[tabs]
======
Operator::
+
--
[,bash]
----
# List all shadow links
kubectl get shadowlink --namespace <shadow-namespace>

# Check the ShadowLink resource details
kubectl describe shadowlink --namespace <shadow-namespace> <shadowlink-name>
----

Verify that the following conditions exist before proceeding with failover:

* ShadowLink resource shows `Synced: True` in conditions
* Shadow topic statuses show `state: active` (not `faulted`)
* Task statuses show `state: active`
--

Helm::
+
--
[,bash]
----
# List all shadow links
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow list

# Check the configuration of your shadow link
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow describe <shadow-link-name>

# Check the status of your disaster recovery link
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow status <shadow-link-name>
----

For detailed command options, see xref:reference:rpk/rpk-shadow/rpk-shadow-list.adoc[`rpk shadow list`], xref:reference:rpk/rpk-shadow/rpk-shadow-describe.adoc[`rpk shadow describe`], and xref:reference:rpk/rpk-shadow/rpk-shadow-status.adoc[`rpk shadow status`].

Verify that the following conditions exist before proceeding with failover:

* Shadow link state should be `ACTIVE`
* Topics should be in `ACTIVE` state (not `FAULTED`)
* Replication lag should be reasonable for your RPO requirements
--
======

==== Understanding replication lag

Use status commands to check lag, which shows the message count difference between source and shadow partitions:

include::manage:partial$shadowing/replication-lag-guidelines.adoc[]

[[document-state]]
=== Document current state

Record the current lag and status before proceeding:

[tabs]
======
Operator::
+
--
[,bash]
----
# Capture current status for post-mortem analysis
kubectl describe shadowlink --namespace <shadow-namespace> <shadowlink-name> > failover-status-$(date +%Y%m%d-%H%M%S).log
----
--

Helm::
+
--
[,bash]
----
# Capture current status for post-mortem analysis
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow status <shadow-link-name> > failover-status-$(date +%Y%m%d-%H%M%S).log
----
--
======

[IMPORTANT]
====
Note the replication lag to estimate potential data loss during failover. For details about shadow link replication tasks, see xref:manage:disaster-recovery/shadowing/overview.adoc#shadow-link-tasks[Shadow link tasks].
====

[[initiate-failover]]
=== Initiate failover

A complete cluster failover is appropriate if you observe that the source cluster is no longer reachable:

[tabs]
======
Operator::
+
--
Delete the `ShadowLink` resource to fail over all topics:

[,bash]
----
kubectl delete shadowlink --namespace <shadow-namespace> <shadowlink-name>
----

.Expected output
[.no-copy]
----
shadowlink.cluster.redpanda.com "<shadowlink-name>" deleted
----

This immediately converts all shadow topics to regular writable topics and stops replication.

[NOTE]
====
The Redpanda Operator does not support selective topic failover. For selective failover, use the `rpk` commands shown in the Helm tab.
====
--

Helm::
+
--
For complete cluster failover (all topics):

[,bash]
----
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow failover <shadow-link-name> --all
----

**Expected output**:
[.no-copy]
----
Successfully initiated the Fail Over for Shadow Link "<shadow-link-name>". To check the status, run:
  rpk shadow status <shadow-link-name>
----

For selective topic failover (when only specific services are affected):

[,bash]
----
# Fail over individual topics
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow failover <shadow-link-name> --topic <topic-name>
----

For detailed command options, see xref:reference:rpk/rpk-shadow/rpk-shadow-failover.adoc[`rpk shadow failover`].
--
======

[[monitor-progress]]
=== Monitor failover progress

Track the failover process:

[tabs]
======
Operator::
+
--
After deleting the `ShadowLink` resource, verify topics are now writable:

[,bash]
----
# Check that shadow link is gone
kubectl get shadowlink --namespace <shadow-namespace>

# List topics on shadow cluster
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk topic list

# Test write to a previously shadow topic
echo "test message" | kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -i -- \
  rpk topic produce <topic-name>
----

.Expected output for kubectl get
[.no-copy]
----
No resources found in <shadow-namespace> namespace.
----

.Expected output for rpk topic produce
[.no-copy]
----
Produced to partition 0 at offset 123 with timestamp 1734567890123.
----

If the shadow link is deleted and you can successfully produce to topics, failover is complete.
--

Helm::
+
--
Monitor status until all topics show `FAILED_OVER`:

[,bash]
----
# Monitor status during failover
watch -n 5 "kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- rpk shadow status <shadow-link-name>"

# Check detailed topic status
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow status <shadow-link-name> --print-topic
----

.Expected output during failover
[.no-copy]
----
OVERVIEW
===
NAME   disaster-recovery-link
STATE  ACTIVE

TOPICS
===
Name: orders, State: FAILED_OVER
Name: inventory, State: FAILED_OVER
Name: transactions, State: FAILING_OVER
----

Wait for all critical topics to reach `FAILED_OVER` state before proceeding.
--
======

[[update-applications]]
=== Update application configuration

Redirect your applications to the shadow cluster by updating connection strings in your applications to point to shadow cluster brokers. If using DNS-based service discovery, update DNS records accordingly. Restart applications to pick up new connection settings and verify connectivity from application hosts to shadow cluster.

[[verify-functionality]]
=== Verify application functionality

Test critical application workflows:

[,bash]
----
# Verify applications can produce messages
echo "failover test" | kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -i -- \
  rpk topic produce <topic-name>

# Verify applications can consume messages
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk topic consume <topic-name> --num 1
----

.Expected output for produce
[.no-copy]
----
Produced to partition 0 at offset 456 with timestamp 1734567890456.
----

.Expected output for consume
[.no-copy]
----
{
  "topic": "<topic-name>",
  "value": "failover test",
  "timestamp": 1734567890456,
  "partition": 0,
  "offset": 456
}
----

Test message production and consumption, consumer group functionality, and critical business workflows to ensure everything is working properly.

[[cleanup-stabilize]]
=== Clean up and stabilize

After all applications are running normally:

[tabs]
======
Operator::
+
--
The `ShadowLink` resource has already been deleted during failover. No additional cleanup is needed.
--

Helm::
+
--
Optionally delete the shadow link (no longer needed):

[,bash]
----
kubectl exec --namespace <shadow-namespace> <shadow-pod-name> --container redpanda -- \
  rpk shadow delete <shadow-link-name>
----

For detailed command options, see xref:reference:rpk/rpk-shadow/rpk-shadow-delete.adoc[`rpk shadow delete`].
--
======

Document the time of failover initiation and completion, applications affected and recovery times, data loss estimates based on replication lag, and issues encountered during failover.

== Troubleshoot

include::troubleshoot:partial$errors-and-solutions.adoc[tags=shadow-link-failover]

include::manage:partial$shadowing/failover-next-steps.adoc[]

For general failover concepts and procedures, see xref:manage:disaster-recovery/shadowing/failover.adoc[].
