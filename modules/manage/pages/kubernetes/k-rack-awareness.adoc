= Enable Rack Awareness in Kubernetes
:description: Enable rack awareness to place partition replicas across different failure zones.
:page-context-links: [{"name": "Linux", "to": "manage:rack-awareness.adoc" },{"name": "Kubernetes", "to": "manage:kubernetes/k-rack-awareness.adoc" } ]
:page-tags: ["Kubernetes", "Helm configuration"]
:page-aliases: manage:kubernetes/kubernetes-rack-awareness.adoc
:page-categories: Management, High Availability
:env-kubernetes: true

include::manage:partial$rack-awareness.adoc[]

When you enable rack awareness in the Redpanda Helm chart, Kubernetes failure zones are treated as racks. Redpanda maps each rack to a failure zone and places partition replicas across them. For more details about Kubernetes failure zones, see the https://kubernetes.io/docs/setup/best-practices/multiple-zones/[Kubernetes documentation^].

== Prerequisites

You must have the following:

* Kubernetes cluster: Ensure you have a running Kubernetes cluster, either locally, such as with minikube or kind, or remotely.

* https://kubernetes.io/docs/tasks/tools/#kubectl[Kubectl^]: Ensure you have the `kubectl` command-line tool installed and configured to communicate with your cluster.

* If you use the xref:deploy:deployment-option/self-hosted/kubernetes/kubernetes-deploy.adoc[Redpanda Operator], you must deploy it with the `--set rbac.createRPKBundleCRs=true` flag to give it the required ClusterRoles to read node labels and annotations.

== Annotate or label Node resources

To assign a failure zone to your Kubernetes nodes, ensure that each of your Node resources is annotated or labeled with a key/value pair that corresponds to a failure zone. The Helm chart assigns each Redpanda broker to a particular rack, according to the failure zone of the Kubernetes node on which the broker is running.

Managed Kubernetes platforms usually annotate Node resources with the availability zone in which the node instance is hosted. For example `topology.kubernetes.io/zone=use-az1`.

To check the value of the `topology.kubernetes.io/zone` key, run the following:

[,bash]
----
kubectl get node \
-o=custom-columns=NODE:.metadata.name,ZONE:.metadata.annotations."topology\.kubernetes\.io/zone"
----

Example output:

[,text,role="no-copy"]
----
NODE                 ZONE
example-worker       use1-az1
example-worker2      use1-az2
example-worker3      use1-az3
----

If you don't see any values in the Zone column, make sure to annotate or label your Node resources with key/value pairs that correspond to your fault-tolerance requirements. For example:

[,bash]
----
kubectl annotate node example-worker topology.kubernetes.io/zone=rack1
kubectl annotate node example-worker2 topology.kubernetes.io/zone=rack2
kubectl annotate node example-worker3 topology.kubernetes.io/zone=rack3
----

If you're running Redpanda in Amazon AWS, you can use the following DaemonSet to label your Node resources with a zone ID:

.Example labeler DaemonSet
[%collapsible]
====
[,yaml]
----
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: labeler
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: labeler
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: labeler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: labeler
subjects:
  - kind: ServiceAccount
    name: labeler
    namespace: <namespace>
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: labeler
spec:
  selector:
    matchLabels:
      name: labeler
  template:
    metadata:
      labels:
        name: labeler
    spec:
      serviceAccountName: labeler
      initContainers:
        - name: labeler
          image: debian:bullseye-slim
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - --
          args:
            - |
              apt-get update -y && apt-get install -y curl jq apt-transport-https ca-certificates
              curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
              # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
              apt-get update -y && apt-get install -y kubectl
              # Get a token to be able to interact with the EC2 instance metadata API v2
              # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html
              TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
              # Get the current node's AZ ID
              AZ_ID=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -v "http://169.254.169.254/latest/meta-data/placement/availability-zone-id")
              kubectl label node/"$HOST" "topology.cloud.redpanda.com/zone-id=$AZ_ID" --overwrite
      containers:
        - name: pause
          image: debian:bullseye-slim
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
            - --
          args:
            - |
              trap : TERM INT; sleep infinity & wait
----
====

== Configure rack awareness

To enable rack awareness in your Redpanda cluster, configure the cluster with the key you used to annotate or label Node resources with the availability zone.

[tabs]
======
Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml,lines=9-15]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    rackAwareness:
      enabled: true
      nodeAnnotation: '<key>'
    serviceAccount:
      create: true
    rbac:
      enabled: true
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`rack-awareness.yaml`
[,yaml]
----
rackAwareness:
  enabled: true
  nodeAnnotation: '<key>'
serviceAccount:
  create: true
rbac:
  enabled: true
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
--values rack-awareness.yaml --reuse-values
```

--set::
+
[,bash,lines=2-5]
----
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set rackAwareness.enabled=true \
  --set rackAwareness.nodeAnnotation='<key>' \
  --set serviceAccount.create=true \
  --set rbac.enabled=true
----

====
--
======

- `rackAwareness.enabled` (*required*): Enables rack awareness for your Redpanda cluster.
- `rackAwareness.nodeAnnotation` (*required*): The label or annotation key to use to define racks. Defaults to the https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone[well-known^] `topology.kubernetes.io/zone` key.

NOTE: The `serviceAccount` and `rbac` configurations are required. These configurations allow the initialization container to securely read the node annotations using the Kubernetes API.

== Verify that rack awareness is enabled

After deploying Redpanda, make sure that rack awareness is enabled and configured on your Redpanda brokers.

Make sure that rack awareness has been enabled and configured on your Redpanda brokers:

```bash
kubectl --namespace <namespace> exec -i -t redpanda-0 -c redpanda -- \
  rpk cluster config get enable_rack_awareness
```

Example output:

[.no-copy]
----
true
----

== Next steps

Use rack awareness with xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Continuous Data Balancing] to continually maintain the configured replication level, even after a rack failure. For a given partition, Redpanda tries to move excess replicas from racks that have more than one replica to racks that have no replicas.

include::shared:partial$suggested-reading.adoc[]

- xref:reference:k-redpanda-helm-spec.adoc#rackawareness[Redpanda Helm Specification]
- xref:reference:k-crd.adoc[Redpanda CRD Reference]
