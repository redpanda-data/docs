= Manage Pod Resources in Kubernetes
:description: Configure your Pod resources such as memory, CPU, and storage.
:tags: ["Kubernetes", "Helm configuration"]
:page-aliases: manage:kubernetes/manage-resources.adoc
:page-categories: Management
:env-kubernetes: true

You can define requirements for Pod resources such as CPU, memory, and storage. Redpanda Data recommends that you determine and set these values before deploying the cluster, but you can also update the values on a running cluster.

== Prerequisites

- See xref:deploy:deployment-option/self-hosted/kubernetes/k-requirements.adoc[] for the minimum worker node, memory, CPU, and storage requirements.

- Make sure that your physical or virtual machines have enough resources to give to Redpanda. To see the available resources on the worker nodes that you have provisioned:
+
[,bash]
----
kubectl describe nodes
----

== Production considerations

- <<cpu, Enable CPU pinning>>. This configuration allows Redpanda to maintain exclusive access to designated CPU cores, improving throughput and reducing latency caused by CPU context switches.

- <<qos, Enable the `Guaranteed` quality of service class for Pods that run Redpanda>>. This setup ensures that the CPU and memory allocated to Redpanda are not subject to throttling or other contention issues, providing a stable and predictable performance environment.

- <<memory, Enable memory locking>>. This configuration prevents the operating system from paging out Redpanda's memory to disk, which can significantly impact performance.

== Limitations

You cannot decrease the number of CPU cores in a running cluster.

[[cpu]]
== CPU pinning

Redpanda is designed to maximize throughput by xref:get-started:architecture.adoc#tpc[pinning its threads to specific CPU cores]. However, CPU pinning is disabled by default in Kubernetes because it uses the https://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html[Completely Fair Scheduler (CFS)] to dynamically manage CPU allocation. The CFS distributes CPU time among all processes based on their priority and resource requests, which can lead to frequent context switches and migration of processes across different cores. This behavior can degrade the performance of Redpanda due to:

- **Loss of CPU cache affinity**: As processes move across cores, they lose the cache locality, leading to increased cache misses and higher latency.
- **Scheduling latency**: Dynamic CPU allocation can introduce delays as the scheduler determines which core a process should run on, based on current load and availability.

Enabling CPU pinning for Redpanda in Kubernetes requires the following:

- *Static CPU Manager policy*: To enable CPU pinning in Kubernetes, you must enable the https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy[`static` CPU Manager policy], which bypasses the default CPU allocation behavior managed by the CFS.
+
When the `static` policy is enabled, each Pod is assigned specific CPU cores, and these assignments remain constant throughout the Pod's lifecycle. This fixed allocation is crucial for Redpanda in production, because it maintains CPU cache affinity. Cache affinity refers to the performance benefit that arises when a process is able to repeatedly access data from the same CPU cache, rather than re-fetching the data from main memory or a distant cache associated with another CPU core.
- *Overprovisioning disabled (default)*: When overprovisioning is disabled in Redpanda, each thread runs on a dedicated core for maximum performance.
- *Guaranteed quality of service for all Pods that run Redpanda*: Only containers that are both part of a https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed[`Guaranteed` Pod] and have integer CPU requests are assigned exclusive CPUs.

=== Enable CPU pinning

. Ensure that your worker nodes have enough CPU cores to allocate to Redpanda. Sharing cores with other applications can negate the benefits of CPU pinning.

. Ensure the https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy[`static` policy] is enabled on your worker nodes.
+
[source,bash]
----
# Check if the static policy is enabled
cat /var/lib/kubelet/config.yaml | grep cpuManagerPolicy
----

. Configure the Redpanda container to use integer cores and disable overprovisioning:
+
[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      cpu:
        cores: <number-of-cpu-cores> <1>
        overprovisioned: false <2>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`cpu-cores.yaml`
[,yaml]
----
resources:
  cpu:
    cores: <number-of-cpu-cores> <1>
    overprovisioned: false <2>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values cpu-cores.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.cpu.cores=<number-of-cpu-cores> \ <1>
  --set resources.cpu.overprovisioned=false <2>
```

====
--
======
+
--
<1> Set the number of cores to an integer such as `4`. The more cores that are allocated to Redpanda, the more throughput it can support. For a description of CPU resource units, see the https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu[Kubernetes documentation^].
+
NOTE: If `resources.cpu.cores` is set using fractions, such as `1100m`, Redpanda may not use all the allocated CPU due to CFS quotas, leading to potential performance bottlenecks. When you use CPU fractions, <<pinning, CPU pinning>> is disabled.
<2> This default setting allows each Redpanda thread to run on a dedicated core for maximum performance. This setting is recommended for environments where high throughput and low latency are critical, and sufficient hardware resources are available.
--

. <<qos, Enable the `Guaranteed` QoS class for your Pods that run Redpanda>>.

[[pinning]]
=== Disable pinning in shared environments

On a dedicated worker node, CPU pinning allows maximum throughput, but in a development environment, where you may run other Pods on the same worker node as Redpanda, CPU pinning can cause your other processes to run slower than usual. In these shared environments, you can enable overprovisioning so that Redpanda reduces the aggressiveness of CPU usage to prevent resource hogging and ensuring fair CPU time distribution among all processes.

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      cpu:
        cores: <number-of-cpu-cores>
        overprovisioned: true
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`cpu-cores-overprovisioned.yaml`
[,yaml]
----
resources:
  cpu:
    cores: <number-of-cpu-cores>
    overprovisioned: true
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values cpu-cores-overprovisioned.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.cpu.cores=<number-of-cpu-cores> \
  --set resources.cpu.overprovisioned=true
```

====
--
======

If you're experimenting with Redpanda in Kubernetes, you can also set the number of CPU cores to millicores to automatically disable CPU pinning.

.`cpu-cores.yaml`
[,yaml]
----
resources:
  cpu:
    cores: 200m
----

[[memory]]
== Configure memory resources

Memory configuration is another critical aspect of resource management. On a worker node, Kubernetes and Redpanda processes are running at the same time, including the Seastar subsystem that is built into the Redpanda binary. Each of these processes consumes memory. You can configure the memory resources that are allocated to these processes.

By default, the Helm chart allocates 80% of the configured memory in `resources.memory.container` to Redpanda, with the remaining reserved for overhead such as the Seastar subsystem and other container processes.
Redpanda Data recommends this default setting.

NOTE: Although you can also allocate the exact amount of memory for Redpanda and the Seastar subsystem manually, Redpanda Data does not recommend this approach because setting the wrong values can lead to performance issues, instability, or data loss. As a result, this approach is not documented here.

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      memory:
        enable_memory_locking: true <1>
        container:
          # If omitted, the `min` value is equal to the `max` value (requested resources defaults to limits)
          # min:
          max: <number><unit> <2>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
resources:
  memory:
    enable_memory_locking: true <1>
    container:
      # If omitted, the `min` value is equal to the `max` value (requested resources defaults to limits)
      # min:
      max: <number><unit> <2>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.memory.enable_memory_locking=true \ <1>
  --set resources.memory.container.max=<number><unit> <2>
```

====
--
======

<1> The amount of memory to give Redpanda, Seastar, and the other container processes. You should give Redpanda at least 2 Gi of memory per core. Given that the Helm chart allocates 80% of the container's memory to Redpanda, leaving the rest for the Seastar subsystem and other processes, set this value to at least 2.5 Gi per core to ensure Redpanda has a full 2 Gi. Redpanda supports the following memory resource units: B, K, M, G, Ki, Mi, and Gi. Memory units are converted to the nearest whole MiB. For a description of memory resource units, see the https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[Kubernetes documentation^].
<2> For production, enable memory locking to prevent the operating system from paging out Redpanda's memory to disk, which can significantly impact performance.

[[qos]]
== Quality of service and resource guarantees

To ensure that Redpanda receives stable and consistent resources, set the https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed[quality of service (QoS) class] to `Guaranteed` by matching resource requests and limits on all containers in the Pods that run Redpanda.

Kubernetes uses QoS to decide which Pods to evict from a worker node that runs out of resources. When a worker node runs out of resources, Kubernetes evicts Pods with a `Guaranteed` QoS last. This stability is crucial for Redpanda because it requires consistent computational and memory resources to maintain high performance.

Kubernetes gives a Pod a `Guaranteed` QoS class when every container inside it has identical resource requests and limits set for both CPU and memory. This strict configuration signals to Kubernetes that these resources are critical and should not be throttled or reclaimed under normal operating conditions. To configure the Pods that run Redpanda with Guaranteed QoS, specify both resource requests and limits for all _enabled_ containers in the Pods, including helm_ref:statefulset.sideCars[] and helm_ref:statefulset.initContainers[].

For example:

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      memory:
        container:
          min: <redpanda-container-memory>
          max: <redpanda-container-memory>
    statefulset:
      sideCars:
        configWatcher:
          resources:
            requests:
              cpu: <redpanda-sidecar-container-cpu>
              memory: <redpanda-sidecar-container-memory>
            limits:
              cpu: <redpanda-sidecar-container-cpu> # Matches the request
              memory: <redpanda-sidecar-container-memory> # Matches the request
      initContainers:
        tuning:
          resources:
            requests:
              cpu: <redpanda-tuning-container-cpu>
              memory: <redpanda-tuning-container-memory>
            limits:
              cpu: <redpanda-tuning-container-cpu> # Matches the request
              memory: <redpanda-tuning-container-memory> # Matches the request
        setTieredStorageCacheDirOwnership:
          resources:
            requests:
              cpu: <redpanda-ts-cache-ownership-container-cpu>
              memory: <redpanda-ts-cache-ownership-container-memory>
            limits:
              cpu: <redpanda-ts-cache-ownership-container-cpu> # Matches the request
              memory: <redpanda-ts-cache-ownership-container-memory> # Matches the request
        configurator:
          resources:
            requests:
              cpu: <redpanda-configurator-container-cpu>
              memory: <redpanda-configurator-container-memory>
            limits:
              cpu: <redpanda-configurator-container-cpu> # Matches the request
              memory: <redpanda-configurator-container-memory> # Matches the request
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
resources:
  memory:
    container:
      min: <redpanda-container-memory>
      max: <redpanda-container-memory>
statefulset:
  sideCars:
    configWatcher:
      resources:
        requests:
          cpu: <redpanda-sidecar-container-cpu>
          memory: <redpanda-sidecar-container-memory>
        limits:
          cpu: <redpanda-sidecar-container-cpu> # Matches the request
          memory: <redpanda-sidecar-container-memory> # Matches the request
  initContainers:
    tuning:
      resources:
        requests:
          cpu: <redpanda-tuning-container-cpu>
          memory: <redpanda-tuning-container-memory>
        limits:
          cpu: <redpanda-tuning-container-cpu> # Matches the request
          memory: <redpanda-tuning-container-memory> # Matches the request
    setTieredStorageCacheDirOwnership:
      resources:
        requests:
          cpu: <redpanda-ts-cache-ownership-container-cpu>
          memory: <redpanda-ts-cache-ownership-container-memory>
        limits:
          cpu: <redpanda-ts-cache-ownership-container-cpu> # Matches the request
          memory: <redpanda-ts-cache-ownership-container-memory> # Matches the request
    configurator:
      resources:
        requests:
          cpu: <redpanda-configurator-container-cpu>
          memory: <redpanda-configurator-container-memory>
        limits:
          cpu: <redpanda-configurator-container-cpu> # Matches the request
          memory: <redpanda-configurator-container-memory> # Matches the request
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.memory.container.min=<redpanda-container-memory> \
  --set resources.memory.container.max=<redpanda-container-memory> \
  --set statefulset.sideCars.configWatcher.resources.requests.cpu=<redpanda-sidecar-container-cpu> \
  --set statefulset.sideCars.configWatcher.resources.requests.memory=<redpanda-sidecar-container-memory> \
  --set statefulset.sideCars.configWatcher.resources.limits.cpu=<redpanda-sidecar-container-cpu> \
  --set statefulset.sideCars.configWatcher.resources.limits.memory=<redpanda-sidecar-container-memory> \
  --set statefulset.initContainers.tuning.resources.requests.cpu=<redpanda-tuning-container-cpu> \
  --set statefulset.initContainers.tuning.resources.requests.memory=<redpanda-tuning-container-memory> \
  --set statefulset.initContainers.tuning.resources.limits.cpu=<redpanda-tuning-container-cpu> \
  --set statefulset.initContainers.tuning.resources.limits.memory=<redpanda-tuning-container-memory> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.requests.cpu=<redpanda-ts-cache-ownership-container-cpu> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.requests.memory=<redpanda-ts-cache-ownership-container-memory> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.limits.cpu=<redpanda-ts-cache-ownership-container-cpu> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.limits.memory=<redpanda-ts-cache-ownership-container-memory> \
  --set statefulset.initContainers.configurator.resources.requests.cpu=<redpanda-configurator-container-cpu> \
  --set statefulset.initContainers.configurator.resources.requests.memory=<redpanda-configurator-container-memory> \
  --set statefulset.initContainers.configurator.resources.limits.cpu=<redpanda-configurator-container-cpu> \
  --set statefulset.initContainers.configurator.resources.limits.memory=<redpanda-configurator-container-memory>
```

====
--
======

When the StatefulSet is deployed, make sure that the QoS for the Pods is set to `Guaranteed`:

[source,bash]
----
kubectl --namespace=<namespace> get pod <pod-name> -o jsonpath='{ .status.qosClass}{"\n"}'
----

== Configure storage capacity

Make sure to provision enough disk storage for your streaming workloads.

If you use PersistentVolumes, you can set the storage capacity for each volume. For instructions, see xref:manage:kubernetes/storage/k-configure-storage.adoc[].

include::shared:partial$suggested-reading.adoc[]

- xref:reference:k-redpanda-helm-spec.adoc#resources[Redpanda Helm Specification]
- xref:reference:k-crd.adoc[Redpanda CRD Reference]
