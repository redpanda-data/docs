= Manage Pod Resources in Kubernetes
:description: Configure your Pod resources such as memory, CPU, and storage.
:tags: ["Kubernetes", "Helm configuration"]
:page-aliases: manage:kubernetes/manage-resources.adoc
:page-categories: Management
:env-kubernetes: true

You can define requirements for Pod resources such as CPU, memory, and storage. Redpanda Data recommends that you determine and set these values before deploying the cluster, but you can also update the values on a running cluster.

== Prerequisites

- See xref:deploy:deployment-option/self-hosted/kubernetes/k-requirements.adoc[] for the minimum worker node, memory, CPU, and storage requirements.

- Make sure that your physical or virtual machines have enough resources to give to Redpanda. To see the available resources on the worker nodes that you have provisioned:
+
[,bash]
----
kubectl describe nodes
----

== Production considerations

- <<qos, Enable the `Guaranteed` quality of service class for Pods that run Redpanda>>. This setup ensures that the CPU and memory allocated to Redpanda are not subject to throttling or other contention issues, providing a stable and predictable performance environment.

- <<memory, Enable memory locking>>. This configuration prevents the operating system from paging out Redpanda's memory to disk, which can significantly impact performance.

[[memory]]
== Configure memory resources

When deploying Redpanda, you must reserve sufficient memory for both Redpanda and other system processes. Redpanda uses the Seastar framework to manage memory through two important flags. In Kubernetes, the values of these flags are usually set for you, depending on how you configure the Redpanda CRD or the Helm chart.

- **`--memory`**: When set, explicitly defines the Redpanda heap size.
- **`--reserve-memory`**: When set, reserves a specific amount of memory for system overhead. If not set, a reserve is automatically calculated.

.Learn more about these Seastar flags
[%collapsible]
====

[cols="1,1,2"]
|===
|                        | **`--memory` Not set**
|                       **`--memory` Set (M)**

| **`--reserve-memory` Not set**  
| Heap size = available memory - calculated reserve
| Heap size = exactly M (if M plus calculated reserve ≤ available memory). Otherwise, startup fails

| **`--reserve-memory` set \(R)**
| Heap size = available memory - R
| Heap size = exactly M (if M + R ≤ available memory). Otherwise, startup fails
|===

Definitions and behavior:

- **Available memory**: The memory remaining after subtracting system requirements, such as `/proc/sys/vm/min_free_kbytes`, from the total or cgroup-limited memory.
- **Calculated reserve**: The greater of 1.5 GiB or 7% of _available memory_ used when `--reserve-memory` is not explicitly set.

====

**Legacy (default) behavior**: By default, the Helm chart and the Redpanda CRD define memory requirements using the `resources.memory.container` configuration, which allocates:

- 80% of the container memory to `--memory`
- 20% to `--reserve-memory`

CAUTION: This default legacy behavior is for backward compatibility only. **Do not use it in production.**

**Production recommendation**: Use `resources.requests.memory` for production deployments to ensure predictable memory allocation by letting Kubernetes schedule and enforce resource limits. When you set a value for `resources.requests.memory`:

- Kubernetes allocates 90% of the requested memory to the Redpanda heap (using `--memory`).
+
The remaining 10% of the requested memory is reserved for container overhead. This allocation ensures that system processes and runtime requirements have sufficient memory while providing a predictable, dedicated heap for Redpanda.
- The `--reserve-memory` value is fixed at 0 because Kubernetes already manages container overhead.

CAUTION: Avoid manually setting the `--memory` and `--reserve-memory` flags unless absolutely necessary. Incorrect settings can lead to performance issues, instability, or data loss.


[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    statefulset:
      additionalRedpandaCmdFlags:
        - '--lock-memory' <1>
    resources:
      requests:
        # Allocates 90% to the --memory Seastar flag
        memory: <number><unit> <2>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
statefulset:
  additionalRedpandaCmdFlags:
    - '--lock-memory' <1>
resources:
  requests:
    # Allocates 90% to the --memory Seastar flag
    memory: <number><unit> <2>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set statefulset.additionalRedpandaCmdFlags=="{--lock-memory}" \ <1>
  --set resources.requests.memory=<number><unit> <2>
```

====
--
======

<1> Enabling memory locking prevents the operating system from paging out Redpanda's memory to disk. This can significantly improve performance by ensuring Redpanda has uninterrupted access to its allocated memory.

<2> Allocate at least 2.22 Gi of memory per core to ensure Redpanda has the 2 Gi per core it requires after accounting for the 90% allocation to the `--memory` flag.
+
Redpanda supports the following memory resource units: B, K, M, G, Ki, Mi, and Gi.
+
Memory units are truncated to the nearest whole MiB. For example, a memory request of 1024 KiB will result in 1 MiB being allocated. For a description of memory resource units, see the https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[Kubernetes documentation^].

[[qos]]
== Quality of service and resource guarantees

To ensure that Redpanda receives stable and consistent resources, set the https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed[quality of service (QoS) class] to `Guaranteed` by matching resource requests and limits on all containers in the Pods that run Redpanda.

Kubernetes uses QoS to decide which Pods to evict from a worker node that runs out of resources. When a worker node runs out of resources, Kubernetes evicts Pods with a `Guaranteed` QoS last. This stability is crucial for Redpanda because it requires consistent computational and memory resources to maintain high performance.

Kubernetes gives a Pod a `Guaranteed` QoS class when every container inside it has identical resource requests and limits set for both CPU and memory. This strict configuration signals to Kubernetes that these resources are critical and should not be throttled or reclaimed under normal operating conditions. To configure the Pods that run Redpanda with `Guaranteed` QoS, specify both resource requests and limits for all _enabled_ containers in the Pods.

For example:

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      requests:
        cpu: <number-of-cpu-cores>
        memory: <redpanda-container-memory>
      limits:
        cpu: <number-of-cpu-cores> # Matches the request
        memory: <redpanda-container-memory> # Matches the request
    statefulset:
      sideCars:
        configWatcher:
          resources:
            requests:
              cpu: <redpanda-sidecar-container-cpu>
              memory: <redpanda-sidecar-container-memory>
            limits:
              cpu: <redpanda-sidecar-container-cpu> # Matches the request
              memory: <redpanda-sidecar-container-memory> # Matches the request
      initContainers:
        tuning:
          resources:
            requests:
              cpu: <redpanda-tuning-container-cpu>
              memory: <redpanda-tuning-container-memory>
            limits:
              cpu: <redpanda-tuning-container-cpu> # Matches the request
              memory: <redpanda-tuning-container-memory> # Matches the request
        setTieredStorageCacheDirOwnership:
          resources:
            requests:
              cpu: <redpanda-ts-cache-ownership-container-cpu>
              memory: <redpanda-ts-cache-ownership-container-memory>
            limits:
              cpu: <redpanda-ts-cache-ownership-container-cpu> # Matches the request
              memory: <redpanda-ts-cache-ownership-container-memory> # Matches the request
        configurator:
          resources:
            requests:
              cpu: <redpanda-configurator-container-cpu>
              memory: <redpanda-configurator-container-memory>
            limits:
              cpu: <redpanda-configurator-container-cpu> # Matches the request
              memory: <redpanda-configurator-container-memory> # Matches the request
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
resources:
  requests:
    cpu: <number-of-cpu-cores>
    memory: <redpanda-container-memory>
  limits:
    cpu: <number-of-cpu-cores> # Matches the request
    memory: <redpanda-container-memory> # Matches the request
statefulset:
  sideCars:
    configWatcher:
      resources:
        requests:
          cpu: <redpanda-sidecar-container-cpu>
          memory: <redpanda-sidecar-container-memory>
        limits:
          cpu: <redpanda-sidecar-container-cpu> # Matches the request
          memory: <redpanda-sidecar-container-memory> # Matches the request
  initContainers:
    tuning:
      resources:
        requests:
          cpu: <redpanda-tuning-container-cpu>
          memory: <redpanda-tuning-container-memory>
        limits:
          cpu: <redpanda-tuning-container-cpu> # Matches the request
          memory: <redpanda-tuning-container-memory> # Matches the request
    setTieredStorageCacheDirOwnership:
      resources:
        requests:
          cpu: <redpanda-ts-cache-ownership-container-cpu>
          memory: <redpanda-ts-cache-ownership-container-memory>
        limits:
          cpu: <redpanda-ts-cache-ownership-container-cpu> # Matches the request
          memory: <redpanda-ts-cache-ownership-container-memory> # Matches the request
    configurator:
      resources:
        requests:
          cpu: <redpanda-configurator-container-cpu>
          memory: <redpanda-configurator-container-memory>
        limits:
          cpu: <redpanda-configurator-container-cpu> # Matches the request
          memory: <redpanda-configurator-container-memory> # Matches the request
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.requests.cpu=<number-of-cpu-cores> \
  --set resources.limits.cpu=<number-of-cpu-cores> \
  --set resources.requests.memory=<redpanda-container-memory> \
  --set resources.limits.memory=<redpanda-container-memory> \
  --set statefulset.sideCars.configWatcher.resources.requests.cpu=<redpanda-sidecar-container-cpu> \
  --set statefulset.sideCars.configWatcher.resources.requests.memory=<redpanda-sidecar-container-memory> \
  --set statefulset.sideCars.configWatcher.resources.limits.cpu=<redpanda-sidecar-container-cpu> \
  --set statefulset.sideCars.configWatcher.resources.limits.memory=<redpanda-sidecar-container-memory> \
  --set statefulset.initContainers.tuning.resources.requests.cpu=<redpanda-tuning-container-cpu> \
  --set statefulset.initContainers.tuning.resources.requests.memory=<redpanda-tuning-container-memory> \
  --set statefulset.initContainers.tuning.resources.limits.cpu=<redpanda-tuning-container-cpu> \
  --set statefulset.initContainers.tuning.resources.limits.memory=<redpanda-tuning-container-memory> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.requests.cpu=<redpanda-ts-cache-ownership-container-cpu> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.requests.memory=<redpanda-ts-cache-ownership-container-memory> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.limits.cpu=<redpanda-ts-cache-ownership-container-cpu> \
  --set statefulset.initContainers.setTieredStorageCacheDirOwnership.resources.limits.memory=<redpanda-ts-cache-ownership-container-memory> \
  --set statefulset.initContainers.configurator.resources.requests.cpu=<redpanda-configurator-container-cpu> \
  --set statefulset.initContainers.configurator.resources.requests.memory=<redpanda-configurator-container-memory> \
  --set statefulset.initContainers.configurator.resources.limits.cpu=<redpanda-configurator-container-cpu> \
  --set statefulset.initContainers.configurator.resources.limits.memory=<redpanda-configurator-container-memory>
```

====
--
======

When the StatefulSet is deployed, make sure that the QoS for the Pods is set to `Guaranteed`:

[source,bash]
----
kubectl --namespace=<namespace> get pod <pod-name> -o jsonpath='{ .status.qosClass}{"\n"}'
----

== Configure storage capacity

Make sure to provision enough disk storage for your streaming workloads.

If you use PersistentVolumes, you can set the storage capacity for each volume. For instructions, see xref:manage:kubernetes/storage/k-configure-storage.adoc[].

[[pinning]]
== Run Redpanda in shared environments

If Redpanda runs in a shared environment, where multiple applications run on the same worker node, you can make Redpanda less aggressive in CPU usage by enabling overprovisioning. This adjustment ensures a fairer distribution of CPU time among all processes, improving overall system efficiency at the cost of Redpanda's performance.

You can enable overprovisioning by either setting the CPU request to a fractional value less than 1 or enabling the `--overprovisioned` flag.

NOTE: You cannot enable overprovisioning when both `resources.requests` and `resources.limits` are set. When both of these configurations are set, the `resources.cpu` parameter (including cores) is ignored.

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha2
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    statefulset:
      additionalRedpandaCmdFlags:
        - '--overprovisioned'
    resources:
      cpu:
        cores: <number-of-cpu-cores>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`cpu-cores-overprovisioned.yaml`
[,yaml]
----
statefulset:
  additionalRedpandaCmdFlags:
    - '--overprovisioned'
resources:
  cpu:
    cores: <number-of-cpu-cores>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values cpu-cores-overprovisioned.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.cpu.cores=<number-of-cpu-cores> \
  --set statefulset.additionalRedpandaCmdFlags=="{--overprovisioned}"
```

====
--
======

If you're experimenting with Redpanda in Kubernetes, you can also set the number of CPU cores to millicores to automatically enable overprovisioning.

.`cpu-cores.yaml`
[,yaml]
----
resources:
  cpu:
    cores: 200m
----

include::shared:partial$suggested-reading.adoc[]

- xref:reference:k-redpanda-helm-spec.adoc#resources[Redpanda Helm Specification]
- xref:reference:k-crd.adoc[Redpanda CRD Reference]
