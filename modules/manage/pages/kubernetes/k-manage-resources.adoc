= Manage Pod Resources in Kubernetes
:description: Configure your Pod resources such as memory, CPU, and storage.
:tags: ["Kubernetes", "Helm configuration"]
:page-aliases: manage:kubernetes/manage-resources.adoc
:page-categories: Management
:env-kubernetes: true

You can define requirements for Pod resources such as CPU, memory, and storage. Redpanda Data recommends that you determine and set these values before deploying the cluster, but you can also update the values on a running cluster.

NOTE: In a running Redpanda cluster, you cannot decrease the number of CPU cores. You can only increase the number of CPU cores.

To see the available resources on the worker nodes that you have provisioned:

[,bash]
----
kubectl describe nodes
----

== Prerequisites

See xref:deploy:deployment-option/self-hosted/kubernetes/k-requirements.adoc[] for memory, CPU, and storage.

Make sure that your physical or virtual machines have enough resources to give to Redpanda. You assign resources to each Redpanda broker using the `values.yaml` settings. If you need to replace a worker node with one that has more resources, xref:./decommission-brokers.adoc[decommission the brokers] before you replace the worker nodes.

== Configure CPU resources

By default, Redpanda pins its threads to all cores that you allocate to it. The more cores that are allocate to Redpanda, the more throughput it can support. You can configure the CPU resources allocated to each Redpanda broker by overriding the default values. For a description of CPU resource units, see the https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu[Kubernetes documentation^].

For production clusters, you should never set `resources.cpu.cores` to anything less than full cores.

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      cpu:
        cores: <number-of-cpu-cores>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`cpu-cores.yaml`
[,yaml]
----
resources:
  cpu:
    cores: <number-of-cpu-cores>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values cpu-cores.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.cpu.cores=<number-of-cpu-cores>
```

====
--
======

IMPORTANT: You cannot decrease the number of CPU cores in a running cluster.

[TIP]
====
If you're experimenting with Redpanda in Kubernetes, you can set the number of CPU cores to millicores, which represent fractions of CPU time.

.`cpu-cores.yaml`
[,yaml]
----
resources:
  cpu:
    cores: 200m
----

When you use millicores, <<pinning, CPU pinning>> is disabled.
====

[[pinning]]
=== Disable pinning in shared environments

By default, Redpanda pins its threads to all cores that you allocate to it.
On a dedicated worker node, CPU pinning allows maximum throughput, but in a development environment, where you may run other Pods on the same worker node as Redpanda, CPU pinning can cause your processes to run slower than usual. In these shared environments, you can tell Redpanda not to pin its threads or memory, and reduce the amount of polling it does to a minimum.

NOTE: In production environments, you must run Redpanda brokers on dedicated worker nodes. See xref:deploy:deployment-option/self-hosted/kubernetes/k-requirements.adoc[Kubernetes Cluster Requirements].

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      cpu:
        cores: <number-of-cpu-cores>
        overprovisioned: true
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`cpu-cores-overprovisioned.yaml`
[,yaml]
----
resources:
  cpu:
    cores: <number-of-cpu-cores>
    overprovisioned: true
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values cpu-cores-overprovisioned.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.cpu.cores=<number-of-cpu-cores> \
  --set resources.cpu.overprovisioned=true
```

====
--
======

== Configure memory resources

On a worker node, Kubernetes and Redpanda processes are running at the same time, including the Seastar subsystem that is built into the Redpanda binary. Each of these processes consumes memory. You can configure the memory resources that are allocated to these processes.

By default, the Helm chart allocates 80% of the configured memory to Redpanda, leaving the rest for the Seastar subsystem and other container processes. Redpanda recommends this default setting.

NOTE: Although you can also allocate the exact amount of memory for Redpanda and the Seastar subsystem manually, Redpanda does not recommend this approach because setting the wrong values can lead to performance issues, instability, or data loss. As a result, this approach is not documented here.

Redpanda supports the following memory resource units: B, K, M, G, Ki, Mi, and Gi. Memory units are converted to the nearest whole MiB. For a description of memory resource units, see the https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[Kubernetes documentation^].

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      memory:
        container:
          # If omitted, the `min` value is equal to the `max` value (requested resources defaults to limits)
          # min:
          max: <number><unit>
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
resources:
  memory:
    container:
      # If omitted, the `min` value is equal to the `max` value (requested resources defaults to limits)
      # min:
      max: <number><unit>
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.memory.container.max=<number><unit>
```

====
--
======

== Set Guaranteed QoS for Redpanda Pods

Kubernetes uses https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed[quality of service (QoS) classes] to decide which Pods to evict from a node that runs out of resources. When a node runs out of resources, Kubernetes evicts Pods with a `Guaranteed` QoS last. This stability is crucial for Redpanda because it requires consistent computational and memory resources to maintain high performance.

The `Guaranteed` QoS class is achieved when every container in a Pod has identical resource requests and limits set for both CPU and memory. This strict configuration signals to Kubernetes that these resources are critical and should not be throttled or reclaimed under normal operating conditions.

To configure the Pods that run Redpanda with Guaranteed QoS, specify both resource requests and limits for all _enabled_ containers in the Pods, including helm_ref:statefulset.sideCars[].

For example:

[tabs]
======
Helm + Operator::
+
--
.`redpanda-cluster.yaml`
[,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  chartRef: {}
  clusterSpec:
    resources:
      memory:
        container:
          min: <number><unit>
          max: <number><unit>
    statefulset:
      sideCars:
        configWatcher:
          enabled: true
          resources:
            requests:
              cpu: "500m"
              memory: "256Mi"
            limits:
              cpu: "500m"  # Matches the request
              memory: "256Mi"  # Matches the request
----

```bash
kubectl apply -f redpanda-cluster.yaml --namespace <namespace>
```

--
Helm::
+
--
[tabs]
====
--values::
+
.`memory.yaml`
[,yaml]
----
resources:
  memory:
    container:
      min: <number><unit>
      max: <number><unit>
statefulset:
  sidecars:
----
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values memory.yaml --reuse-values
```

--set::
+
```bash
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --set resources.memory.container.min=<number><unit> \
  --set resources.memory.container.max=<number><unit> \
```

====
--
======

IMPORTANT: Ensure that the `requests` are equal to `limits` for both CPU and memory. This configuration prevents Kubernetes from overcommitting resources on a node, providing a stable environment for your Redpanda workload.

[source,bash]
----
kubectl --namespace=<namespace> get pod <pod-name> -o jsonpath='{ .status.qosClass}{"\n"}'
----

=== Static CPU assignments

In addition to setting `Guaranteed` QoS, you can configure static CPU assignments by specifying whole integer values for `resources.cpu.cores` can further enhance performance. This approach aligns with Linux's Completely Fair Scheduler (CFS) by avoiding fractional CPU allocations, which can lead to thread interruptions and reduced throughput.

When Redpanda is configured with whole CPU cores, it can effectively manage these cores without interference from the CFS bandwidth control, which is particularly important for achieving maximum data processing throughput.

Example configuration in `redpanda-cluster.yaml`:

[source,yaml]
----
apiVersion: cluster.redpanda.com/v1alpha1
kind: Redpanda
metadata:
  name: redpanda
spec:
  resources:
    cpu:
      cores: 4  # Specify as an even integer
----

TIP: Consider enabling Kubernetes's Static CPU Manager policy on your cluster nodes to provide exclusive CPU allocation for performance-sensitive applications like Redpanda. This setup is beneficial in production environments where resource predictability is crucial.

If `resources.cpu.cores` is set using fractions (e.g., `1100m` for 1.1 cores), Redpanda may not fully utilize the allocated CPU due to CFS quotas, leading to potential performance bottlenecks. Always aim for whole number assignments to ensure that CPU resources are fully utilized by Redpanda.

For further details on Kubernetes resource management and QoS classes, see the official Kubernetes documentation: https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/[Quality of Service for Pods^].

== Configure storage capacity

Make sure to provision enough disk storage for your streaming workloads.

If you use PersistentVolumes, you can set the storage capacity for each volume. For instructions, see xref:manage:kubernetes/storage/k-configure-storage.adoc[].

include::shared:partial$suggested-reading.adoc[]

- xref:reference:k-redpanda-helm-spec.adoc#resources[Redpanda Helm Specification]
- xref:reference:k-crd.adoc[Redpanda CRD Reference]
