= Access Topic Data in Iceberg Tables
:description: Learn how to access Redpanda topic data stored in Iceberg tables, using table metadata or a catalog integration.
:page-categories: Iceberg, Tiered Storage, Management, High Availability, Data Replication, Integration

To read from the Redpanda-generated xref:manage:iceberg/topic-iceberg-integration.adoc[Iceberg table], your Iceberg-compatible client or tool needs access to the catalog to retrieve the table metadata and know the current state of the table. The catalog provides the current table metadata, which includes locations for all the table's data files. You can configure Redpanda to either connect to a REST-based catalog, or use a file-system based catalog. 

For production usage, Redpanda recommends a REST catalog integration to manage Iceberg metadata so you can more easily facilitate governance, ensure discoverability, and maintain consistency across multiple engines or tools. However, if it is not possible to use a REST catalog, you may use the file-system based catalog, which does not require you to maintain a separate service to access the Iceberg data. In either case, you use the catalog to load, query, or refresh the Iceberg table as you produce to the Redpanda topic. See the documentation for your query engine or Iceberg-compatible tool for specific guidance on adding the Iceberg tables to your data warehouse or lakehouse using the catalog. 

After you have xref:manage:iceberg/topic-iceberg-integration.adoc#enable-iceberg-integration[enabled the Iceberg integration] for a topic and selected a catalog type, you cannot switch to another catalog type.

== Integrate REST catalog

Redpanda supports connecting to and updating an Iceberg catalog using a REST API, compliant with the https://github.com/apache/iceberg/blob/main/open-api/rest-catalog-open-api.yaml[Iceberg REST Catalog API specification^]. Use this catalog integration type with REST-enabled Iceberg catalog services, such as https://docs.databricks.com/en/data-governance/unity-catalog/index.html[Databricks Unity^] and https://other-docs.snowflake.com/en/opencatalog/overview[Snowflake Open Catalog^].

To integrate a REST catalog, set the following cluster configuration properties:

* config_ref:iceberg_catalog_type,true,properties/cluster-properties[`iceberg_catalog_type`]: `rest`
* config_ref:iceberg_rest_catalog_endpoint,true,properties/cluster-properties[`iceberg_rest_catalog_endpoint`]: The endpoint URL for your Iceberg catalog, which you either manage directly, or is managed by an external catalog service.
* config_ref:iceberg_rest_catalog_client_id,true,properties/cluster-properties[`iceberg_rest_catalog_client_id`]: The ID to connect to the REST catalog.
* config_ref:iceberg_rest_catalog_client_secret,true,properties/cluster-properties[`iceberg_rest_catalog_client_secret`]: The secret data to connect to the REST catalog.

For REST catalogs that use self-signed certificates, also configure these properties:

* config_ref:iceberg_rest_catalog_trust_file,true,properties/cluster-properties[`iceberg_rest_catalog_trust_file`]: The path to a file containing a certificate chain to trust for the REST catalog.
* config_ref:iceberg_rest_catalog_crl_file,true,properties/cluster-properties[`iceberg_rest_catalog_crl_file`]: The path to the certificate revocation list for the specified trust file.

See xref:reference:properties/cluster-properties.adoc[Cluster Configuration Properties] for the full list of cluster properties to configure for a catalog integration.

=== Example REST catalog configuration

For example, if you have Redpanda cluster configuration properties set to connect to a REST catalog:

[,yaml]
----
iceberg_catalog_type: rest 
iceberg_rest_catalog_endpoint: http://catalog-service:8181 
iceberg_rest_catalog_client_id: <rest-connection-user>
iceberg_rest_catalog_client_secret: <rest-connection-password>
----

And you use Apache Spark as a processing engine, configured to use a catalog named `streaming`:

[,spark]
----
spark.sql.catalog.streaming = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.streaming.type = rest
spark.sql.catalog.streaming.uri = http://catalog-service:8181
----

Using Spark SQL, you can query the Iceberg table directly by specifying the catalog name:

[,sql]
----
SELECT * FROM streaming.redpanda.<topic-name>;
----

Spark can use the REST catalog to automatically discover the topic's Iceberg table. Redpanda puts the Iceberg table into a namespace called `redpanda`, creating the namespace if necessary.

Depending on your processing engine, you may need to also create a table in the engine to point the data lakehouse at the table in the catalog. For an example, see xref:manage:iceberg/redpanda-topics-iceberg-snowflake-catalog.adoc[].

== Integrate file-system based catalog (`object_storage`)

By default, Iceberg topics use the file-system based catalog (config_ref:iceberg_catalog_type,true,properties/cluster-properties[`iceberg_catalog_type`] cluster configuration set to `object_storage`). Redpanda stores the table metadata in https://iceberg.apache.org/javadoc/1.5.0/org/apache/iceberg/hadoop/HadoopCatalog.html[HadoopCatalog^] format in the same object storage bucket or container as the data files.

If using the `object_storage` catalog type, you provide the object storage URL to an Iceberg client so it can access the catalog and data files for your Redpanda Iceberg tables.

=== Specify metadata location

The config_ref:iceberg_catalog_base_location,true,properties/cluster-properties[`iceberg_catalog_base_location`] property stores the base path for the file-system based catalog if using the `object_storage` catalog type. The default value is `redpanda-iceberg-catalog`. 

CAUTION: Do not change the `iceberg_catalog_base_location` value after you have enabled Iceberg integration for a topic.

If your engine needs the full JSON metadata path, use the following:

```
redpanda-iceberg-catalog/metadata/redpanda/<topic-name>/v<version-number>.metadata.json
```

This provides read access to all snapshots in the given version (denoted by `version-number`) of a table. 

NOTE: Redpanda automatically removes expired snapshots on a periodic basis. Snapshot expiry helps maintain a smaller metadata size and reduces the window available for xref:manage:iceberg/query-iceberg-topics.adoc#query-using-time-travel[time travel].  

=== Example file-system based catalog configuration

To configure Apache Spark to use a file system-based catalog, specify at least the following properties:

[,spark]
----
spark.sql.catalog.streaming = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.streaming.type = hadoop
# AWS S3 bucket used for object storage
spark.sql.catalog.streaming.warehouse = s3a://<bucket-name>/redpanda-iceberg-catalog
# You may need to configure additional properties based on your object storage provider.
# See https://spark.apache.org/docs/latest/configuration.html
----

Depending on your processing engine, you may need to also create a new table to point the data lakehouse at the table location.

== Next steps

* xref:manage:iceberg/query-iceberg-topics.adoc[]
* xref:manage:iceberg/redpanda-topics-iceberg-snowflake-catalog.adoc[]

