= Deserialize Messages in {ui}
:page-aliases: console:features/record-deserialization.adoc, manage:console/protobuf.adoc, reference:console/record-deserialization.adoc
// tag::single-source[]
:description: Learn how {ui} deserializes messages.

In Redpanda, the messages exchanged between producers and consumers contain raw bytes. Schemas work as an agreed-upon format, like a contract, for producers and consumers to serialize and deserialize those messages. If a producer breaks this contract, consumers can fail.

{ui} automatically tries to deserialize incoming messages and displays them in human-readable format. It tests different deserialization strategies until it finds one with no errors. If no deserialization attempts are successful, {ui} renders the byte array in a hex viewer. Sometimes, the payload is displayed in hex bytes because it's encrypted or because it uses a serializer that {ui} cannot deserialize. When this happens, {ui} displays troubleshooting information. You can also download the raw bytes of the message to feed it directly to your client deserializer or share it with a support team.

All deserialized messages are rendered as JSON objects and can be used as JavaScript objects in
xref:./programmable-push-filters.adoc[JavaScript filters (push filters)].

ifndef::env-cloud[]
== Prerequisites

Ensure that Redpanda Console is configured to handle the specific deserialization formats you plan to use, such as Avro, Protobuf, or MessagePack.
endif::[]

== Display messages in a specific format

{ui} tries to automatically identify the correct deserialization type by decoding the message's key, value, or header with all available deserialization methods. To display your messages in another format:

. Open your topic.
. Click the cog icon.
. Click *Deserialization*.
. Choose a new deserializer for either the keys or values in your messages.

Supported deserializers include:

* Plain text
* Kafka's internal binary formats; for example, the `__consumer_offsets` topic
* JSON
* JSON with Schema Registry encoding
* Smile
* XML
* Avro with Schema Registry encoding
* Protobuf
* Protobuf with Schema Registry encoding
* Messagepack (for topics explicitly enabled to test MessagePack)
* UTF-8 / strings
* `uint8`, `uint16`, `uint32`, `uint64`

ifndef::env-cloud[Encoding formats that are not self-contained require additional configuration.]

== Protobuf deserialization

If you have one or more topics that contain Protobuf serialized messages, {ui} can deserialize
the binary content into JSON, which makes the message human-readable and usable in
xref:reference:console/programmable-push-filters.adoc[JavaScript filters (push filters)]. Protobuf serialization is commonly used with Kafka clusters in two ways:

* With the Schema Registry (schema meta information gets embedded as part of the record's value)
* To write Protobuf serialized content into Kafka topics without a binary or custom wrapper

{ui} supports both formats. Most Kafka clients that serialize Protobuf messages put the serialized byte array into a binary wrapper that contains meta information, like the schema ID or the used prototypes, so the application that deserializes the Kafka records must recognize the format. The deserialization process requires {ui} to be aware of the used `.proto` files as well as a mapping of what prototype should be used for each topic. This information can either be sourced from the Schema Registry or it can be provided with additional configuration so the files can be pulled from the local file system or a GitHub repository.

NOTE: To support imports, all prototypes are first registered in a proto registry.
You must ensure that all imported prototypes are part of the repository. Standard types (such as Google's timestamp type) are included by default and don't need to be added.

=== Schema Registry

Messages that have been serialized using Confluent's KafkaProtobufSerializer can only be deserialized if the Schema Registry is configured to recognize them.
Unlike other providers, the Schema Registry does not require you to set up mappings that define which topics use which prototypes. Instead,
this information is inferred from the messages, and the Schema Registry finds the right prototype for deserialization.

ifdef::env-cloud[]
The Schema Registry is included with {ui} deployments.
endif::[]
ifndef::env-cloud[]
To set up the Schema Registry in {ui} with self-hosted Redpanda, see xref:console:ui/schema-reg.adoc[]. The Protobuf deserializer uses the same Schema Registry client that is configured under `kafka.schemaRegistry`.

For more details, see xref:console:config/deserialization.adoc[].
endif::[]

== Suggested reading

* xref:manage:schema-reg/schema-reg-overview.adoc[]

// end::single-source[]