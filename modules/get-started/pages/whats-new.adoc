= What's New
:description: Summary of new features and updates in the release.
:page-aliases: get-started:whats-new-233.adoc, get-started:whats-new-241.adoc

This topic includes new content added in version {page-component-version}. For a complete list of all product updates, see the https://github.com/redpanda-data/redpanda/releases/[Redpanda release notes^]. See also:

* xref:redpanda-cloud:get-started:whats-new-cloud.adoc[]
* xref:redpanda-cloud:get-started:cloud-overview.adoc#redpanda-cloud-vs-self-managed-feature-compatibility[Redpanda Cloud vs Self-Managed feature compatibility]

== Iceberg improvements

xref:manage:iceberg/topic-iceberg-integration.adoc[Iceberg-enabled topics] now support custom partitioning for improved query performance, snapshot expiry, and a dead-letter queue for invalid records. Schema evolution is also supported with schema mutations implemented according to the Iceberg specification.

== Protobuf normalization in Schema Registry

Redpanda now supports normalization of Protobuf schemas in the Schema Registry. You can normalize Avro, JSON, and Protobuf schemas both during registration and lookup. For more information, see the xref:manage:schema-reg/schema-reg-overview.adoc#schema-normalization[Schema Registry overview], and the xref:api:ROOT:pandaproxy-schema-registry.adoc[Schema Registry API reference].

== SASL/PLAIN authentication

You now can configure Kafka clients to authenticate using xref:manage:security/authentication#enable-sasl.adoc[SASL/PLAIN] with a single account using the same username and password. Unlike SASL/SCRAM, which uses a challenge response with hashed credentials, SASL/PLAIN transmits plaintext passwords. You enable SASL/PLAIN by appending `PLAIN` to the list of SASL mechanisms.

== New cluster properties

The following cluster properties are new in this version:

* `iceberg_invalid_record_action`

== Client quota properties removed

The following client configuration properties were deprecated in version 24.2.1, and have been removed in this release:

* `kafka_client_group_byte_rate_quota`
* `kafka_client_group_fetch_byte_rate_quota`
* `target_quota_byte_rate`
* `target_fetch_quota_byte_rate`
* `kafka_admin_topic_api_rate`

Use xref:reference:rpk/rpk-cluster/rpk-cluster-quotas.adoc[`rpk cluster quotas`] to manage xref:manage:cluster-maintenance/manage-throughput.adoc#client-throughput-limits[client throughput limits] based on the Kafka API.