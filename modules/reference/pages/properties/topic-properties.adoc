= Topic Configuration Properties
:page-aliases: reference:topic-properties.adoc
:description: Reference of topic configuration properties.

A topic-level property sets a Redpanda or Kafka configuration for a particular topic.

Many topic-level properties have corresponding xref:manage:cluster-maintenance/cluster-property-configuration.adoc[cluster properties] that set a default value for all topics of a cluster. To customize the value for a topic, you can set a topic-level property that overrides the value of the corresponding cluster property.

NOTE: All topic properties take effect immediately after being set. 

|===
| Topic property | Corresponding cluster property

| <<cleanuppolicy,`cleanup.policy`>>
| xref:./cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]

| <<flushbytes,`flush.bytes`>>
| xref:./cluster-properties.adoc#raft_replica_max_pending_flush_bytes[`raft_replica_max_pending_flush_bytes`]

| <<flushms,`flush.ms`>>
| xref:./cluster-properties.adoc#raft_replica_max_flush_delay_ms[`raft_replica_max_flush_delay_ms`]

| <<initialretentionlocaltargetms,`initial.retention.local.target.ms`>>
| xref:./cluster-properties.adoc#initial_retention_local_target_ms_default[`initial_retention_local_target_ms_default`]

| <<retentionbytes,`retention.bytes`>>
| xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]

| <<retentionms,`retention.ms`>>
| xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]

| <<segmentms,`segment.ms`>>
| xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]

| <<segmentbytes,`segment.bytes`>>
| xref:reference:properties/cluster-properties.adoc#log_segment_size[`log_segment_size`]

| <<compressiontype,`compression.type`>>
| xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]

| <<messagetimestamptype,`message.timestamp.type`>>
| xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]

| <<maxmessagebytes,`max.message.bytes`>>
| xref:reference:properties/cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]

| <<replicationfactor,`replication.factor`>>
| xref:./cluster-properties.adoc#default_topic_replication[`default_topic_replication`]

| <<writecaching,`write.caching`>>
| xref:./cluster-properties.adoc#write_caching_default[`write_caching_default`]
|===

[NOTE]
====
The `SOURCE` output of the xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe <topic>`] command describes how the property is set for the topic:

* `DEFAULT_CONFIG` is set by a Redpanda default.
* `DYNAMIC_TOPIC_CONFIG` is set by the user specifically for the topic and overrides inherited default configurations, such as a default or a cluster-level property.

Although xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] doesn't report `replication.factor` as a configuration, `replication.factor` can indeed be set by using the xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`] command.
====

== Examples

The following examples show how to configure topic-level properties. Set a topic-level property for a topic to override the value of corresponding cluster property.

=== Create topic with topic properties

To set topic properties when creating a topic, use the xref:reference:rpk/rpk-topic/rpk-topic-create.adoc[rpk topic create] command with the `-c` option.

For example, to create a topic with the `cleanup.policy` property set to `compact`:

[tabs]
====
Local::
+
--

```bash
rpk topic create -c cleanup.policy=compact <topic-name>
```

--
Kubernetes::
+
--

```bash
kubectl exec <pod-name> -- rpk topic create -c cleanup.policy=compact<topic-name>
```

--
====

To configure multiple properties for a topic, use the `-c` option for each property.

For example, to create a topic with all necessary properties for Tiered Storage:

[tabs]
====
Local::
+
--

```bash
rpk topic create -c redpanda.remote.recovery=true -c redpanda.remote.write=true -c redpanda.remote.read=true <topic-name>
```

--
Kubernetes::
+
--

```bash
kubectl exec <pod-name> -- rpk topic create -c redpanda.remote.recovery=true -c redpanda.remote.write=true -c redpanda.remote.read=true <topic-name>
```

--
====

=== Modify topic properties

To modify topic properties of an existing topic, use the xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[rpk topic alter-config] command.

For example, to modify a topic's `retention.ms` property:

[tabs]
====
Local::
+
--

```bash
rpk topic alter-config <topic-name> --set retention.ms=<retention-time>
```

--
Kubernetes::
+
--

```bash
kubectl exec <pod-name> -- rpk topic alter-config <topic-name> --set retention.ms=<retention-time>
```

--
====

== Properties

This section describes all supported topic-level properties.

=== Disk space properties

Configure properties to manage the disk space used by a topic:

- Clean up log segments by deletion and/or compaction (<<cleanuppolicy, `cleanup.policy`>>).
- Retain logs up to a maximum size per partition before cleanup (<<retentionbytes, `retention.bytes`>>).
- Retain logs for a maximum duration before cleanup (<<retentionms, `retention.ms`>>).
- Periodically close an active log segment (<<segmentms, `segment.ms`>>).
- Limit the maximum size of an active log segment (<<segmentbytes, `segment.bytes`>>).
- Cache batches until the segment appender chunk is full, instead of fsyncing for every `acks=all` write (<<writecaching,`write.caching`>>). With `write.caching` enabled, fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.

---

[[cleanuppolicy]]
==== cleanup.policy

The cleanup policy to apply for log segments of a topic.

When `cleanup.policy` is set, it overrides the cluster property xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`] for the topic.

**Default**: `[delete]`

**Values**:

- `[delete]` - Deletes data according to size-based or time-based retention limits, or both.
- `[compact]` - Deletes data according to a key-based retention policy, discarding all but the latest value for each key.
- `[compact,delete]` - The latest values are kept for each key, while the remaining data is deleted according to retention limits.

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]
- xref:manage:tiered-storage.adoc#compacted-topics-in-tiered-storage[Compacted topics in Tiered Storage]

---

[[flushms]]
==== flush.ms

The maximum delay (in ms) between two subsequent fsyncs. After this delay, the log is automatically fsynced.

**Default**: `100`

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc[]

---

[[flushbytes]]
==== flush.bytes

The maximum bytes not fsynced per partition. If this configured threshold is reached, the log is automatically fsynced, even though it wasn't explicitly requested.

**Default**: `262144`

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc[]

---

[[retentionbytes]]
==== retention.bytes

A size-based retention limit that configures the maximum size that a topic partition can grow before becoming eligible for cleanup.

If `retention.bytes` is set to a positive value, it overrides the cluster property xref:cluster-properties.adoc#retention_bytes[`retention_bytes`] for the topic, and the total retained size for the topic is `retention.bytes` multiplied by the number of partitions for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, cleanup occurs when either limit is reached.

**Default**: null

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---

[[retentionms]]
==== retention.ms

A time-based retention limit that configures the maximum duration that a log's segment file for a topic is retained before it becomes eligible to be cleaned up. To consume all data, a consumer of the topic must read from a segment before its `retention.ms` elapses, otherwise the segment may be compacted and/or deleted. If a non-positive value, no per-topic limit is applied.

If `retention.ms` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`] for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, the earliest occurring limit applies.

**Default**: null

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---

[[segmentms]]
==== segment.ms

The maximum duration that a log segment of a topic is active (open for writes and not deletable). A periodic event, with `segment.ms` as its period, forcibly closes the active segment and transitions, or rolls, to a new active segment. The closed (inactive) segment is then eligible to be cleaned up according to cleanup and retention properties.

If set to a positive duration, `segment.ms` overrides the cluster property xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`] and its lower and upper bounds set by xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`] and xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`], respectively.

**Default**: null

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#log-rolling[Log rolling]

---

[[segmentbytes]]
==== segment.bytes

The maximum size of an active log segment for a topic. When the size of an active segment exceeds `segment.bytes`, the segment is closed and a new active segment is created. The closed, inactive segment is then eligible to be cleaned up according to retention properties.

When `segment.bytes` is set to a positive value, it overrides the cluster property xref:reference:properties/cluster-properties.adoc#log_segment_size[`log_segment_size`] for the topic.

**Default**: null

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]
- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]
- xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---

[[writecaching]]
==== write.caching

The write caching mode to apply to a topic. 

When `write.caching` is set, it overrides the cluster property xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.

**Default**: `false`

**Values**:

- `true` - Enables write caching for a topic, according to <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>.
- `false` - Disables write caching for a topic, according to <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>.

**Related topics**:

- xref:develop:config-topics.adoc#configure-write-caching[Write caching]

---

=== Message properties

Configure properties for the messages of a topic:

- Compress a message or batch to reduce storage space and increase throughput (<<compressiontype, `compression.type`>>).
- Set the source of a message's timestamp (<<messagetimestamptype, `message.timestamp.type`>>).
- Set the maximum size of a message (<<maxmessagebytes, `max.message.bytes`>>).

[[compressiontype]]
==== compression.type

The type of compression algorithm to apply for all messages of a topic. When a compression type is set for a topic, producers compress and send messages, nodes (brokers) store and send compressed messages, and consumers receive and uncompress messages.

Enabling compression reduces message size, which improves throughput and decreases storage for messages with repetitive values and data structures. The trade-off is increased CPU utilization and network latency to perform the compression. You can also enable producer batching to increase compression efficiency, since the messages in a batch likely have repeated data that can be compressed.

When `compression.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`] for the topic.

NOTE: The valid values of `compression.type` are taken from `log_compression_type` and differ from Kafka's compression types.

**Default**: `none`

**Values**:

- `none`
- `gzip`
- `lz4`
- `snappy`
- `zstd`
- `producer`

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]
- xref:develop:produce-data/configure-producers.adoc#commonly-used-producer-configuration-options[Common producer configuration options]

---

[[messagetimestamptype]]
==== message.timestamp.type

The source of a message's timestamp: either the message's creation time or its log append time.

When `message.timestamp.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`] for the topic.

**Default**: `CreateTime`

**Values**:

- `CreateTime`
- `LogAppendTime`

---

[[maxmessagebytes]]
==== max.message.bytes

The maximum size of a message or batch of a topic. If a compression type is enabled, `max.message.bytes` sets the maximum size of the compressed message or batch.

If `max.message.bytes` is set to a positive value, it overrides the cluster property xref:reference:properties/cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`] for the topic.

**Default**: null

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]

---

=== Tiered Storage properties

Configure properties to manage topics for xref:manage:tiered-storage.adoc[Tiered Storage]:

- Upload and fetch data to and from object storage for a topic (<<redpandaremotewrite, `redpanda.remote.write`>> and <<redpandaremoteread, `redpanda.remote.read`>>).
- Configure size-based and time-based retention properties for local storage of a topic (<<retentionlocaltargetbytes, `retention.local.target.bytes`>> and <<retentionlocaltargetms, `retention.local.target.ms`>>).
- Recover or reproduce data for a topic from object storage (<<redpandaremoterecovery, `redpanda.remote.recovery`>>).
- Delete data from object storage for a topic when it's deleted from local storage (<<redpandaremotedelete, `redpanda.remote.delete`>>).

[[redpandaremotewrite]]
==== redpanda.remote.write

A flag for enabling Redpanda to upload data for a topic from local storage to object storage. When set to `true` together with <<redpandaremoteread, `redpanda.remote.read`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

[[redpandaremoteread]]
==== redpanda.remote.read

A flag for enabling Redpanda to fetch data for a topic from object storage to local storage. When set to `true` together with <<redpandaremotewrite, `redpanda.remote.write`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

[[initialretentionlocaltargetbytes]]
==== initial.retention.local.target.bytes

A size-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

**Default**: null

**Related topics**:

- xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---

[[initialretentionlocaltargetms]]
==== initial.retention.local.target.ms

A time-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

**Default**: null

**Related topics**:

- xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---

[[retentionlocaltargetbytes]]
==== retention.local.target.bytes

A size-based retention limit for Tiered Storage that configures the maximum size that a topic partition in local storage can grow before becoming eligible for cleanup. It applies per partition and is equivalent to <<retentionbytes, `retention.bytes`>> without Tiered Storage.

**Default**: null

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

[[retentionlocaltargetms]]
==== retention.local.target.ms

A time-based retention limit for Tiered Storage that sets the maximum duration that a log's segment file for a topic is retained in local storage before it's eligible for cleanup. This property is equivalent to <<retentionms, `retention.ms`>> without Tiered Storage.

**Default**: 86400000

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

[[redpandaremoterecovery]]
==== redpanda.remote.recovery

A flag that enables the recovery or reproduction of a topic from object storage for Tiered Storage. The recovered data is saved in local storage, and the maximum amount of recovered data is determined by the local storage retention limits of the topic.

TIP: You can only configure `redpanda.remote.recovery` when you create a topic. You cannot apply this setting to existing topics.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

[[redpandaremotedelete]]
==== redpanda.remote.delete

A flag that enables deletion of data from object storage for Tiered Storage when it's deleted from local storage for a topic.

NOTE: `redpanda.remote.delete` doesn't apply to Remote Read Replica topics: a Remote Read Replica topic isn't deleted from object storage when this flag is `true`.

**Default**:

- `false` for topics created using Redpanda 22.2 or earlier.
- `true` for topics created in Redpanda 22.3 and later, including new topics on upgraded clusters.

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---

=== Remote Read Replica properties

Configure properties to manage topics for xref:manage:remote-read-replicas.adoc[Remote Read Replicas].

==== redpanda.remote.readreplica

The name of the object storage bucket for a Remote Read Replica topic.

CAUTION: Setting `redpanda.remote.readreplica` together with either `redpanda.remote.read` or `redpanda.remote.write` results in an error.

**Default**: null

**Related topics**:

- xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---

=== Apache Iceberg integration properties

Integrate Redpanda topics as Iceberg tables.

==== redpanda.iceberg.mode

Enable the Iceberg integration for the topic. You can choose one of three modes.

**Default**: `disabled`

**Values**:

- `key_value`: Creates an Iceberg table with a `Key` column and a `Value` column. Redpanda stores the raw topic content in the `Value` column.
- `value_schema_id_prefix`: Creates an Iceberg table whose structure matches the Redpanda schema for the topic, with columns corresponding to each field. Redpanda parses the topic values per field and stores them in the corresponding table columns. 
- `disabled`: Disables writing to an Iceberg table for the topic.

**Related topics**:

- xref:manage:topic-iceberg-integration.adoc[]

==== redpanda.iceberg.delete

Whether the corresponding Iceberg table is deleted upon deleting the topic.

**Default**: `true`

**Related topics**:

- xref:manage:topic-iceberg-integration.adoc[]

==== redpanda.iceberg.partition.spec

The https://iceberg.apache.org/docs/nightly/partitioning/[partitioning^] specification for the Iceberg table.

**Default**: `(hour(redpanda.timestamp))`

**Related topics**:

- xref:manage:topic-iceberg-integration.adoc[]

==== redpanda.iceberg.invalid.record.action

Whether to write invalid records to a dead-letter queue (DLQ).

**Default**: `dlq_table`

**Values**:

- `drop`: Disable the DLQ and drop invalid records.
- `dlq_table`: Write invalid records to a separate DLQ Iceberg table.

**Related topics**:

- xref:manage:topic-iceberg-integration.adoc[]

=== Redpanda topic properties

Configure Redpanda-specific topic properties.

[[redpandaleaderspreference]]
==== redpanda.leaders.preference

The preferred location (rack) for partition leaders of a topic.

This property inherits the value from the config_ref:default_leaders_preference,true,properties/cluster-properties[] cluster configuration property. You may override the cluster-wide setting by specifying the value for individual topics.

If the cluster configuration property config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, Leader Pinning is disabled across the cluster.

**Default**: `none`

**Values**:

- `none`: Opt out the topic from Leader Pinning.
- `racks:<rack1>[,<rack2>,...]`: Specify the preferred location (rack) of all topic partition leaders. The list can contain one or more rack IDs. If you specify multiple IDs, Redpanda tries to distribute the partition leader locations equally across brokers in these racks.

**Related topics**:

- xref:develop:produce-data/leader-pinning.adoc[Leader pinning]

---

==== replication.factor

The number of replicas of a topic to save in different nodes (brokers) of a cluster.

If `replication.factor` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication] for the topic.

NOTE: Although `replication.factor` isn't returned or displayed by xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] as a valid Kafka property, you can set it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]. When the `replication.factor` of a topic is altered, it isn't simply a property value that's updated, but rather the actual replica sets of topic partitions that are changed.

**Default**: null

**Related topics**:

- xref:develop:config-topics.adoc#choose-the-replication-factor.adoc[Choose the replication factor]
- xref:develop:config-topics.adoc#change-the-replication-factor[Change the replication factor]

---

[[deleteretentionms]]
==== delete.retention.ms

The retention time for tombstone records in a compacted topic. Redpanda removes tombstone records after the retention limit is exceeded.

If you have enabled Tiered Storage and set <<redpandaremoteread,`redpanda.remote.read`>> or <<redpandaremotewrite,`redpanda.remote.write`>> for the topic, you cannot enable tombstone removal. 

If both `delete.retention.ms` and the cluster property config_ref:tombstone_retention_ms,true,properties/cluster-properties[] are set, `delete.retention.ms` overrides the cluster level tombstone retention for an individual topic.

*Unit:* milliseconds

**Default**: null

**Related topics**:

- xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]

---

== Related topics

- xref:develop:produce-data/configure-producers.adoc[Configure Producers]
- xref:develop:config-topics.adoc[Manage Topics]
