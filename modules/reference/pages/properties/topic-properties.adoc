= Topic Configuration Properties
:page-aliases: reference:topic-properties.adoc
:description: Reference of topic configuration properties.

A topic-level property sets a Redpanda or Kafka configuration for a particular topic.

Many topic-level properties have corresponding xref:manage:cluster-maintenance/cluster-property-configuration.adoc[cluster properties] that set a default value for all topics of a cluster. To customize the value for a topic, you can set a topic-level property that overrides the value of the corresponding cluster property.

For information on how to configure topic properties, see xref:manage:cluster-maintenance/topic-property-configuration.adoc[].

NOTE: All topic properties take effect immediately after being set.

== Topic property mappings

|===
| Topic property | Corresponding cluster property

| <<cleanuppolicy,`cleanup.policy`>>
| xref:./cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]

| <<compactionstrategy,`compaction.strategy`>>
| xref:./cluster-properties.adoc#compaction_strategy[`compaction_strategy`]

| <<compressiontype,`compression.type`>>
| xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]

| <<deleteretentionms,`delete.retention.ms`>>
| xref:./cluster-properties.adoc#tombstone_retention_ms[`tombstone_retention_ms`]
| <<flushbytes,`flush.bytes`>>
| xref:./cluster-properties.adoc#flush_bytes[`flush_bytes`]

| <<flushms,`flush.ms`>>
| xref:./cluster-properties.adoc#flush_ms[`flush_ms`]

| <<initialretentionlocaltargetbytes,`initial.retention.local.target.bytes`>>
| xref:./cluster-properties.adoc#initial_retention_local_target_bytes[`initial_retention_local_target_bytes`]

| <<initialretentionlocaltargetms,`initial.retention.local.target.ms`>>
| xref:./cluster-properties.adoc#initial_retention_local_target_ms[`initial_retention_local_target_ms`]

| <<maxcompactionlagms,`max.compaction.lag.ms`>>
| xref:./cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]

| <<maxmessagebytes,`max.message.bytes`>>
| xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]

| <<messagetimestamptype,`message.timestamp.type`>>
| xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]

| <<mincleanabledirtyratio,`min.cleanable.dirty.ratio`>>
| xref:./cluster-properties.adoc#min_cleanable_dirty_ratio[`min_cleanable_dirty_ratio`]

| <<mincompactionlagms,`min.compaction.lag.ms`>>
| xref:./cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]

| <<replicationfactor,`replication.factor`>>
| xref:./cluster-properties.adoc#replication_factor[`replication_factor`]

| <<retentionbytes,`retention.bytes`>>
| xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]

| <<retentionlocaltargetbytes,`retention.local.target.bytes`>>
| xref:./cluster-properties.adoc#retention_local_target_bytes[`retention_local_target_bytes`]

| <<retentionlocaltargetms,`retention.local.target.ms`>>
| xref:./cluster-properties.adoc#retention_local_target_ms[`retention_local_target_ms`]

| <<retentionms,`retention.ms`>>
| xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]

| <<segmentbytes,`segment.bytes`>>
| xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]

| <<segmentms,`segment.ms`>>
| xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]

| <<writecaching,`write.caching`>>
| xref:./cluster-properties.adoc#write_caching_default[`write_caching_default`]

|===

== Topic properties

Properties are grouped by feature area. Within each group, properties are listed in alphabetical order.

---
=== Retention and Compaction Properties

These properties control how data is stored, for how long, and when it is deleted or compacted.

[[cleanuppolicy]]
==== cleanup.policy

The cleanup policy to apply for log segments of a topic.

When `cleanup.policy` is set, it overrides the cluster property xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`] for the topic.

include::develop:partial$topic-properties-warning.adoc[]

*Type:* string

*Accepted values:* [`delete`, `compact`, `compact,delete`]

**Default**: `delete`

**Values**:

- `delete` - Deletes data according to size-based or time-based retention limits, or both.
- `compact` - Deletes data according to a key-based retention policy, discarding all but the latest value for each key.
- `compact,delete` - The latest values are kept for each key, while the remaining data is deleted according to retention limits.

*Related cluster property:* xref:./cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]
- xref:manage:tiered-storage.adoc#compacted-topics-in-tiered-storage[Compacted topics in Tiered Storage]

---
[[compactionstrategy]]
==== compaction.strategy

Specifies the strategy used to determine which records to remove during log compaction. The compaction strategy controls how Redpanda identifies and removes duplicate records while preserving the latest value for each key.

*Type:* string

*Default:* `offset`

*Accepted values:*

* `offset` - Uses record offsets to determine which records to compact (default and currently the only supported strategy)

*Related cluster property:* xref:./cluster-properties.adoc#compaction_strategy[`compaction_strategy`]

---
[[deleteretentionms]]
==== delete.retention.ms

The retention time for tombstone records in a compacted topic. Redpanda removes tombstone records after the retention limit is exceeded.

If you have enabled Tiered Storage and set <<redpandaremoteread,`redpanda.remote.read`>> or <<redpandaremotewrite,`redpanda.remote.write`>> for the topic, you cannot enable tombstone removal.

If both `delete.retention.ms` and the cluster property config_ref:tombstone_retention_ms,true,properties/cluster-properties[] are set, `delete.retention.ms` overrides the cluster level tombstone retention for an individual topic.

*Type:* integer

*Accepted values:* [1, 9223372036854775] milliseconds

*Default*: null

*Related cluster property:* xref:./cluster-properties.adoc#tombstone_retention_ms[`tombstone_retention_ms`]

**Related topics**:

- xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]

---
[[maxcompactionlagms]]
==== max.compaction.lag.ms

The maximum amount of time (in ms) that a log segment can remain unaltered before it is eligible for compaction in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`] for the topic.

*Type:* integer

*Accepted values:* [`1`, `9223372036854775`]

**Default:* `9223372036854775`

*Related cluster property:* xref:./cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]

**Related topics**:

- xref:manage:cluster-maintenance/compaction-settings.adoc#configuration-options[Configure maximum compaction lag]

---
[[mincleanabledirtyratio]]
==== min.cleanable.dirty.ratio

The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic.

*Type:* number

*Accepted values:* [`0`, `1.0`]

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#min_cleanable_dirty_ratio[`min_cleanable_dirty_ratio`]

**Related topics**:

- xref:manage:cluster-maintenance/compaction-settings.adoc[]

---
[[mincompactionlagms]]
==== min.compaction.lag.ms

The minimum amount of time (in ms) that a log segment must remain unaltered before it can be compacted in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`] for the topic.

*Type:* integer

*Accepted values:* [`0`, `9223372036854775`]

**Default:* `0`

*Related cluster property:* xref:./cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]

**Related topics**:

- xref:manage:cluster-maintenance/compaction-settings.adoc#configure-min-compaction-lag[Configure minimum compaction lag]

---
[[retentionbytes]]
==== retention.bytes

A size-based retention limit that configures the maximum size that a topic partition can grow before becoming eligible for cleanup.

If `retention.bytes` is set to a positive value, it overrides the cluster property xref:cluster-properties.adoc#retention_bytes[`retention_bytes`] for the topic, and the total retained size for the topic is `retention.bytes` multiplied by the number of partitions for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, cleanup occurs when either limit is reached.

*Type:* integer

*Accepted values:* [1, 9223372036854775807] bytes

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---
[[retentionms]]
==== retention.ms

A time-based retention limit that configures the maximum duration that a log's segment file for a topic is retained before it becomes eligible to be cleaned up. To consume all data, a consumer of the topic must read from a segment before its `retention.ms` elapses, otherwise the segment may be compacted and/or deleted. If a non-positive value, no per-topic limit is applied.

If `retention.ms` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`] for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, the earliest occurring limit applies.

*Type:* integer

*Accepted values:* [1, 9223372036854775] milliseconds

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---
### Segment and Message Properties

These properties control the size and lifecycle of log segment files and settings for individual messages.

[[compressiontype]]
==== compression.type

IMPORTANT: This property is ignored regardless of the value specified. The behavior is always the same as the `producer` value. If producers send compressed data, Redpanda stores and serves it as-is; if producers send uncompressed data, Redpanda stores it uncompressed. Other listed values are accepted for Apache Kafka compatibility but are ignored by the broker. This property may appear in Admin API and `rpk topic describe` outputs for compatibility.

Redpanda does not enforce or change compression for stored data; configure compression in your producers.

Enabling compression reduces message size, which improves throughput and decreases storage for messages with repetitive values and data structures. The trade-off is increased CPU utilization and network latency to perform the compression. You can also enable producer batching to increase compression efficiency, since the messages in a batch likely have repeated data that can be compressed.

When `compression.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`] for the topic.

NOTE: The valid values of `compression.type` are taken from `log_compression_type` and differ from Kafka's compression types.

*Type:* string

*Accepted Values:* `producer`. The following values are accepted for Kafka compatibility but ignored by the broker: `gzip`, `snappy`, `lz4`, `zstd`, `none`.

**Default**: `none`

*Related cluster property:* xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]
- xref:develop:produce-data/configure-producers.adoc#commonly-used-producer-configuration-options[Common producer configuration options]

---
[[maxmessagebytes]]
==== max.message.bytes

The maximum size of a message or batch of a topic. If a compression type is enabled, `max.message.bytes` sets the maximum size of the compressed message or batch.

If `max.message.bytes` is set to a positive value, it overrides the cluster property xref:reference:properties/cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`] for the topic.

*Type:* integer

*Accepted values:* [1, 2147483647] bytes

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]

---
[[messagetimestamptype]]
==== message.timestamp.type

The source of a message's timestamp: either the message's creation time or its log append time.

When `message.timestamp.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`] for the topic.

*Type:* string

*Accepted values:* [`CreateTime`, `LogAppendTime`]

**Default**: `CreateTime`

**Values**:

- `CreateTime`
- `LogAppendTime`

*Related cluster property:* xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]

---
[[segmentbytes]]
==== segment.bytes

The maximum size of an active log segment for a topic. When the size of an active segment exceeds `segment.bytes`, the segment is closed and a new active segment is created. The closed, inactive segment is then eligible to be cleaned up according to retention properties.

When `segment.bytes` is set to a positive value, it overrides the cluster property xref:reference:properties/cluster-properties.adoc#log_segment_size[`log_segment_size`] for the topic.

*Type:* integer

*Accepted values:* [1, 9223372036854775807] bytes

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]
- xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]
- xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---
[[segmentms]]
==== segment.ms

The maximum duration that a log segment of a topic is active (open for writes and not deletable). A periodic event, with `segment.ms` as its period, forcibly closes the active segment and transitions, or rolls, to a new active segment. The closed (inactive) segment is then eligible to be cleaned up according to cleanup and retention properties.

If set to a positive duration, `segment.ms` overrides the cluster property xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`] and its lower and upper bounds set by xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`] and xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`], respectively.

*Type:* integer

*Accepted values:* [600000, 31536000000] milliseconds (clamped by cluster bounds)

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]

**Related topics**:

- xref:manage:cluster-maintenance/disk-utilization.adoc#log-rolling[Log rolling]

---
### Performance and Cluster Properties

These properties control disk flushing, replication, and how topics interact with the cluster.

[[flushbytes]]
==== flush.bytes

The maximum bytes not fsynced per partition. If this configured threshold is reached, the log is automatically fsynced, even though it wasn't explicitly requested.

*Type:* integer

*Accepted values:* [1, 9223372036854775807] bytes

**Default**: `262144`

*Related cluster property:* xref:./cluster-properties.adoc#flush_bytes[`flush_bytes`]

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc[]

---
[[flushms]]
==== flush.ms

The maximum delay (in ms) between two subsequent fsyncs. After this delay, the log is automatically fsynced.

*Type:* integer

*Accepted values:* [1, 9223372036854775] milliseconds

**Default**: `100`

*Related cluster property:* xref:./cluster-properties.adoc#flush_ms[`flush_ms`]

**Related topics**:

- xref:develop:produce-data/configure-producers.adoc[]

---
[[redpandaleaderspreference]]
==== redpanda.leaders.preference

The preferred location (rack) for partition leaders of a topic.

This property inherits the value from the config_ref:default_leaders_preference,true,properties/cluster-properties[] cluster configuration property. You may override the cluster-wide setting by specifying the value for individual topics.

If the cluster configuration property config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, Leader Pinning is disabled across the cluster.

**Default**: `none`

**Values**:

- `none`: Opt out the topic from Leader Pinning.
- `racks:<rack1>[,<rack2>,...]`: Specify the preferred location (rack) of all topic partition leaders. The list can contain one or more rack IDs. If you specify multiple IDs, Redpanda tries to distribute the partition leader locations equally across brokers in these racks.

**Related topics**:

- xref:develop:produce-data/leader-pinning.adoc[Leader pinning]

---
[[replicationfactor]]
==== replication.factor

The number of replicas of a topic to save in different nodes (brokers) of a cluster.

If `replication.factor` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication] for the topic.

NOTE: Although `replication.factor` isn't returned or displayed by xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] as a valid Kafka property, you can set it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]. When the `replication.factor` of a topic is altered, it isn't simply a property value that's updated, but rather the actual replica sets of topic partitions that are changed.

*Type:* integer

*Accepted values:* [`1`, `512`]

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#default_topic_replication[`default_topic_replication`]

**Related topics**:

- xref:develop:config-topics.adoc#choose-the-replication-factor.adoc[Choose the replication factor]
- xref:develop:config-topics.adoc#change-the-replication-factor[Change the replication factor]

---
[[writecaching]]
==== write.caching

The write caching mode to apply to a topic.

When `write.caching` is set, it overrides the cluster property xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.

*Type:* boolean

**Default**: `false`

**Values**:

- `true` - Enables write caching for a topic, according to <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>.
- `false` - Disables write caching for a topic, according to <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>.

*Related cluster property:* xref:./cluster-properties.adoc#write_caching_default[`write_caching_default`]

**Related topics**:

- xref:develop:config-topics.adoc#configure-write-caching[Write caching]

---
### Tiered Storage properties

Configure properties to manage topics for xref:manage:tiered-storage.adoc[Tiered Storage].

[[initialretentionlocaltargetbytes]]
==== initial.retention.local.target.bytes

A size-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

*Type:* integer

*Accepted values:* [1, 9223372036854775807] bytes

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#initial_retention_local_target_bytes[`initial_retention_local_target_bytes`]

**Related topics**:

- xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---
[[initialretentionlocaltargetms]]
==== initial.retention.local.target.ms

A time-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

*Type:* integer

*Accepted values:* [1, 9223372036854775] milliseconds

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#initial_retention_local_target_ms[`initial_retention_local_target_ms`]

**Related topics**:

- xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---
[[redpandaremotedelete]]
==== redpanda.remote.delete

A flag that enables deletion of data from object storage for Tiered Storage when it's deleted from local storage for a topic.

NOTE: `redpanda.remote.delete` doesn't apply to Remote Read Replica topics: a Remote Read Replica topic isn't deleted from object storage when this flag is `true`.

**Default**:

- `false` for topics created using Redpanda 22.2 or earlier.
- `true` for topics created in Redpanda 22.3 and later, including new topics on upgraded clusters.

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
[[redpandaremoteread]]
==== redpanda.remote.read

A flag for enabling Redpanda to fetch data for a topic from object storage to local storage. When set to `true` together with <<redpandaremotewrite, `redpanda.remote.write`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
[[redpandaremoterecovery]]
==== redpanda.remote.recovery

A flag that enables the recovery or reproduction of a topic from object storage for Tiered Storage. The recovered data is saved in local storage, and the maximum amount of recovered data is determined by the local storage retention limits of the topic.

TIP: You can only configure `redpanda.remote.recovery` when you create a topic. You cannot apply this setting to existing topics.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
[[redpandaremotewrite]]
==== redpanda.remote.write

A flag for enabling Redpanda to upload data for a topic from local storage to object storage. When set to `true` together with <<redpandaremoteread, `redpanda.remote.read`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

**Default**: false

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
[[retentionlocaltargetbytes]]
==== retention.local.target.bytes

A size-based retention limit for Tiered Storage that configures the maximum size that a topic partition in local storage can grow before becoming eligible for cleanup. It applies per partition and is equivalent to <<retentionbytes, `retention.bytes`>> without Tiered Storage.

*Type:* integer

*Accepted values:* [1, 9223372036854775807] bytes

**Default**: null

*Related cluster property:* xref:./cluster-properties.adoc#retention_local_target_bytes[`retention_local_target_bytes`]

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
[[retentionlocaltargetms]]
==== retention.local.target.ms

A time-based retention limit for Tiered Storage that sets the maximum duration that a log's segment file for a topic is retained in local storage before it's eligible for cleanup. This property is equivalent to <<retentionms, `retention.ms`>> without Tiered Storage.

*Type:* integer

*Accepted values:* [1, 9223372036854775] milliseconds

**Default**: 86400000

*Related cluster property:* xref:./cluster-properties.adoc#retention_local_target_ms[`retention_local_target_ms`]

**Related topics**:

- xref:manage:tiered-storage.adoc[Tiered Storage]

---
### Remote Read Replica properties

Configure properties to manage topics for xref:manage:remote-read-replicas.adoc[Remote Read Replicas].

[[redpandaremotereadreplica]]
==== redpanda.remote.readreplica

The name of the object storage bucket for a Remote Read Replica topic.

CAUTION: Setting `redpanda.remote.readreplica` together with either `redpanda.remote.read` or `redpanda.remote.write` results in an error.

**Default**: null

**Related topics**:

- xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---
### Apache Iceberg integration properties

Integrate Redpanda topics as Iceberg tables.

[[redpandaicebergdelete]]
==== redpanda.iceberg.delete

Whether the corresponding Iceberg table is deleted upon deleting the topic.

**Default**: `true`

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---
[[redpandaiceberginvalidrecordaction]]
==== redpanda.iceberg.invalid.record.action

Whether to write invalid records to a dead-letter queue (DLQ).

**Default**: `dlq_table`

**Values**:

- `drop`: Disable the DLQ and drop invalid records.
- `dlq_table`: Write invalid records to a separate DLQ Iceberg table.

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]

---
[[redpandaicebergmode]]
==== redpanda.iceberg.mode

Enable the Iceberg integration for the topic. You can choose one of four modes.

**Default**: `disabled`

**Values**:

- `key_value`: Creates an Iceberg table with a `Key` column and a `Value` column. Redpanda stores the raw topic content in the `Value` column.
- `value_schema_id_prefix`: Creates an Iceberg table whose structure matches the Redpanda schema for the topic, with columns corresponding to each field. Redpanda uses the Schema Registry wire format, consisting of the "magic byte" and schema ID encoded in the payload header, to parse the topic values per field and store them in the corresponding table columns.
- `value_schema_latest`: Creates an Iceberg table whose structure matches the latest schema version in Schema Registry that matches the subject name. This mode is compatible with Avro and Protobuf schemas and is used when you don't produce to the topic using the wire format. See xref:manage:iceberg/choose-iceberg-mode.adoc#override-value-schema-latest-default[Choose an Iceberg Mode] for details on using this mode.
- `disabled`: Disables writing to an Iceberg table for the topic.

**Related topics**:

- xref:manage:iceberg/choose-iceberg-mode.adoc[]
- xref:manage:iceberg/about-iceberg-topics.adoc[]

---
[[redpandaicebergpartitionspec]]
==== redpanda.iceberg.partition.spec

The link:https://iceberg.apache.org/docs/nightly/partitioning/[partitioning^] specification for the Iceberg table.

**Default**: `(hour(redpanda.timestamp))`

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc#use-custom-partitioning[Use custom partitioning]

---
[[redpandaicebergtargetlagms]]
==== redpanda.iceberg.target.lag.ms

Controls how often the data in the Iceberg table is refreshed with new data from the topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.

**Default**: `60000`

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]