= Cluster Configuration Properties
:page-aliases: reference:tunable-properties.adoc, reference:cluster-properties.adoc
:description: Reference of cluster configuration properties.

Cluster properties are configuration settings that control the behavior of a Redpanda cluster at a global level. Configuring cluster properties allows you to adapt Redpanda to specific workloads, optimize resource usage, and enable or disable features. 

For information on how to edit cluster properties, see xref:manage:cluster-maintenance/cluster-property-configuration.adoc[] or xref:manage:kubernetes/k-cluster-property-configuration.adoc[].

NOTE: Some cluster properties require that you restart the cluster for any updates to take effect. See the specific property details to identify whether or not a restart is required.

== Cluster configuration

=== abort_index_segment_size

Capacity (in number of txns) of an abort index segment.

Each partition tracks the aborted transaction offset ranges to help service client requests. If the number of transactions increases beyond this threshold, they are flushed to disk to ease memory pressure. Then they're loaded on demand. This configuration controls the maximum number of aborted transactions before they are flushed to disk.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `50000`

---

=== abort_timed_out_transactions_interval_ms

Interval, in milliseconds, at which Redpanda looks for inactive transactions and aborts them.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000` (10 s)

---

=== admin_api_require_auth

Whether Admin API clients must provide HTTP basic authentication headers.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== aggregate_metrics

Enable aggregation of metrics returned by the xref:reference:internal-metrics-reference.adoc[`/metrics`] endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== alive_timeout_ms

The amount of time since the last broker status heartbeat. After this time, a broker is considered offline and not alive.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000`

---

=== alter_topic_cfg_timeout_ms

The duration, in milliseconds, that Redpanda waits for the replication of entries in the controller log when executing a request to alter topic configurations. This timeout ensures that configuration changes are replicated across the cluster before the alteration request is considered complete.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000` (5 s)

---

=== append_chunk_size

Size of direct write operations to disk in bytes. A larger chunk size can improve performance for write-heavy workloads, but increase latency for these writes as more data is collected before each write operation. A smaller chunk size can decrease write latency, but potentially increase the number of disk I/O operations.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `16384`

---

=== audit_client_max_buffer_size

Defines the number of bytes allocated by the internal audit client for audit messages. When changing this, you must disable audit logging and then re-enable it for the change to take effect. Consider increasing this if your system generates a very large number of audit records in a short amount of time.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `16777216`

---

// tag::audit_enabled[]
=== audit_enabled

ifndef::env-cloud[]
include::reference:partial$enterprise-licensed-property.adoc[]
endif::[]

Enables or disables audit logging. When you set this to true, Redpanda checks for an existing topic named `_redpanda.audit_log`. If none is found, Redpanda automatically creates one for you.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

ifndef::env-cloud[]
*Enterprise license required*: `true`

*Default:* `false`
endif::[]

---

// end::audit_enabled[]


=== audit_enabled_event_types

List of strings in JSON style identifying the event types to include in the audit log. This may include any of the following: `management, produce, consume, describe, heartbeat, authenticate, schema_registry, admin`.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Default:* `[management, authenticate, admin]`

---


// tag::audit_excluded_principals[]
=== audit_excluded_principals

List of user principals to exclude from auditing.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

ifndef::env-cloud[]
*Default:* `null`
endif::[]

---

// end::audit_excluded_principals[]


// tag::audit_excluded_topics[]
=== audit_excluded_topics

List of topics to exclude from auditing.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

ifndef::env-cloud[]
*Default:* `null`
endif::[]

---

// end::audit_excluded_topics[]

// tag::audit_log_num_partitions[]
=== audit_log_num_partitions

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

Defines the number of partitions used by a newly-created audit topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics.

*Unit:* number of partitions per topic

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

ifndef::env-cloud[]
*Default:* `12`
endif::[]

---

// end::audit_log_num_partitions[]

=== audit_log_replication_factor

Defines the replication factor for a newly-created audit log topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics. Setting this value is optional. If a value is not provided, Redpanda will use the value specified for `internal_topic_replication_factor`.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `null`

---

=== audit_queue_drain_interval_ms

Interval, in milliseconds, at which Redpanda flushes the queued audit log messages to the audit log topic. Longer intervals may help prevent duplicate messages, especially in high throughput scenarios, but they also increase the risk of data loss during shutdowns where the queue is lost.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `500`

---

=== audit_queue_max_buffer_size_per_shard

Defines the maximum amount of memory in bytes used by the audit buffer in each shard. Once this size is reached, requests to log additional audit messages will return a non-retryable error. Limiting the buffer size per shard helps prevent any single shard from consuming excessive memory due to audit log messages.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `1048576`

---


=== auto_create_topics_enabled

Allow automatic topic creation. To prevent excess topics, this property is not supported on Redpanda Cloud BYOC and Dedicated clusters. You should explicitly manage topic creation for these Redpanda Cloud clusters.

If you produce to a topic that doesn't exist, the topic will be created with defaults if this property is enabled.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== cluster_id

NOTE: This property is read-only in Redpanda Cloud.

Cluster identifier.

*Requires restart:* No

*Gets restored during cluster restore:* No

*Visibility:* `user`

*Type:* string

*Default:* `null`

---


// tag::compacted_log_segment_size[]
=== compacted_log_segment_size

Size (in bytes) for each compacted log segment.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `268435456`

---

// end::compacted_log_segment_size[]

=== compaction_ctrl_backlog_size

Target backlog size for compaction controller. If not set the max backlog size is configured to 80% of total disk space available.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== compaction_ctrl_d_coeff

Derivative coefficient for compaction PID controller.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `0.2`

---

=== compaction_ctrl_i_coeff

Integral coefficient for compaction PID controller.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `0.0`

---

=== compaction_ctrl_max_shares

Maximum number of I/O and CPU shares that compaction process can use.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `1000`

---

=== compaction_ctrl_min_shares

Minimum number of I/O and CPU shares that compaction process can use.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `10`

---

=== compaction_ctrl_p_coeff

Proportional coefficient for compaction PID controller. This must be negative, because the compaction backlog should decrease when the number of compaction shares increases.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `-12.5`

---

=== compaction_ctrl_update_interval_ms

The interval (in milliseconds) for updating the controller responsible for compaction tasks. The controller uses this interval to decide how to prioritize background compaction work, which is essential for maintaining efficient storage use.

This is an internal-only configuration and should be enabled only after consulting with Redpanda support.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `30000` (30 s)

---

=== consumer_group_lag_collection_interval_sec

How often to run the collection loop when <<enable_consumer_group_metrics,`enable_consumer_group_metrics`>> contains `consumer_lag`.

Reducing the value of `consumer_group_lag_collection_interval_sec` increases the metric collection frequency, which may raise resource utilization. In most environments, this impact is minimal, but it's best practice to monitor broker resource usage in high-scale settings.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `60`

---

=== controller_backend_housekeeping_interval_ms

Interval between iterations of controller backend housekeeping loop.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1000` (1 s)

---

=== controller_log_accummulation_rps_capacity_acls_and_users_operations

Maximum capacity of rate limit accumulation in controller ACLs and users operations limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== controller_log_accummulation_rps_capacity_configuration_operations

Maximum capacity of rate limit accumulation in controller configuration operations limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== controller_log_accummulation_rps_capacity_move_operations

Maximum capacity of rate limit accumulation in controller move operations limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== controller_log_accummulation_rps_capacity_node_management_operations

Maximum capacity of rate limit accumulation in controller node management operations limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== controller_log_accummulation_rps_capacity_topic_operations

Maximum capacity of rate limit accumulation in controller topic operations limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== controller_snapshot_max_age_sec

Maximum amount of time before Redpanda attempts to create a controller snapshot after a new controller command appears.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `60`

---

=== core_balancing_continuous

include::reference:partial$enterprise-licensed-property.adoc[]

If set to `true`, move partitions between cores in runtime to maintain balanced partition distribution.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Enterprise license required*: `true`

*Default:* `false`

---

=== core_balancing_debounce_timeout

Interval, in milliseconds, between trigger and invocation of core balancing.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000` (10 s)

---

=== core_balancing_on_core_count_change

If set to `true`, and if after a restart the number of cores changes, Redpanda will move partitions between cores to maintain balanced partition distribution.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== cpu_profiler_enabled

Enables CPU profiling for Redpanda.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== cpu_profiler_sample_period_ms

The sample period for the CPU profiler.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `100`

---

=== create_topic_timeout_ms

Timeout, in milliseconds, to wait for new topic creation.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `2000`

---


// tag::data_transforms_binary_max_size[]
=== data_transforms_binary_max_size

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

The maximum size for a deployable WebAssembly binary that the broker can store.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `10485760`

---

// end::data_transforms_binary_max_size[]

=== data_transforms_commit_interval_ms

The commit interval at which data transforms progress.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---


// tag::data_transforms_enabled[]
=== data_transforms_enabled

Enables WebAssembly-powered data transforms directly in the broker. When `data_transforms_enabled` is set to `true`, Redpanda reserves memory for data transforms, even if no transform functions are currently deployed. This memory reservation ensures that adequate resources are available for transform functions when they are needed, but it also means that some memory is allocated regardless of usage.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

ifndef::env-cloud[]
*Default:* `false`
endif::[]

---

// end::data_transforms_enabled[]


=== data_transforms_logging_buffer_capacity_bytes

Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered log messages; that is, logs emitted but not yet produced.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `512000`

---

=== data_transforms_logging_flush_interval_ms

Flush interval for transform logs. When a timer expires, pending logs are collected and published to the `transform_logs` topic.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `500`

---

// tag::data_transforms_logging_line_max_bytes[]
=== data_transforms_logging_line_max_bytes

Transform log lines truncate to this length. Truncation occurs after any character escaping.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

ifndef::env-cloud[]
*Default:* `1024`
endif::[]

---

// end::data_transforms_logging_line_max_bytes[]

// tag::data_transforms_per_core_memory_reservation[]
=== data_transforms_per_core_memory_reservation

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

The amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

ifndef::env-cloud[]
*Default:* `20971520`
endif::[]

---

// end::data_transforms_per_core_memory_reservation[]

// tag::data_transforms_per_function_memory_limit[]
=== data_transforms_per_function_memory_limit

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

The amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Default:* `2097152`

---

// end::data_transforms_per_function_memory_limit[]


=== data_transforms_read_buffer_memory_percentage

include::reference:partial$internal-use-property.adoc[]

The percentage of available memory in the transform subsystem to use for read buffers.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `45`

---

=== data_transforms_runtime_limit_ms

The maximum amount of runtime to start up a data transform, and the time it takes for a single record to be transformed.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---

=== data_transforms_write_buffer_memory_percentage

include::reference:partial$internal-use-property.adoc[]

The percentage of available memory in the transform subsystem to use for write buffers.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `45`

---

=== datalake_coordinator_snapshot_max_delay_secs

Maximum amount of time the coordinator waits to snapshot after a command appears in the log.

*Unit*: seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `900`

---

=== datalake_scheduler_block_size_bytes

Size, in bytes, of each memory block reserved for record translation, as tracked by the datalake scheduler.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `4_mib`

---

=== datalake_scheduler_max_concurrent_translations

The maximum number of translations that the datalake scheduler will allow to run at a given time. If a translation is requested, but the number of running translations exceeds this value, the request will be put to sleep temporarily, polling until capacity becomes available.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `4`

---

=== datalake_scheduler_time_slice_ms

Time, in milliseconds, for a datalake translation as scheduled by the datalake scheduler. After a translation is scheduled, it will run until either the time specified has elapsed or all pending records on its source partition have been translated.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000`

---

=== debug_bundle_auto_removal_seconds

If set, how long debug bundles are kept in the debug bundle storage directory after they are created. If not set, debug bundles are kept indefinitely.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `null`

---

=== debug_bundle_storage_dir

Path to the debug bundle storage directory. Note: Changing this path does not clean up existing debug bundles. If not set, the debug bundle is stored in the Redpanda data directory specified in the redpanda.yaml broker configuration file.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

*Default:* `null`

---

=== debug_load_slice_warning_depth

The recursion depth after which debug logging is enabled automatically for the log reader.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `null`

---

=== default_leaders_preference

include::reference:partial$enterprise-licensed-property.adoc[]

Default settings for preferred location of topic partition leaders. It can be either "none" (no preference), or "racks:<rack1>,<rack2>,..." (prefer brokers with rack ID from the list).

The list can contain one or more rack IDs. If you specify multiple IDs, Redpanda tries to distribute the partition leader locations equally across brokers in these racks.

If config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, leader pinning is disabled across the cluster.

*Requires restart:* No

*Visibility:* `user`

*Enterprise license required*: Any value other than the default `none`

*Default:* `none`

*Related topics*:

- xref:develop:produce-data/leader-pinning.adoc[Leader pinning]

---

=== default_num_windows

Default number of quota tracking windows.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `10`

---


=== default_topic_partitions

Default number of partitions per topic.

*Unit:* number of partitions per topic

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `1`

---


// tag::default_topic_replications[]
=== default_topic_replications

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud. In Redpanda Cloud, all new topics are created with a replication factor of 3.
endif::[]

Default replication factor for new topics.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

ifndef::env-cloud[]
*Default:* `1`
endif::[]

---

// end::default_topic_replications[]

=== default_window_sec

Default quota tracking window size in milliseconds.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1000`

---

=== development_enable_cloud_topics

Enable cloud topics.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== disable_batch_cache

Disable batch cache in log manager.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== disable_cluster_recovery_loop_for_tests

include::reference:partial$internal-use-property.adoc[]

Disables the cluster recovery loop.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---


=== disable_metrics

Disable registering the metrics exposed on the internal `/metrics` endpoint.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== disable_public_metrics

Disable registering the metrics exposed on the `/public_metrics` endpoint.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== disk_reservation_percent

The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well 
as when cloud cache uses a dedicated disk. 

It is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.

*Unit*: percentage of total disk size.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `25.0`

---

=== enable_cluster_metadata_upload_loop

Enables cluster metadata uploads. Required for xref:manage:whole-cluster-restore.adoc[whole cluster restore].

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

// tag::enable_consumer_group_metrics[]
=== enable_consumer_group_metrics

List of enabled consumer group metrics. Accepted values include:

- `group`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_consumers[`redpanda_kafka_consumer_group_consumers`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_topics[`redpanda_kafka_consumer_group_topics`] metrics.
- `partition`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_committed_offset[`redpanda_kafka_consumer_group_committed_offset`] metric.
- `consumer_lag`: Enables the xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_max[`redpanda_kafka_consumer_group_lag_max`] and xref:reference:public-metrics-reference.adoc#redpanda_kafka_consumer_group_lag_sum[`redpanda_kafka_consumer_group_lag_sum`] metrics
+
Enabling `consumer_lag` may add a small amount of additional processing overhead to the brokers, especially in environments with a high number of consumer groups or partitions.
+
ifndef::env-cloud[]
Use the xref:reference:properties/cluster-properties.adoc#consumer_group_lag_collection_interval_sec[`consumer_group_lag_collection_interval_sec`] property to control the frequency of consumer lag metric collection.
endif::[]

*Requires restart:* No

*Visibility:* `user`

*Type:* array

ifndef::env-cloud[]
*Default:* `["group", "partition"]`

*Related topics*:

- xref:manage:monitoring.adoc#consumers[Monitor consumer group lag]
endif::[]

ifdef::env-cloud[]
*Related topics*:

- xref:manage:monitor-cloud.adoc#consumers[Monitor consumer group lag]
endif::[]

---
// end::enable_consumer_group_metrics[]

=== enable_controller_log_rate_limiting

Limits the write rate for the controller log.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== enable_idempotence

Enable idempotent producers.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---


=== enable_host_metrics

Enable exporting of some host metrics like `/proc/diskstats`, `/proc/snmp` and `/proc/net/netstat`.

Host metrics are prefixed with xref:reference:internal-metrics-reference.adoc#vectorized_host_diskstats_discards[`vectorized_host`] and are available on the `/metrics` endpoint.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== enable_leader_balancer

Enable automatic leadership rebalancing.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== enable_metrics_reporter

Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by <<metrics_reporter_report_interval,`metrics_reporter_report_interval`>>.

[NOTE]
====
The cluster metrics of the metrics reporter are different from xref:manage:monitoring.adoc[monitoring metrics].

* The metrics reporter exports customer usage metrics for consumption by Redpanda Data.
* Monitoring metrics are exported for consumption by Redpanda users.
====

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== enable_mpx_extensions

Enable Redpanda extensions for MPX.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== enable_pid_file

Enable PID file. You should not need to change.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== enable_rack_awareness

Enable rack-aware replica assignment.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== enable_sasl

Enable SASL authentication for Kafka connections. Authorization is required to modify this property. See also <<kafka_enable_authorization,`kafka_enable_authorization`>>.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== enable_schema_id_validation

ifndef::env-cloud[]
include::reference:partial$enterprise-licensed-property.adoc[]
endif::[]

Mode to enable server-side schema ID validation.

*Related topics*:

* xref:manage:schema-reg/schema-id-validation.adoc[Server-Side Schema ID Validation]

*Requires restart:* No

*Visibility:* `user`

*Accepted Values*:

* `none`: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified.
* `redpanda`: Schema validation is enabled. Only Redpanda topic properties are accepted.
* `compat`: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.

*Enterprise license required*: `compat` , `redpanda`

*Default:* `none`

---


=== enable_transactions

Enable transactions (atomic writes).

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---


=== enable_usage

Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== features_auto_enable

Whether new feature flags auto-activate after upgrades (true) or must wait for manual activation via the Admin API (false).

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---


=== fetch_max_bytes

Maximum number of bytes returned in a fetch request.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `57671680`

---


=== fetch_pid_d_coeff

Derivative coefficient for fetch PID controller.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `0.0`

---

=== fetch_pid_i_coeff

Integral coefficient for fetch PID controller.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `0.01`

---


=== fetch_pid_max_debounce_ms

The maximum debounce time the fetch PID controller will apply, in milliseconds.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `100`

---


=== fetch_pid_p_coeff

Proportional coefficient for fetch PID controller.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `100.0`

---

=== fetch_pid_target_utilization_fraction

A fraction, between 0 and 1, for the target reactor utilization of the fetch scheduling group.

*Unit:* fraction

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `0.2`

---


=== fetch_read_strategy

The strategy used to fulfill fetch requests.

* `polling`: Repeatedly polls every partition in the request for new data. The polling interval is set by <<fetch_reads_debounce_timeout,`fetch_reads_debounce_timeout`>> (deprecated).

* `non_polling`: The backend is signaled when a partition has new data, so Redpanda doesn’t need to repeatedly read from every partition in the fetch. Redpanda Data recommends using this value for most workloads, because it can improve fetch latency and CPU utilization.

* `non_polling_with_debounce`: This option behaves like `non_polling`, but it includes a debounce mechanism with a fixed delay specified by <<fetch_reads_debounce_timeout,`fetch_reads_debounce_timeout`>> at the start of each fetch. By introducing this delay, Redpanda can accumulate more data before processing, leading to fewer fetch operations and returning larger amounts of data. Enabling this option reduces reactor utilization, but it may also increase end-to-end latency.

*Requires restart:* No

*Visibility:* `tunable`

*Accepted Values:* `polling`, `non_polling`, `non_polling_with_debounce`

*Default:* `non_polling`

---


=== fetch_reads_debounce_timeout

Time to wait for the next read in fetch requests when the requested minimum bytes was not reached.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1`

---

// end::fetch_reads_debounce_timeout[]

=== fetch_session_eviction_timeout_ms

Time duration after which the inactive fetch session is removed from the fetch session cache. Fetch sessions are used to implement the incremental fetch requests where a consumer does not send all requested partitions to the server but the server tracks them for the consumer.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `60000`

---


=== group_initial_rebalance_delay

Delay added to the rebalance phase to wait for new members.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---


=== group_max_session_timeout_ms

The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `300000`

---


=== group_min_session_timeout_ms

The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `6000`

---


=== group_new_member_join_timeout

Timeout for new member joins.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000`

---


=== group_offset_retention_check_ms

Frequency rate at which the system should check for expired group offsets.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `600000` (10 min)

---


=== group_offset_retention_sec

Consumer group offset retention seconds. To disable offset retention, set this to null.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `604800` (one week)

---


=== group_topic_partitions

Number of partitions in the internal group membership topic.

*Unit:* number of partitions per topic

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `16`

---


=== health_manager_tick_interval

How often the health manager runs.

*Unit:* milliseconds
*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `180000` (3 min)

---

=== health_monitor_max_metadata_age

Maximum age of the metadata cached in the health monitor of a non-controller broker.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

// tag::http_authentication[]
=== http_authentication

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

ifndef::env-cloud[]
include::reference:partial$enterprise-licensed-property.adoc[]
endif::[]

A list of supported HTTP authentication mechanisms. Accepted Values: `BASIC`, `OIDC`.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Accepted Values:* `BASIC`, `OIDC`

ifndef::env-cloud[]
*Enterprise license required*: `OIDC`

*Default:* `[basic]`
endif::[]

---

// end::http_authentication[]

// tag::iceberg_backlog_controller_p_coeff[]
=== iceberg_backlog_controller_p_coeff

Proportional coefficient for the Iceberg backlog controller. Number of shares assigned to the datalake scheduling group will be proportional to the backlog size error. A negative value means larger and faster changes in the number of shares in the datalake scheduling group.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `-0.0003`

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_backlog_controller_p_coeff[]

// tag::iceberg_catalog_base_location[]
=== iceberg_catalog_base_location

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

Base path for the object-storage-backed Iceberg catalog. After Iceberg is enabled, do not change this value.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `redpanda-iceberg-catalog`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]
- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_catalog_base_location[]


=== iceberg_catalog_commit_interval_ms

The frequency at which the Iceberg coordinator commits topic files to the catalog. This is the interval between commit transactions across all topics monitored by the coordinator, not the interval between individual commits.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `60000`

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---


// tag::iceberg_catalog_type[]
=== iceberg_catalog_type

Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: `rest`, `object_storage`.

NOTE: You must set <<iceberg_rest_catalog_endpoint,`iceberg_rest_catalog_endpoint`>> at the same time that you set `iceberg_catalog_type` to `rest`.

*Requires restart:* Yes

*Visibility:* `user`

*Accepted values:* `rest`, `object_storage`

ifndef::env-cloud[]
*Default:* `object_storage`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_catalog_type[]

// tag::iceberg_default_partition_spec[] 
=== iceberg_default_partition_spec

ifndef::env-cloud[]
Default value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-partition-spec[`redpanda.iceberg.partition.spec`] topic property that determines the partition spec for the Iceberg table corresponding to the topic.
endif::[]

ifdef::env-cloud[]
Default value for the `redpanda.iceberg.partition.spec` topic property that determines the partition spec for the Iceberg table corresponding to the topic.
endif::[]

*Requires restart:* No

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `(hour(redpanda.timestamp))`

Partitions the topic by extracting the hour from `redpanda.timestamp`, grouping records by hour to optimize queries.
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc#enable-iceberg-integration[Enable Iceberg integration]

---

// end::iceberg_default_partition_spec[]

// tag::iceberg_delete[]
=== iceberg_delete

Default value for the `redpanda.iceberg.delete` topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

ifndef::env-cloud[]
*Default:* `true`
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_delete[]

=== iceberg_disable_automatic_snapshot_expiry

Whether to disable automatic Iceberg snapshot expiry. This property may be useful if the Iceberg catalog expects to perform snapshot expiry on its own.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

// tag::iceberg_disable_snapshot_tagging[]
=== iceberg_disable_snapshot_tagging

Whether to disable tagging of Iceberg snapshots. These tags are used to ensure that the snapshots that Redpanda writes are retained during snapshot removal, which in turn, helps Redpanda ensure exactly-once delivery of records. Disabling tags is therefore not recommended, but it may be useful if the Iceberg catalog does not support tags.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

ifndef::env-cloud[]
*Default:* `false`
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_disable_snapshot_tagging[] 

// tag::iceberg_enabled[] 
=== iceberg_enabled

ifndef::env-cloud[]
Enables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-enabled[`redpanda.iceberg.enabled`] topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.
endif::[]

ifdef::env-cloud[]
Enables the translation of topic data into Iceberg tables. Setting `iceberg_enabled` to `true` activates the feature at the cluster level, but each topic must also set the `redpanda.iceberg.enabled` topic-level property to `true` to use it. If `iceberg_enabled` is set to `false`, then the feature is disabled for all topics in the cluster, overriding any topic-level settings.
endif::[]

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

ifndef::env-cloud[]
*Default:* `false`
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_enabled[]

// tag::iceberg_invalid_record_action[]
=== iceberg_invalid_record_action

ifndef::env-cloud[]
Default value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-invalid-record-action[`redpanda.iceberg.invalid.record.action`] topic property.
endif::[]

ifdef::env-cloud[]
Default value for the `redpanda.iceberg.invalid.record.action` topic property.
endif::[]

*Requires restart:* No

*Visibility:* `user`

ifndef::env-cloud[]
*Default:* `dlq_table`
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc#manage-dead-letter-queue[Manage dead-letter queue]

---

// end::iceberg_invalid_record_action[]

// tag::iceberg_rest_catalog_authentication_mode[]
=== iceberg_rest_catalog_authentication_mode

The authentication mode for client requests made to the Iceberg catalog. Choose from: `none`, `bearer`, and `oauth2`. In `oauth2` mode, the credentials specified in `iceberg_rest_catalog_client_id` and `iceberg_rest_catalog_client_secret` are used to obtain a bearer token from the URI defined by `iceberg_rest_catalog_oauth2_server_uri.`. In `bearer` mode, the token specified in `iceberg_rest_catalog_token` is used unconditionally, and no attempts are made to refresh the token. 

ifdef::env-cloud[]
Redpanda recommends using `oauth2`.
endif::[]

*Requires restart:* Yes

*Visibility:* `user`

ifndef::env-cloud[]
*Default:* `none`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_authentication_mode[]

// tag::iceberg_rest_catalog_client_id[]
=== iceberg_rest_catalog_client_id

The client ID used to query the REST catalog API for the OAuth token. Required if catalog type is set to `rest`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_client_id[]

// tag::iceberg_rest_catalog_client_secret[] 
=== iceberg_rest_catalog_client_secret

Secret used with the client ID to query the OAuth token endpoint for Iceberg REST catalog authentication. Required if catalog type is set to `rest` and `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_client_secret[] 

// tag::iceberg_rest_catalog_crl[]
=== iceberg_rest_catalog_crl

The contents of a certificate revocation list for `iceberg_rest_catalog_trust`. Takes precedence over `iceberg_rest_catalog_crl_file`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

---

// end::iceberg_rest_catalog_crl[]

=== iceberg_rest_catalog_crl_file

Path to certificate revocation list for `iceberg_rest_catalog_trust_file`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

*Default:* `null`

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// tag::iceberg_rest_catalog_endpoint[]
=== iceberg_rest_catalog_endpoint

URL of Iceberg REST catalog endpoint.

NOTE: If you set <<iceberg_catalog_type,`iceberg_catalog_type`>> to `rest`, you must also set this property at the same time.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_endpoint[]

// tag::iceberg_rest_catalog_oauth2_server_uri[]
=== iceberg_rest_catalog_oauth2_server_uri

The OAuth URI used to retrieve access tokens for Iceberg REST catalog authentication. If left undefined, the deprecated Iceberg catalog endpoint `/v1/oauth/tokens` is used instead.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_oauth2_server_uri[]

// tag::iceberg_rest_catalog_oauth2_scope[]
=== iceberg_rest_catalog_oauth2_scope

The OAuth scope used to retrieve access tokens for Iceberg catalog authentication. Only meaningful when `iceberg_rest_catalog_authentication_mode` is set to `oauth2`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `principal_role:all`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_oauth2_scope[]

// tag::iceberg_rest_catalog_prefix[]
=== iceberg_rest_catalog_prefix

Prefix part of the Iceberg REST catalog URL. Prefix is appended to the catalog path, for example `/v1/\{prefix}/namespaces`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_prefix[] 

// tag::iceberg_rest_catalog_request_timeout_ms[]
=== iceberg_rest_catalog_request_timeout_ms

Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

ifndef::env-cloud[]
*Default:* `10000`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_request_timeout_ms[]

// tag::iceberg_rest_catalog_token[]
=== iceberg_rest_catalog_token

Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties <<iceberg_rest_catalog_client_id,`iceberg_rest_catalog_client_id`>>  and <<iceberg_rest_catalog_client_secret,`iceberg_rest_catalog_client_secret`>>.

Required if <<iceberg_rest_catalog_authentication_mode, `iceberg_rest_catalog_authentication_mode`>> is set to `bearer`.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

// end::iceberg_rest_catalog_token[]

// tag::iceberg_rest_catalog_trust[]
=== iceberg_rest_catalog_trust

The contents of a certificate chain to trust for the REST Iceberg catalog.

ifndef::env-cloud[]
Takes precedence over <<iceberg_rest_catalog_trust_file, `iceberg_rest_catalog_trust_file`>>.
endif::[]

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `null`
endif::[]

---

// end::iceberg_rest_catalog_trust[]

=== iceberg_rest_catalog_trust_file

Path to a file containing a certificate chain to trust for the REST Iceberg catalog.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* string

*Default:* `null`

**Related topics**:

- xref:manage:iceberg/use-iceberg-catalogs.adoc[]

---

=== iceberg_target_backlog_size

Average size per partition of the datalake translation backlog that the backlog controller tries to maintain. When the backlog size is larger than the set point, the backlog controller will increase the translation scheduling group priority.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `5242880`

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// tag::iceberg_target_lag_ms[]
=== iceberg_target_lag_ms

ifndef::env-cloud[]
Default value for the xref:reference:properties/topic-properties.adoc#redpanda-iceberg-target-lag-ms[`redpanda.iceberg.target.lag.ms`] topic property, which controls how often the data in an Iceberg table is refreshed with new data from the corresponding Redpanda topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.

endif::[]

ifdef::env-cloud[]
Default value for the `redpanda.iceberg.target.lag.ms` topic property, which controls how often the data in an Iceberg table is refreshed with new data from the corresponding Redpanda topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.

endif::[]

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

ifndef::env-cloud[]
*Default:* `60000`
endif::[]

**Related topics**:

- xref:manage:iceberg/about-iceberg-topics.adoc[]

---

// end::iceberg_target_lag_ms[]

=== id_allocator_batch_size

The ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `1000`

---

=== id_allocator_log_capacity

Capacity of the `id_allocator` log in number of batches. After it reaches `id_allocator_stm`, it truncates the log's prefix.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `100`

---

=== initial_retention_local_target_bytes_default

Initial local retention size target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then  all locally-retained data will be delivered to learner when joining the partition replica set.

*Unit*: bytes

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `null`

---

=== initial_retention_local_target_ms_default

Initial local retention time target for partitions of topics with xref:manage:tiered-storage.adoc[Tiered Storage] enabled. If no initial local target retention is configured, then all locally-retained data will be delivered to learner when joining the partition replica is set.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `null`

---

=== internal_topic_replication_factor

Target replication factor for internal topics.

*Unit*: number of replicas per topic.
*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `3`

---

=== join_retry_timeout_ms

Time between cluster join retries in milliseconds.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000`

---

=== kafka_batch_max_bytes

Maximum size of a batch processed by the server. If the batch is compressed, the limit applies to the compressed batch size.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `1048576`

---

=== kafka_connection_rate_limit

Maximum connections per second for one core. If `null` (the default), then the number of connections per second is unlimited.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-9223372036854775808`, `9223372036854775807`]

*Default:* `null`

---

=== kafka_connection_rate_limit_overrides

Overrides the maximum connections per second for one core for the specified IP addresses (for example, `['127.0.0.1:90', '50.20.1.1:40']`)

*Related topics*:

* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Default:* `null`

---

=== kafka_connections_max

Maximum number of Kafka client connections per broker. If `null`, the property is disabled.

*Unit*: number of Kafka client connections per broker

*Default*: null

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Related topics*:

* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]

---

=== kafka_connections_max_overrides

A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Default*: `{}` (empty list)

*Related topics*:

* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]

---

=== kafka_connections_max_per_ip

Maximum number of Kafka client connections per IP address, per broker. If `null`, the property is disabled.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `null`

*Related topics*:

* xref:manage:cluster-maintenance/configure-availability.adoc#limit-client-connections[Limit client connections]

---

=== kafka_enable_authorization

Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by <<enable_sasl,`enable_sasl`>>.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `null`

*Accepted Values:*

* `null`: Ignored. Authorization is enabled with <<enable_sasl,`enable_sasl`>>: `true`
* `true`: authorization is required.
* `false`: authorization is disabled.

*Related properties*:

* <<enable_sasl,enable_sasl>>
* `kafka_api[].authentication_method`

---

=== kafka_enable_describe_log_dirs_remote_storage

Whether to include Tiered Storage as a special remote:// directory in `DescribeLogDirs Kafka` API requests.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== kafka_enable_partition_reassignment

Enable the Kafka partition reassignment API.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== kafka_group_recovery_timeout_ms

Kafka group recovery timeout.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000` (30 sec)

---

=== kafka_max_bytes_per_fetch

Limit fetch responses to this many bytes, even if the total of partition bytes limits is higher.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `67108864`

---


=== kafka_memory_batch_size_estimate_for_fetch

The size of the batch used to estimate memory consumption for fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard. Larger sizes prevent running out of memory because of too many concurrent fetch requests.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `1048576`

---

=== kafka_memory_share_for_fetch

The share of Kafka subsystem memory that can be used for fetch read buffers, as a fraction of the Kafka subsystem memory amount.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* number

*Default:* `0.5`

---

=== kafka_mtls_principal_mapping_rules

Principal mapping rules for mTLS authentication on the Kafka API. If `null`, the property is disabled.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Default:* `null`

---

=== kafka_nodelete_topics

A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.

*Requires restart:* No

*Visibility:* `user`

*Type:* string array

*Default*: `['_redpanda.audit_log', '__consumer_offsets', '_schemas']`

*Related topics*:

* xref:develop:consume-data/consumer-offsets.adoc[Consumer Offsets]
* xref:manage:schema-registry.adoc[Schema Registry]

---

=== kafka_noproduce_topics

A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.

*Requires restart:* No

*Visibility:* `user`

*Type:* array

*Default*: `['_redpanda.audit_log']`

---

=== kafka_qdc_depth_alpha

Smoothing factor for Kafka queue depth control depth tracking.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `0.8`

---

=== kafka_qdc_depth_update_ms

Update frequency for Kafka queue depth control.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `7000`

---

=== kafka_qdc_enable

Enable Kafka queue depth control.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== kafka_qdc_idle_depth

Queue depth when idleness is detected in Kafka queue depth control.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `10`

---

=== kafka_qdc_latency_alpha

Smoothing parameter for Kafka queue depth control latency tracking.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `0.002`

---

=== kafka_qdc_max_depth

Maximum queue depth used in Kafka queue depth control.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `100`

---

=== kafka_qdc_max_latency_ms

Maximum latency threshold for Kafka queue depth control depth tracking.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `80`

---

=== kafka_qdc_min_depth

Minimum queue depth used in Kafka queue depth control.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `1`

---

=== kafka_qdc_window_count

Number of windows used in Kafka queue depth control latency tracking.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `12`

---

=== kafka_qdc_window_size_ms

Window size for Kafka queue depth control latency tracking.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1500`

---


=== kafka_request_max_bytes

Maximum size of a single request processed using the Kafka API.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `104857600`

---


=== kafka_rpc_server_stream_recv_buf

Maximum size of the user-space receive buffer. If `null`, this limit is not applied.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== kafka_rpc_server_tcp_recv_buf

Size of the Kafka server TCP receive buffer. If `null`, the property is disabled.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `null`

---

=== kafka_rpc_server_tcp_send_buf

Size of the Kafka server TCP transmit buffer. If `null`, the property is disabled.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`] aligned to 4096 bytes

*Default:* `null`

---

=== kafka_sasl_max_reauth_ms

The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down.

IMPORTANT: If this property is not set (or set to `null`), session expiry is disabled, and a connection could live long after the client's credentials are expired or revoked.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `null`

---

=== kafka_schema_id_validation_cache_capacity

Per-shard capacity of the cache for validating schema IDs.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `128`

---

=== kafka_tcp_keepalive_timeout

TCP keepalive idle timeout in seconds for Kafka connections. This describes the timeout between TCP keepalive probes that the remote site successfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed, applies to new connections only.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `120`

---


=== kafka_tcp_keepalive_probe_interval_seconds

TCP keepalive probe interval in seconds for Kafka connections. This describes the timeout between unacknowledged TCP keepalives. Refers to the TCP_KEEPINTVL socket option. When changed, applies to new connections only.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `60`

---

=== kafka_tcp_keepalive_probes

TCP keepalive unacknowledged probes until the connection is considered dead for Kafka connections. Refers to the TCP_KEEPCNT socket option. When changed, applies to new connections only.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `3`

---

=== kafka_throughput_control

List of throughput control groups that define exclusions from node-wide throughput limits. Clients excluded from node-wide throughput limits are still potentially subject to client-specific throughput limits.

Each throughput control group consists of:

* `name` (optional) - any unique group name
* `client_id` - regex to match client_id

Example values:

* `[{'name': 'first_group','client_id': 'client1'}, {'client_id': 'consumer-\d+'}]`
* `[{'name': 'catch all'}]`
* `[{'name': 'missing_id', 'client_id': '+empty'}]`

A connection is assigned the first matching group and is then excluded from throughput control. A `name` is not required, but can help you categorize the exclusions. Specifying `+empty` for the `client_id` will match on clients that opt not to send a `client_id`. You can also optionally omit the `client_id` and specify only a `name`, as shown. In this situation, all clients will match the rule and Redpanda will exclude them from all from node-wide throughput control.

*Requires restart:* No

*Visibility:* `user`

*Type:* string array

*Accepted Values:* list of control groups of the format `{'name' : 'group name', 'client_id' : 'regex pattern'}`

*Default*: `[]` (empty list)

*Related topics*:

* xref:manage:cluster-maintenance/manage-throughput.adoc[Manage throughput]

---

=== kafka_throughput_controlled_api_keys

List of Kafka API keys that are subject to cluster-wide and node-wide throughput limit control.

*Requires restart:* No

*Visibility:* `user`

*Type:* list<string>

*Default:* `["produce", "fetch"]`

---

=== kafka_throughput_limit_node_in_bps

The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.

*Unit:* bytes per second

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-9223372036854775808`, `9223372036854775807`]

*Default:* `null`

*Related topics*:

* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]

---

=== kafka_throughput_limit_node_out_bps

The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.

*Unit:* bytes per second

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-9223372036854775808`, `9223372036854775807`]

*Default:* `null`

*Related topics*:

* xref:manage:cluster-maintenance/manage-throughput.adoc#node-wide-throughput-limits[Node-wide throughput limits]

---

=== kafka_throughput_replenish_threshold

Threshold for refilling the token bucket as part of enforcing throughput limits.

This threshold is evaluated with each request for data. When the number of tokens to replenish exceeds this threshold, then tokens are added to the token bucket. This ensures that the atomic is not being updated for the token count with each request. The range for this threshold is automatically clamped to the corresponding throughput limit for ingress and egress.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* For ingress, [1, xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_in_bps[`kafka_throughput_limit_node_in_bps`]]. For egress, [1, xref:reference:cluster-properties.adoc#kafka_throughput_limit_node_out_bps[`kafka_throughput_limit_node_out_bps`]]

*Default:* `1`

*Related topics*:

* xref:manage:cluster-maintenance/manage-throughput.adoc[Manage Throughput]

---

=== kvstore_flush_interval

Key-value store flush interval (in milliseconds).

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10`

---

=== kvstore_max_segment_size

Key-value maximum segment size (in bytes).

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `16777216`

---

=== leader_balancer_idle_timeout

Leadership rebalancing idle timeout.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `120000` (2 min)

---

=== leader_balancer_mute_timeout

The length of time that a glossterm:Raft[] group is muted after a leadership rebalance operation. Any group that has been moved, regardless of whether the move succeeded or failed, undergoes a cooling-off period. This prevents Raft groups from repeatedly experiencing leadership rebalance operations in a short time frame, which can lead to instability in the cluster.

The leader balancer maintains a list of muted groups and reevaluates muted status at the start of each balancing iteration. Muted groups still contribute to overall cluster balance calculations although they can't themselves be moved until the mute period is over. 

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `300000` (5 min)

*Related topics*:

* xref:manage:cluster-maintenance/cluster-balancing.adoc[]

---

=== leader_balancer_node_mute_timeout

The duration after which a broker that hasn't sent a heartbeat is considered muted. This timeout sets a threshold for identifying brokers that shouldn't be targeted for leadership transfers when the cluster rebalances, for example, because of unreliable network connectivity.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `20000` (20 s)

*Related topics*:

* xref:manage:cluster-maintenance/cluster-balancing.adoc[]

---

=== leader_balancer_transfer_limit_per_shard

Per shard limit for in-progress leadership transfers.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `512`

---

=== legacy_group_offset_retention_enabled

Group offset retention is enabled by default starting in Redpanda version 23.1. To enable offset retention after upgrading from an older version, set this option to true.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== legacy_permit_unsafe_log_operation

Flag to enable a Redpanda cluster operator to use unsafe control characters within strings, such as consumer group names or user names. This flag applies only for Redpanda clusters that were originally on version 23.1 or earlier and have been upgraded to version 23.2 or later. Starting in version 23.2, newly-created Redpanda clusters ignore this property.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

*Related properties*: <<legacy_unsafe_log_warning_interval_sec,legacy_unsafe_log_warning_interval_sec>>

---

=== legacy_unsafe_log_warning_interval_sec

Period at which to log a warning about using unsafe strings containing control characters. If unsafe strings are permitted by `legacy_permit_unsafe_log_operation`, a warning will be logged at an interval specified by this property.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `300`

*Related properties*: <<legacy_permit_unsafe_log_operation,legacy_permit_unsafe_log_operation>>

---

=== log_cleanup_policy

Default cleanup policy for topic logs.

The topic property xref:./topic-properties.adoc#cleanuppolicy[`cleanup.policy`] overrides the value of `log_cleanup_policy` at the topic level.

*Requires restart:* No

*Visibility:* `user`

*Accepted Values*: `compact`, `delete`, `compact,delete`

*Default:* `delete`

---


=== log_compaction_adjacent_merge_self_compaction_count

The number of self compactions that must occur before an adjacent compaction is attempted in the log. If set to `null`, every segment in the log must be self-compacted before an adjacent compaction is attempted.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `10`

---

=== log_compaction_interval_ms

How often to trigger background compaction.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

=== log_compaction_use_sliding_window

Use sliding window compaction.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== log_compression_type

Default topic compression type.

The topic property xref:./topic-properties.adoc#compressiontype[`compression.type`] overrides the value of `log_compression_type` at the topic level.

*Requires restart:* No

*Visibility:* `user`

*Accepted Values:* `gzip`, `snappy`, `lz4`, `zstd`, `producer`, `none`.

*Default:* `producer`

---

=== log_disable_housekeeping_for_tests

Disables the housekeeping loop for local storage. This property is used to simplify testing, and should not be set in production.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== log_message_timestamp_alert_after_ms

Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning the messages are in the future relative to the broker's clock.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `7200000` (2 h)

---

=== log_message_timestamp_alert_before_ms

Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning the messages are in the past relative to the broker's clock. To disable this check, set to `null`.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `null`

---

=== log_message_timestamp_type

Default timestamp type for topic messages (CreateTime or LogAppendTime).

The topic property xref:./topic-properties.adoc#messagetimestamptype[`message.timestamp.type`] overrides the value of `log_message_timestamp_type` at the topic level.

*Requires restart:* No

*Visibility:* `user`

*Accepted Values:* `CreateTime`, `LogAppendTime`.

*Default:* `CreateTime`

---

=== log_retention_ms

The amount of time to keep a log file before deleting it (in milliseconds). If set to `-1`, no time limit is applied. This is a cluster-wide default when a topic does not set or disable xref:./topic-properties.adoc#retentionms[`retention.ms`].

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `604800000` (one week)

---


// tag::log_segment_ms[]
=== log_segment_ms

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

Default lifetime of log segments. If `null`, the property is disabled, and no default lifetime is set. Any value under 60 seconds (60000 ms) is rejected. This property can also be set in the Kafka API using the Kafka-compatible alias, `log.roll.ms`.

ifndef::env-cloud[]
The topic property xref:./topic-properties.adoc#segmentms[`segment.ms`] overrides the value of `log_segment_ms` at the topic level.
endif::[]

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

ifndef::env-cloud[]
*Default:* `1209600000` (2 weeks)

*Related properties*:

* <<log_segment_ms_min,`log_segment_ms_min`>>
* <<log_segment_ms_max,`log_segment_ms_max`>>
endif::[]

---

// end::log_segment_ms[]

=== log_segment_ms_max

Upper bound on topic `segment.ms`: higher values will be clamped to this value.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `31536000000` (one year)

---

=== log_segment_ms_min

Lower bound on topic `segment.ms`: lower values will be clamped to this value.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `600000` (10 min)

---


=== log_segment_size

Default log segment size in bytes for topics which do not set `segment.bytes`.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `134217728`

---

// end::log_segment_size[]

=== log_segment_size_jitter_percent

Random variation to the segment size limit used for each partition.

*Unit:* percent

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `65535`]

*Default:* `5`

---

=== log_segment_size_max

Upper bound on topic `segment.bytes`: higher values will be clamped to this limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `null`

---

=== log_segment_size_min

Lower bound on topic `segment.bytes`: lower values will be clamped to this limit.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `1048576`

---

=== lz4_decompress_reusable_buffers_disabled

Disable reusable preallocated buffers for LZ4 decompression.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== max_compacted_log_segment_size

Maximum compacted segment size after consolidation.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `5368709120`

---

=== max_concurrent_producer_ids

Maximum number of active producer sessions. When the threshold is passed, Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, its message batches are rejected, and an out of order sequence error is emitted. Consumers don't affect this setting.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `18446744073709551615`

---

=== max_in_flight_pandaproxy_requests_per_shard

Maximum number of in-flight HTTP requests to HTTP Proxy permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `500`

---

=== max_in_flight_schema_registry_requests_per_shard

Maximum number of in-flight HTTP requests to Schema Registry permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `500`

---

=== max_kafka_throttle_delay_ms

Fail-safe maximum throttle delay on Kafka requests.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000`

---

=== max_transactions_per_coordinator

Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).

For details, see xref:develop:transactions#transaction-usage-tips[Transaction usage tips].

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `18446744073709551615`

---

=== members_backend_retry_ms

Time between members backend reconciliation loop retries.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000` (5 s)

---

=== memory_abort_on_alloc_failure

If `true`, the Redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== metadata_dissemination_interval_ms

Interval for metadata dissemination batching.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---

=== metadata_dissemination_retries

Number of attempts to look up a topic's metadata-like shard before a request fails. This configuration controls the number of retries that request handlers perform when internal topic metadata (for topics like tx, consumer offsets, etc) is missing. These topics are usually created on demand when users try to use the cluster for the first time and it may take some time for the creation to happen and the metadata to propagate to all the brokers (particularly the broker handling the request). In the meantime Redpanda waits and retries. This configuration controls the number retries.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `30`

---

=== metadata_dissemination_retry_delay_ms

Delay before retrying a topic lookup in a shard or other meta tables.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `500`

---

=== metadata_status_wait_timeout_ms

Maximum time to wait in metadata request for cluster health to be refreshed.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `2000`

---

=== metrics_reporter_report_interval

Cluster metrics reporter report interval.

*Unit:* milliseconds
*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `86400000` (one day)

---

=== metrics_reporter_tick_interval

Cluster metrics reporter tick interval.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `60000` (1 min)

---

=== metrics_reporter_url

URL of the cluster metrics reporter.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* string

*Default:* `https://m.rp.vectorized.io/v2`

---

=== min_cleanable_dirty_ratio

The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic. The topic property `min.cleanable.dirty.ratio` overrides this value at the topic level.

*Requires restart:* No

*Visibility:* `user`

*Type:* number

*Default:* `0.5`

---

// tag::minimum_topic_replications[]
=== minimum_topic_replications

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

Minimum allowable replication factor for topics in this cluster. The set value must be positive, odd, and equal to or less than the number of available brokers. Changing this parameter only restricts newly-created topics. Redpanda returns an `INVALID_REPLICATION_FACTOR` error on any attempt to create a topic with a replication factor less than this property.

If you change the `minimum_topic_replications` setting, the replication factor of existing topics remains unchanged. However, Redpanda will log a warning on start-up with a list of any topics that have fewer replicas than this minimum. For example, you might see a message such as `Topic X has a replication factor less than specified minimum: 1 < 3`.

*Unit*: minimum number of replicas per topic

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`1`, `32767`]

ifndef::env-cloud[]
*Default:* `1`
endif::[]

---

// end::minimum_topic_replications[]

=== node_isolation_heartbeat_timeout

How long after the last heartbeat request a node will wait before considering itself to be isolated.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-9223372036854775808`, `9223372036854775807`]

*Default:* `3000`

---

=== node_management_operation_timeout_ms

Timeout for executing node management operations.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000` (5 s)

---

=== node_status_interval

Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.

*Unit:* milliseconds
*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `100`

---

=== node_status_reconnect_max_backoff_ms

Maximum backoff (in milliseconds) to reconnect to an unresponsive peer during node status liveness checks.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `15000`

---

=== oidc_clock_skew_tolerance

The amount of time (in seconds) to allow for when validating the expiry claim in the token.

*Unit*: seconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `30`

---

// tag::oidc_discovery_url[]
=== oidc_discovery_url

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

The URL pointing to the well-known discovery endpoint for the OIDC provider.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

ifdef::env-cloud[]
*Default:* `https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration`
endif::[]

---

// end::oidc_discovery_url[]

=== oidc_keys_refresh_interval

The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `3600000`

---

// tag::oidc_principal_mapping[]
=== oidc_principal_mapping

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

Rule for mapping JWT payload claim to a Redpanda user principal.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `$.sub`

*Related topics*:

* xref:manage:security/authentication.adoc#oidc[OpenID Connect authentication]
* xref:manage:kubernetes/security/authentication/k-authentication.adoc[OpenID Connect authentication in Kubernetes]

endif::[]

---

// end::oidc_principal_mapping[]

// tag::oidc_token_audience[]
=== oidc_token_audience

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

A string representing the intended recipient of the token.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

ifndef::env-cloud[]
*Default:* `redpanda`
endif::[]

---

// end::oidc_token_audience[]

=== partition_autobalancing_concurrent_moves

Number of partitions that can be reassigned at once.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `50`

---

=== partition_autobalancing_max_disk_usage_percent

NOTE: This property applies only when <<partition_autobalancing_mode,`partition_autobalancing_mode`>> is set to `continuous`.

When the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node.

*Unit*: percent of disk used

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `80`

*Related topics*:

* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]

---

=== partition_autobalancing_min_size_threshold

Minimum size of partition that is going to be prioritized when rebalancing a cluster due to the disk size threshold being breached. This value is calculated automatically by default.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== partition_autobalancing_mode

include::reference:partial$enterprise-licensed-property.adoc[]

Mode of xref:manage:cluster-maintenance/cluster-balancing.adoc[partition balancing] for a cluster.

*Requires restart:* No

*Visibility:* `user`

*Accepted values:* 

* `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an xref:get-started:licenses.adoc[enterprise license], and it is customized by <<partition_autobalancing_node_availability_timeout_sec,`partition_autobalancing_node_availability_timeout_sec`>> and <<partition_autobalancing_max_disk_usage_percent,`partition_autobalancing_max_disk_usage_percent`>> properties.
* `node_add`: partition balancing happens when a node is added.
* `off`: partition balancing is disabled. This option is not recommended for production clusters.

*Enterprise license required*: `continuous`

*Default:* `node_add`

*Related topics*:

* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]

---

=== partition_autobalancing_node_availability_timeout_sec

NOTE: This property applies only when <<partition_autobalancing_mode,`partition_autobalancing_mode`>> is set to `continuous`.

When a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `900` (15 min)

*Related topics*:

* xref:manage:cluster-maintenance/continuous-data-balancing.adoc[Configure Continuous Data Balancing]

---

=== partition_autobalancing_tick_interval_ms

Partition autobalancer tick interval.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000` (30 s)

---

=== partition_autobalancing_tick_moves_drop_threshold

If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the in-progress moves are fewer than 80.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `0.2`

---

=== partition_autobalancing_topic_aware

If `true`, Redpanda prioritizes balancing a topic’s partition replica count evenly across all brokers while it’s balancing the cluster’s overall partition count. Because different topics in a cluster can have vastly different load profiles, this better distributes the workload of the most heavily-used topics evenly across brokers.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default*: true

*Related topics*:

* xref:manage:cluster-maintenance/cluster-balancing.adoc[]

---

=== partition_manager_shutdown_watchdog_timeout

A threshold value to detect partitions which might have been stuck while shutting down. After this threshold, a watchdog in partition manager will log information about partition shutdown not making progress.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000`

---

=== pp_sr_smp_max_non_local_requests

Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the `seastar::smp_service_group` documentation).

See https://docs.seastar.io/master/[Seastar documentation^]

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `null`

---

=== quota_manager_gc_sec

Quota manager GC frequency in milliseconds.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000` (30 s)

---

=== raft_replica_max_flush_delay_ms

Maximum delay between two subsequent flushes. After this delay, the log is automatically force flushed.

*Unit*: milliseconds

*Requires restart:* No

*Nullable:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`1`, `17592186044415`]

*Default:* `100`

---

=== election_timeout_ms

Raft election timeout expressed in milliseconds.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1500`

---

=== raft_enable_longest_log_detection

Enables an additional step in leader election where a candidate is allowed to wait for all the replies from the broker it requested votes from. This may introduce a small delay when recovering from failure, but it prevents truncation if any of the replicas have more data than the majority.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== raft_enable_lw_heartbeat

Enables Raft optimization of heartbeats.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== raft_heartbeat_disconnect_failures

The number of failed heartbeats after which an unresponsive TCP connection is forcibly closed. To disable forced disconnection, set to 0.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `3`

---

=== raft_heartbeat_interval_ms

Number of milliseconds for Raft leader heartbeats.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`1`, `17592186044415`]

*Default:* `150`

---

=== raft_heartbeat_timeout_ms

Raft heartbeat RPC (remote procedure call) timeout. Raft uses a heartbeat mechanism to maintain leadership authority and to trigger leader elections. The `raft_heartbeat_interval_ms` is a periodic heartbeat sent by the partition leader to all followers to declare its leadership. If a follower does not receive a heartbeat within the `raft_heartbeat_timeout_ms`, then it triggers an election to choose a new partition leader.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---

=== raft_io_timeout_ms

Raft I/O timeout.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

=== raft_learner_recovery_rate

Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders. This rate limit is placed on a node sending data to a recovering node. Each sending node is limited to this rate. The recovering node accepts data as fast as possible according to the combined limits of all healthy nodes in the cluster. For example, if two nodes are sending data to the recovering node, and `raft_learner_recovery_rate` is 100 MB/sec, then the recovering node will recover at a rate of 200 MB/sec.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `104857600`

---


=== raft_max_buffered_follower_append_entries_bytes_per_shard

The total size of append entry requests that may be cached per shard, using the Raft-buffered protocol. When an entry is cached, the leader can continue serving requests because the ordering of the cached requests cannot change. When the total size of cached requests reaches the set limit, back pressure is applied to throttle producers.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `0`

---

=== raft_max_inflight_follower_append_entries_requests_per_shard

The maximum number of append entry requests that may be sent from Raft groups on a Seastar shard to the current node, and are awaiting a reply. This property replaces `raft_max_concurrent_append_requests_per_follower`.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1024`

---

=== raft_max_recovery_memory

Maximum memory that can be used for reads in Raft recovery process by default 15% of total memory.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `null`

---

=== raft_recovery_concurrency_per_shard

Number of partitions that may simultaneously recover data to a particular shard. This number is limited to avoid overwhelming nodes when they come back online after an outage.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `64`

---

=== raft_recovery_default_read_size

Specifies the default size of a read issued during Raft follower recovery.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `524288`

---

=== raft_recovery_throttle_disable_dynamic_mode

include::reference:partial$internal-use-property.adoc[]

Disables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems. 

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== raft_replica_max_flush_delay_ms

Maximum delay between two subsequent flushes. After this delay, the log is automatically force flushed.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `100`

---

=== raft_replica_max_pending_flush_bytes

Maximum number of bytes that are not flushed per partition. If the configured threshold is reached, the log is automatically flushed even if it has not been explicitly requested.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `262144`

---

=== raft_replicate_batch_window_size

Maximum size of requests cached for replication.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `1048576`

---

=== raft_smp_max_non_local_requests

Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the `seastar::smp_service_group` documentation).

See https://docs.seastar.io/master/[Seastar documentation^]
*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `null`

---

=== raft_timeout_now_timeout_ms

Timeout for Raft's timeout_now RPC. This RPC is used to force a follower to dispatch a round of votes immediately.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1000`

---

=== raft_transfer_leader_recovery_timeout_ms

Follower recovery timeout waiting period when transferring leadership.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

=== readers_cache_eviction_timeout_ms

Duration after which inactive readers are evicted from cache.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `30000`

---

=== readers_cache_target_max_size

Maximum desired number of readers cached per NTP. This a soft limit, meaning that a number of readers in cache may temporarily increase as cleanup is performed in the background.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `200`

---

=== reclaim_batch_cache_min_free

Minimum amount of free memory maintained by the batch cache background reclaimer.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `67108864`

---

=== reclaim_growth_window

Starting from the last point in time when memory was reclaimed from the batch cache, this is the duration during which the amount of memory to reclaim grows at a significant rate, based on heuristics about the amount of available memory.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---

=== reclaim_max_size

Maximum batch cache reclaim size.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `4194304`

---

=== reclaim_min_size

Minimum batch cache reclaim size.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `131072`

---

=== reclaim_stable_window

If the duration since the last time memory was reclaimed is longer than the amount of time specified in this property, the memory usage of the batch cache is considered stable, so only the minimum size (<<reclaim_min_size,`reclaim_min_size`>>) is set to be reclaimed.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

=== recovery_append_timeout_ms

Timeout for append entry requests issued while updating a stale follower.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000`

---

=== release_cache_on_segment_roll

Flag for specifying whether or not to release cache when a full segment is rolled.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== replicate_append_timeout_ms

Timeout for append entry requests issued while replicating entries.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `3000`

---

=== retention_bytes

Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied.

The topic property xref:./topic-properties.adoc#retentionbytes[`retention.bytes`] overrides the value of `retention_bytes` at the topic level.

*Unit*: bytes per partition.

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `null`

---


=== retention_local_strict

Flag to allow Tiered Storage topics to expand to consumable retention policy limits. When this flag is enabled, non-local retention settings are used, and local retention settings are used to inform data removal policies in low-disk space scenarios.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== retention_local_strict_override

Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== retention_local_target_bytes_default

Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled.

This property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention].

NOTE: Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.

*Related properties*:

* <<retention_local_target_ms_default,retention_local_target_ms_default>>

*Unit*: bytes

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Default:* `null`

---


=== retention_local_target_capacity_bytes

The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded.

NOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.

*Unit*: percentage of total disk size

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `null`

---

=== retention_local_target_capacity_percent

The target capacity in percent of unreserved space (<<disk_reservation_percent,`disk_reservation_percent`>>) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.

NOTE: Redpanda Data recommends setting only one of <<retention_local_target_capacity_bytes,`retention_local_target_capacity_bytes`>> or <<retention_local_target_capacity_percent,`retention_local_target_capacity_percent`>>. If both are set, the minimum of the two is used as the effective target capacity.

*Unit*: percentage of total disk size

*Requires restart:* No

*Visibility:* `user`

*Type:* number

*Default:* `80.0`

---

=== retention_local_target_ms_default

Local retention time target for partitions of topics with object storage write enabled.

This property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. See xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention].

NOTE: Both <<retention_local_target_bytes_default,`retention_local_target_bytes_default`>> and <<retention_local_target_ms_default,`retention_local_target_ms_default`>> can be set. The limit that is reached first is applied.

*Related properties*:

* <<retention_local_target_bytes_default,retention_local_target_bytes_default>>

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `86400000` (one day)

---


=== retention_local_trim_interval

The period during which disk usage is checked for disk pressure, and data is optionally trimmed to meet the target.

*Unit*: milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `17592186044415`]

*Default:* `30000` (30 s)

---

=== retention_local_trim_overage_coeff

The space management control loop reclaims the overage multiplied by this this coefficient to compensate for data that is written during the idle period between control loop invocations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* number

*Default:* `2.0`

---

=== rm_sync_timeout_ms

Resource manager's synchronization timeout. Specifies the maximum time for this node to wait for the internal state machine to catch up with all events written by previous leaders before rejecting a request.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---

=== rpc_client_connections_per_peer

The maximum number of connections a broker will open to each of its peers.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `128`

---

=== rpc_server_compress_replies

Enable compression for internal RPC (remote procedure call) server replies.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== rpc_server_listen_backlog

Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.

*Unit*: number of queue entries
*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `null`

---

=== rpc_server_tcp_recv_buf

Internal RPC TCP receive buffer size. If `null` (the default value), no buffer size is set by Redpanda.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `null`

---

=== rpc_server_tcp_send_buf

Internal RPC TCP send buffer size. If `null` (the default value), then no buffer size is set by Redpanda.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `null`

---

=== rpk_path

Path to RPK binary.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* string

*Default:* `/usr/bin/rpk`

---

=== rps_limit_acls_and_users_operations

Rate limit for controller ACLs and user's operations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1000`

---

=== rps_limit_configuration_operations

Rate limit for controller configuration operations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1000`

---

=== rps_limit_move_operations

Rate limit for controller move operations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1000`

---

=== rps_limit_node_management_operations

Rate limit for controller node management operations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1000`

---

=== rps_limit_topic_operations

Rate limit for controller topic operations.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `1000`

---

=== memory_enable_memory_sampling

When `true`, memory allocations are sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, Redpanda will periodically log the top-n allocation sites.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== sasl_kerberos_config

The location of the Kerberos `krb5.conf` file for Redpanda.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

*Default:* `/etc/krb5.conf`

---

=== sasl_kerberos_keytab

The location of the Kerberos keytab file for Redpanda.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

*Default:* `/var/lib/redpanda/redpanda.keytab`

---

=== sasl_kerberos_principal

The primary of the Kerberos Service Principal Name (SPN) for Redpanda.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

*Default:* `redpanda`

---

=== sasl_kerberos_principal_mapping

Rules for mapping Kerberos principal names to Redpanda user principals.

*Requires restart:* No

*Visibility:* `user`

*Type:* string array

*Default:* `[default]`

---

// tag::sasl_mechanisms[]
=== sasl_mechanisms

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

ifndef::env-cloud[]
include::reference:partial$enterprise-licensed-property.adoc[]
endif::[]

A list of supported SASL mechanisms. Accepted values: `SCRAM`, `GSSAPI`, `OAUTHBEARER`, `PLAIN`.  Note that in order to enable PLAIN, you must also enable SCRAM.

*Requires restart:* No

*Visibility:* `user`

*Type:* string array

*Accepted values*: `SCRAM`, `GSSAPI`, `OAUTHBEARER`, `PLAIN`

ifndef::env-cloud[]
*Enterprise license required*:  `GSSAPI`, `OAUTHBEARER`

*Default:* `[SCRAM]`
endif::[]

---

// end::sasl_mechanisms[]

=== schema_registry_normalize_on_startup

Normalize schemas as they are read from the topic on startup.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---


=== segment_appender_flush_timeout_ms

Maximum delay until buffered data is written.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1000` (1 s)

---

=== segment_fallocation_step

Size for segments fallocation.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `33554432`

---

=== space_management_enable

Option to explicitly disable automatic disk space management. If this property was explicitly disabled while using v23.2, it will remain disabled following an upgrade.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `true`

---

=== space_management_enable_override

Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== space_management_max_log_concurrency

Maximum parallel logs inspected during space management process.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `65535`]

*Default:* `20`

---

=== space_management_max_segment_concurrency

Maximum parallel segments inspected during space management process.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `65535`]

*Default:* `10`

---

=== storage_compaction_index_memory

Maximum number of bytes that may be used on each shard by compaction index writers.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `134217728`

---

=== storage_compaction_key_map_memory

Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `134217728`

---

=== storage_compaction_key_map_memory_limit_percent

Limit on <<storage_compaction_key_map_memory,`storage_compaction_key_map_memory`>>, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. 

NOTE: Memory per shard is computed after <<data_transforms_per_core_memory_reservation,`data_transforms_per_core_memory_reservation`>>, and only applies when <<log_compaction_use_sliding_window,`log_compaction_use_sliding_window`>> is set to `true`.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* number

*Default:* `12.0`

---

=== storage_ignore_cstore_hints

When set, cstore hints are ignored and not used for data access (but are otherwise generated).

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

=== storage_ignore_timestamps_in_future_sec

The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda.

When determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime).

By default, `storage_ignore_timestamps_in_future_sec` is disabled (null).

[TIP]
====
To figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system:

. Look for logs with segments that are unexpectedly large and not being cleaned up.
. In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized.
. If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed.
. If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.
====

*Unit*: seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `null`

---

=== storage_max_concurrent_replay

Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `1024`

---

=== storage_min_free_bytes

Threshold of minimum bytes free space before rejecting producers.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `5368709120`

---

=== storage_read_buffer_size

Size of each read buffer (one per in-flight read, per log segment).

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `131072`

---

=== storage_read_readahead_count

How many additional reads to issue ahead of current read location.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `1`

---

=== storage_reserve_min_segments

The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-32768`, `32767`]

*Default:* `2`

---

=== storage_space_alert_free_threshold_bytes

Threshold of minimum bytes free space before setting storage space alert.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `0`

---

=== storage_space_alert_free_threshold_percent

Threshold of minimum percent free space before setting storage space alert.

*Unit:* percent

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `5`

---

=== storage_strict_data_init

Requires that an empty file named `.redpanda_data_dir` be present in the xref:reference:properties/broker-properties.adoc#data_directory[`data_ directory`]. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.

*Requires restart:* No

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== storage_target_replay_bytes

Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints.

*Unit:* bytes

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `10737418240`

---

=== superusers

List of superuser usernames.

*Requires restart:* No

*Visibility:* `user`

*Type:* string

*Default:* `null`

---

=== tls_certificate_name_format

The format of the certificates's distinguished name to use for mTLS principal mapping. The `legacy` format would appear as 'C=US,ST=California,L=San Francisco,O=Redpanda,CN=redpanda', while the `rfc2253` format would appear as 'CN=redpanda,O=Redpanda,L=San Francisco,ST=California,C=US'.

*Requires restart:* No

*Visibility:* `user`

*Default:* `legacy`

---

=== tls_enable_renegotiation

TLS client-initiated renegotiation is considered unsafe and is disabled by default . Only re-enable it if you are experiencing issues with your TLS-enabled client. This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* boolean

*Default:* `false`

---

// tag::tls_min_version[]
=== tls_min_version

ifdef::env-cloud[]
NOTE: This property is read-only in Redpanda Cloud.
endif::[]

The minimum TLS version that Redpanda clusters support. This property prevents client applications from negotiating a downgrade to the TLS version when they make a connection to a Redpanda cluster.

*Requires restart:* Yes

*Visibility:* `user`

*Accepted values:* `v1.0`, `v1.1`, `v1.2`, `v1.3`

*Type:* string

ifndef::env-cloud[]
*Default:* `v1.2`
endif::[]

---

// end::tls_min_version[]

=== tm_sync_timeout_ms

Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `10000`

---


=== tombstone_retention_ms

The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`. A typical default setting is `86400000`, or 24 hours.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`1`, `17592186044415`]

*Default:* `null`

ifndef::env-cloud[]
*Related topics:* xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]
endif::[]

---


=== topic_fds_per_partition

File descriptors required per partition replica. If topic creation results in the ratio of file descriptor limit to partition replicas being lower than this value, creation of new topics is fails.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `5`

---

=== topic_memory_per_partition

Required memory in bytes per partition replica when creating or altering topics. The total size of the memory pool for partitions is the total memory available to Redpanda times `topic_partitions_memory_allocation_percent`. Each partition created requires `topic_memory_per_partition` bytes from that pool. If insufficient memory is available, creating or altering topics fails.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `204800`

---

=== topic_partitions_memory_allocation_percent

Percentage of total memory to reserve for topic partitions. See <<topic_memory_per_partition, `topic_memory_per_partition`>> for details.

*Unit:* percent

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `10`

---

=== topic_partitions_per_shard

Maximum number of partition replicas per shard. If topic creation results in the ratio of partition replicas to shards being higher than this value, creation of new topics fails.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `5000`

---

=== topic_partitions_reserve_shard0

Reserved partition slots on shard (CPU core) 0 on each node.  If this is greater than or equal to <<topic_partitions_per_core,`topic_partitions_per_core`>>, no data partitions will be scheduled on shard 0.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `4294967295`]

*Default:* `0`

---


=== transaction_coordinator_cleanup_policy

Cleanup policy for a transaction coordinator topic.

*Requires restart:* No

*Visibility:* `user`

*Type:* string array

*Accepted Values*: `compact`, `delete`, `["compact","delete"]`, `none`

*Default:* `delete`

---


=== transaction_coordinator_delete_retention_ms

Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than <<transactional_id_expiration_ms,`transactional_id_expiration_ms`>>.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `604800000` (10080 min)

---


=== transaction_coordinator_log_segment_size

The size (in bytes) each log segment should be.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `1073741824` (1 GB)

---


=== transaction_coordinator_partitions

Number of partitions for transactions coordinator.

*Unit:* number of partitions per topic

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-2147483648`, `2147483647`]

*Default:* `50`

---


=== transaction_max_timeout_ms

The maximum allowed timeout for transactions. If a client-requested transaction timeout exceeds this configuration, the broker returns an error during transactional producer initialization. This guardrail prevents hanging transactions from blocking consumer progress.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `900000`

---


=== transactional_id_expiration_ms

Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `604800000` (10080 min)

---


=== tx_timeout_delay_ms

Delay before scheduling the next check for timed out transactions.

*Unit:* milliseconds

*Requires restart:* Yes

*Visibility:* `user`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `1000`

---

=== unsafe_enable_consumer_offsets_delete_retention

Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda support.

*Requires restart:* Yes

*Visibility:* `user`

*Type:* boolean

*Default:* `false`

---

=== usage_disk_persistance_interval_sec

The interval in which all usage stats are written to disk.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `300` (5 min)

---

=== usage_num_windows

The number of windows to persist in memory and disk.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Default:* `24`

---

=== usage_window_width_interval_sec

The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval.

*Unit:* seconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17179869184`, `17179869183`]

*Default:* `3600`

---

=== use_fetch_scheduler_group

Use a separate scheduler group for fetch processing.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== use_kafka_handler_scheduler_group

Use a separate scheduler group to handle parsing Kafka protocol requests.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== use_produce_scheduler_group

Use a separate scheduler group to process Kafka produce requests.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* boolean

*Default:* `true`

---

=== virtual_cluster_min_producer_ids

Minimum number of active producers per virtual cluster.

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`0`, `18446744073709551615`]

*Default:* `18446744073709551615`

---

=== wait_for_leader_timeout_ms

Timeout to wait for leadership in metadata cache.

*Unit:* milliseconds

*Requires restart:* No

*Visibility:* `tunable`

*Type:* integer

*Accepted values:* [`-17592186044416`, `17592186044415`]

*Default:* `5000`

---


=== write_caching_default

The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. 

Fsyncs follow <<raft_replica_max_pending_flush_bytes,`raft_replica_max_pending_flush_bytes`>> and <<raft_replica_max_flush_delay_ms,`raft_replica_max_flush_delay_ms`>>, whichever is reached first.

The `write_caching_default` cluster property can be overridden with the xref:reference:properties/topic-properties.adoc#writecaching[`write.caching`] topic property.

*Requires restart:* no

*Type*: string

*Accepted values:*

* `true`
* `false`
* `disabled`: This takes precedence over topic overrides and disables write caching for the entire cluster.

*Default*: For clusters in production mode, the default is `false`. For clusters in development mode, the default is `true`.

*Related topics*:

* xref:develop:config-topics.adoc#configure-write-caching[Write caching]

---

=== zstd_decompress_workspace_bytes

Size of the zstd decompression workspace.

*Unit:* bytes

*Requires restart:* Yes

*Visibility:* `tunable`

*Type:* integer

*Default:* `8388608`

---


