// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1396244485/Review+Redpanda+configuration+properties
// tag::category-retention-compaction[]
=== cleanup.policy

The cleanup policy to apply for log segments of a topic.
When `cleanup.policy` is set, it overrides the cluster property xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`] for the topic.

*Type:* string

*Accepted values:* [`delete`, `compact`, `compact,delete`]

*Related cluster property:* xref:reference:cluster-properties.adoc#log_cleanup_policy[log_cleanup_policy]

*Nullable:* No

*Related topics:*

* xref:cluster-properties.adoc#log_cleanup_policy[`log_cleanup_policy`]

---
// end::category-retention-compaction[]

=== cloud_storage_inventory_hash_path_directory

Directory to store inventory report hashes for use by cloud storage scrubber. If not specified, Redpanda uses a default path within the data directory.

*Type:* string

*Default:* `null`

*Nullable:* No


.Example
[,yaml]
----
redpanda:
  cloud_storage_inventory_hash_store: <inventory-hash-directory-path>
----


---

=== cloud_storage_materialized_manifest_ttl_ms

The interval, in milliseconds, determines how long the materialized manifest can stay in the cache under contention. This setting is used for performance tuning. When the spillover manifest is materialized and stored in the cache, and the cache needs to evict it, it uses this value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval, after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.

*Type:* string

*Default:* `null`

*Nullable:* No

---

// tag::category-retention-compaction[]
=== compaction.strategy

Specifies the strategy used to determine which records to remove during log compaction. The compaction strategy controls how Redpanda identifies and removes duplicate records while preserving the latest value for each key.

*Type:* string

*Related cluster property:* xref:reference:cluster-properties.adoc#compaction_strategy[compaction_strategy]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#compaction_strategy[`compaction_strategy`]

---
// end::category-retention-compaction[]

// tag::category-segment-message[]
=== compression.type

Redpanda ignores this property and always uses producer compression semantics. If producers send compressed data, Redpanda stores and serves it as-is. If producers send uncompressed data, Redpanda stores it uncompressed.

This property exists for Apache Kafka compatibility. Configure compression in your producers instead of using this topic property.

Compression reduces message size and improves throughput, but increases CPU utilization. Enable producer batching to increase compression efficiency.

When set, this property overrides the cluster property xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`] for the topic.

*Type:* string

*Accepted values:* [`none`, `gzip`, `snappy`, `lz4`, `zstd`]

*Related cluster property:* xref:reference:cluster-properties.adoc#log_compression_type[log_compression_type]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]
* xref:./cluster-properties.adoc#log_compression_type[`log_compression_type`]
* xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]
* xref:develop:produce-data/configure-producers.adoc#commonly-used-producer-configuration-options[Common producer configuration options]

---
// end::category-segment-message[]

// tag::category-schema-registry[]
=== confluent.key.schema.validation

Enable validation of the schema ID for keys on a record. This is a compatibility alias for `redpanda.key.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's key is registered in the Schema Registry according to the configured subject name strategy.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-schema-registry[]
=== confluent.key.subject.name.strategy

The subject name strategy for keys when `confluent.key.schema.validation` is enabled. This is a compatibility alias for `redpanda.key.subject.name.strategy` that determines how the topic and schema are mapped to a subject name in the Schema Registry.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-schema-registry[]
=== confluent.value.schema.validation

Enable validation of the schema ID for values on a record. This is a compatibility alias for `redpanda.value.schema.id.validation`. When enabled, Redpanda validates that the schema ID encoded in the record's value is registered in the Schema Registry according to the configured subject name strategy.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-schema-registry[]
=== confluent.value.subject.name.strategy

The subject name strategy for values when `confluent.value.schema.validation` is enabled. This is a compatibility alias for `redpanda.value.subject.name.strategy`. This determines how the topic and schema are mapped to a subject name in the Schema Registry.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-retention-compaction[]
=== delete.retention.ms

The retention time for tombstone records in a compacted topic. Redpanda removes tombstone records after the retention limit is exceeded.

If you have enabled Tiered Storage and set <<redpandaremoteread,`redpanda.remote.read`>> or <<redpandaremotewrite,`redpanda.remote.write`>> for the topic, you cannot enable tombstone removal.

If both `delete.retention.ms` and the cluster property config_ref:tombstone_retention_ms,true,properties/cluster-properties[] are set, `delete.retention.ms` overrides the cluster level tombstone retention for an individual topic.

*Type:* string

*Related cluster property:* xref:reference:cluster-properties.adoc#delete_retention_ms[delete_retention_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#tombstone_retention_ms[`tombstone_retention_ms`]
* xref:manage:cluster-maintenance/compaction-settings.adoc#tombstone-record-removal[Tombstone record removal]

---
// end::category-retention-compaction[]

// tag::category-performance-cluster[]
=== flush.bytes

The maximum bytes not fsynced per partition. If this configured threshold is reached, the log is automatically fsynced, even though it wasn't explicitly requested.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#flush_bytes[flush_bytes]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#flush_bytes[`flush_bytes`]

---
// end::category-performance-cluster[]

// tag::category-performance-cluster[]
=== flush.ms

The maximum delay (in ms) between two subsequent fsyncs. After this delay, the log is automatically fsynced.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#flush_ms[flush_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#flush_ms[`flush_ms`]

---
// end::category-performance-cluster[]

// tag::category-tiered-storage[]
=== initial.retention.local.target.bytes

A size-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#initial_retention_local_target_bytes[initial_retention_local_target_bytes]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#initial_retention_local_target_bytes[`initial_retention_local_target_bytes`]
* xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-tiered-storage[]
=== initial.retention.local.target.ms

A time-based initial retention limit for Tiered Storage that determines how much data in local storage is transferred to a partition replica when a cluster is resized. If `null` (default), all locally retained data is transferred.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#initial_retention_local_target_ms[initial_retention_local_target_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#initial_retention_local_target_ms[`initial_retention_local_target_ms`]
* xref:manage:tiered-storage.adoc#fast-commission-and-decommission[Fast commission and decommission through Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-retention-compaction[]
=== max.compaction.lag.ms

The maximum amount of time (in ms) that a log segment can remain unaltered before it is eligible for compaction in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`] for the topic.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#max_compaction_lag_ms[max_compaction_lag_ms]

*Nullable:* No

*Related topics:*

* xref:cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]
* xref:./cluster-properties.adoc#max_compaction_lag_ms[`max_compaction_lag_ms`]
* xref:manage:cluster-maintenance/compaction-settings.adoc#configuration-options[Configure maximum compaction lag]

---
// end::category-retention-compaction[]

// tag::category-segment-message[]
=== max.message.bytes

The maximum size of a message or batch of a topic. If a compression type is enabled, `max.message.bytes` sets the maximum size of the compressed message or batch.

If `max.message.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`] for the topic.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#kafka_batch_max_bytes[kafka_batch_max_bytes]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]
* xref:./cluster-properties.adoc#kafka_batch_max_bytes[`kafka_batch_max_bytes`]
* xref:develop:produce-data/configure-producers.adoc#message-batching[Message batching]

---
// end::category-segment-message[]

// tag::category-segment-message[]
=== message.timestamp.type

The source of a message's timestamp: either the message's creation time or its log append time.

When `message.timestamp.type` is set, it overrides the cluster property xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`] for the topic.

*Type:* string

*Accepted values:* [`CreateTime`, `LogAppendTime`]

*Related cluster property:* xref:reference:cluster-properties.adoc#log_message_timestamp_type[log_message_timestamp_type]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]
* xref:./cluster-properties.adoc#log_message_timestamp_type[`log_message_timestamp_type`]

---
// end::category-segment-message[]

// tag::category-retention-compaction[]
=== min.cleanable.dirty.ratio

The minimum ratio between the number of bytes in dirty segments and the total number of bytes in closed segments that must be reached before a partition's log is eligible for compaction in a compact topic.

*Type:* number

*Accepted values:* [`0`, `1.0`]

*Related cluster property:* xref:reference:cluster-properties.adoc#min_cleanable_dirty_ratio[min_cleanable_dirty_ratio]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#min_cleanable_dirty_ratio[`min_cleanable_dirty_ratio`]

---
// end::category-retention-compaction[]

// tag::category-retention-compaction[]
=== min.compaction.lag.ms

The minimum amount of time (in ms) that a log segment must remain unaltered before it can be compacted in a compact topic. Overrides the cluster property xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`] for the topic.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#min_compaction_lag_ms[min_compaction_lag_ms]

*Nullable:* No

*Related topics:*

* xref:cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]
* xref:./cluster-properties.adoc#min_compaction_lag_ms[`min_compaction_lag_ms`]
* xref:manage:cluster-maintenance/compaction-settings.adoc#configure-min-compaction-lag[Configure minimum compaction lag]

---
// end::category-retention-compaction[]

// tag::category-tiered-storage[]
=== redpanda.cloud_topic.enabled

No description available.

*Type:* string

*Nullable:* No

---
// end::category-tiered-storage[]

// tag::category-iceberg-integration[]
=== redpanda.iceberg.delete

Whether the corresponding Iceberg table is deleted upon deleting the topic.

*Type:* string

*Nullable:* No

---
// end::category-iceberg-integration[]

// tag::category-iceberg-integration[]
=== redpanda.iceberg.invalid.record.action

Whether to write invalid records to a dead-letter queue (DLQ).

*Type:* string

*Nullable:* No

*Related topics:*

* xref:manage:iceberg/about-iceberg-topics.adoc#troubleshoot-errors[Troubleshoot errors]

---
// end::category-iceberg-integration[]

// tag::category-iceberg-integration[]
=== redpanda.iceberg.mode

Enable the Iceberg integration for the topic. You can choose one of four modes.

*Type:* string

*Nullable:* No

*Related topics:*

* xref:manage:iceberg/choose-iceberg-mode.adoc#override-value-schema-latest-default[Choose an Iceberg Mode]

---
// end::category-iceberg-integration[]

// tag::category-iceberg-integration[]
=== redpanda.iceberg.partition.spec

The link:https://iceberg.apache.org/docs/nightly/partitioning/[partitioning^] specification for the Iceberg table.

*Type:* string

*Nullable:* No

*Related topics:*

* xref:manage:iceberg/about-iceberg-topics.adoc#use-custom-partitioning[Use custom partitioning]

---
// end::category-iceberg-integration[]

// tag::category-iceberg-integration[]
=== redpanda.iceberg.target.lag.ms

Controls how often the data in the Iceberg table is refreshed with new data from the topic. Redpanda attempts to commit all data produced to the topic within the lag target, subject to resource availability.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Nullable:* No

---
// end::category-iceberg-integration[]

// tag::category-schema-registry[]
=== redpanda.key.schema.id.validation

No description available.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-schema-registry[]
=== redpanda.key.subject.name.strategy

No description available.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-performance-cluster[]
=== redpanda.leaders.preference

The preferred location (rack) for partition leaders of a topic.

This property inherits the value from the config_ref:default_leaders_preference,true,properties/cluster-properties[] cluster configuration property. You may override the cluster-wide setting by specifying the value for individual topics.

If the cluster configuration property config_ref:enable_rack_awareness,true,properties/cluster-properties[] is set to `false`, Leader Pinning is disabled across the cluster.

*Type:* string

*Nullable:* No

*Related topics:*

* xref:develop:produce-data/leader-pinning.adoc[Leader pinning]

---
// end::category-performance-cluster[]

// tag::category-other[]
// tag::exclude-from-docs[]
=== redpanda.remote.allowgaps

No description available.

*Type:* string

*Nullable:* No

---
// end::exclude-from-docs[]
// end::category-other[]

// tag::category-tiered-storage[]
=== redpanda.remote.delete

A flag that enables deletion of data from object storage for Tiered Storage when it's deleted from local storage for a topic.

NOTE: `redpanda.remote.delete` doesn't apply to Remote Read Replica topics: a Remote Read Replica topic isn't deleted from object storage when this flag is `true`.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Nullable:* No

*Related topics:*

* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-tiered-storage[]
=== redpanda.remote.read

A flag for enabling Redpanda to fetch data for a topic from object storage to local storage. When set to `true` together with <<redpandaremotewrite, `redpanda.remote.write`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Nullable:* No

*Related topics:*

* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-remote-read-replica[]
=== redpanda.remote.readreplica

The name of the object storage bucket for a Remote Read Replica topic.

CAUTION: Setting `redpanda.remote.readreplica` together with either `redpanda.remote.read` or `redpanda.remote.write` results in an error.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Nullable:* No

*Related topics:*

* xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---
// end::category-remote-read-replica[]

// tag::category-tiered-storage[]
=== redpanda.remote.recovery

A flag that enables the recovery or reproduction of a topic from object storage for Tiered Storage. The recovered data is saved in local storage, and the maximum amount of recovered data is determined by the local storage retention limits of the topic.

TIP: You can only configure `redpanda.remote.recovery` when you create a topic. You cannot apply this setting to existing topics.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Nullable:* No

*Related topics:*

* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-tiered-storage[]
=== redpanda.remote.write

A flag for enabling Redpanda to upload data for a topic from local storage to object storage. When set to `true` together with <<redpandaremoteread, `redpanda.remote.read`>>, it enables the xref:manage:tiered-storage.adoc[Tiered Storage] feature.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Nullable:* No

*Related topics:*

* xref:manage:tiered-storage.adoc[Tiered Storage]
* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-schema-registry[]
=== redpanda.value.schema.id.validation

No description available.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-schema-registry[]
=== redpanda.value.subject.name.strategy

No description available.

*Type:* string

*Nullable:* No

---
// end::category-schema-registry[]

// tag::category-other[]
// tag::exclude-from-docs[]
=== redpanda.virtual.cluster.id

No description available.

*Type:* string

*Nullable:* No

---
// end::exclude-from-docs[]
// end::category-other[]

// tag::category-performance-cluster[]
=== replication.factor

The number of replicas of a topic to save in different nodes (brokers) of a cluster.

If `replication.factor` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication] for the topic.

NOTE: Although `replication.factor` isn't returned or displayed by xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`] as a valid Kafka property, you can set it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]. When the `replication.factor` of a topic is altered, it isn't simply a property value that's updated, but rather the actual replica sets of topic partitions that are changed.

*Type:* integer

*Accepted values:* integer (1 or greater)

*Related cluster property:* xref:reference:cluster-properties.adoc#replication_factor[replication_factor]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#default_topic_replication[default_topic_replication]
* xref:reference:rpk/rpk-topic/rpk-topic-describe.adoc[`rpk topic describe`]
* xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]
* xref:./cluster-properties.adoc#default_topic_replication[`default_topic_replication`]
* xref:develop:config-topics.adoc#choose-the-replication-factor[Choose the replication factor]
* xref:develop:config-topics.adoc#change-the-replication-factor[Change the replication factor]

---
// end::category-performance-cluster[]

// tag::category-retention-compaction[]
=== retention.bytes

A size-based retention limit that configures the maximum size that a topic partition can grow before becoming eligible for cleanup.

If `retention.bytes` is set to a positive value, it overrides the cluster property xref:cluster-properties.adoc#retention_bytes[`retention_bytes`] for the topic, and the total retained size for the topic is `retention.bytes` multiplied by the number of partitions for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, cleanup occurs when either limit is reached.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#retention_bytes[retention_bytes]

*Nullable:* No

*Related topics:*

* xref:cluster-properties.adoc#retention_bytes[`retention_bytes`]
* xref:./cluster-properties.adoc#retention_bytes[`retention_bytes`]
* xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---
// end::category-retention-compaction[]

// tag::category-tiered-storage[]
=== retention.local.target.bytes

A size-based retention limit for Tiered Storage that configures the maximum size that a topic partition in local storage can grow before becoming eligible for cleanup. It applies per partition and is equivalent to <<retentionbytes, `retention.bytes`>> without Tiered Storage.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#retention_local_target_bytes[retention_local_target_bytes]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#retention_local_target_bytes[`retention_local_target_bytes`]
* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-tiered-storage[]

// tag::category-tiered-storage[]
=== retention.local.target.ms

A time-based retention limit for Tiered Storage that sets the maximum duration that a log's segment file for a topic is retained in local storage before it's eligible for cleanup. This property is equivalent to <<retentionms, `retention.ms`>> without Tiered Storage.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#retention_local_target_ms[retention_local_target_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#retention_local_target_ms[`retention_local_target_ms`]
* xref:manage:tiered-storage.adoc[Tiered Storage]
* xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---
// end::category-tiered-storage[]

// tag::category-retention-compaction[]
=== retention.ms

A time-based retention limit that configures the maximum duration that a log's segment file for a topic is retained before it becomes eligible to be cleaned up. To consume all data, a consumer of the topic must read from a segment before its `retention.ms` elapses, otherwise the segment may be compacted and/or deleted. If a non-positive value, no per-topic limit is applied.

If `retention.ms` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`] for the topic.

When both size-based (`retention.bytes`) and time-based (`retention.ms`) retention limits are set, the earliest occurring limit applies.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#retention_ms[retention_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]
* xref:./cluster-properties.adoc#log_retention_ms[`log_retention_ms`]
* xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]

---
// end::category-retention-compaction[]

// tag::category-segment-message[]
=== segment.bytes

The maximum size of an active log segment for a topic. When the size of an active segment exceeds `segment.bytes`, the segment is closed and a new active segment is created. The closed, inactive segment is then eligible to be cleaned up according to retention properties.

When `segment.bytes` is set to a positive value, it overrides the cluster property xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`] for the topic.

*Type:* integer

*Accepted values:* bytes (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#log_segment_size[log_segment_size]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]
* xref:./cluster-properties.adoc#log_segment_size[`log_segment_size`]
* xref:manage:cluster-maintenance/disk-utilization.adoc#configure-segment-size[Configure segment size]
* xref:manage:cluster-maintenance/disk-utilization.adoc#configure-message-retention[Configure message retention]
* xref:manage:remote-read-replicas.adoc[Remote Read Replicas]

---
// end::category-segment-message[]

// tag::category-segment-message[]
=== segment.ms

The maximum duration that a log segment of a topic is active (open for writes and not deletable). A periodic event, with `segment.ms` as its period, forcibly closes the active segment and transitions, or rolls, to a new active segment. The closed (inactive) segment is then eligible to be cleaned up according to cleanup and retention properties.

If set to a positive duration, `segment.ms` overrides the cluster property xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]. Values are automatically clamped between the cluster bounds set by xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`] (default: 10 minutes) and xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`] (default: 1 year). If your configured value exceeds these bounds, Redpanda uses the bound value and logs a warning. Check current cluster bounds with `rpk cluster config get log_segment_ms_min log_segment_ms_max`.

*Type:* integer

*Accepted values:* milliseconds (integer)

*Related cluster property:* xref:reference:cluster-properties.adoc#segment_ms[segment_ms]

*Nullable:* No

*Related topics:*

* xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]
* xref:./cluster-properties.adoc#log_segment_ms_min[`log_segment_ms_min`]
* xref:./cluster-properties.adoc#log_segment_ms_max[`log_segment_ms_max`]
* xref:./cluster-properties.adoc#log_segment_ms[`log_segment_ms`]
* xref:manage:cluster-maintenance/disk-utilization.adoc#log-rolling[Log rolling]

---
// end::category-segment-message[]

// tag::category-performance-cluster[]
=== write.caching

The write caching mode to apply to a topic.

When `write.caching` is set, it overrides the cluster property xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow <<flushms, `flush.ms`>> and <<flushbytes, `flush.bytes`>>, whichever is reached first.

*Type:* boolean

*Accepted values:* [`true`, `false`]

*Related cluster property:* xref:reference:cluster-properties.adoc#write_caching[write_caching]

*Nullable:* No

*Related topics:*

* xref:cluster-properties.adoc#write_caching_default[`write_caching_default`]
* xref:./cluster-properties.adoc#write_caching_default[`write_caching_default`]
* xref:develop:config-topics.adoc#configure-write-caching[Write caching]
* xref:manage:tiered-storage.adoc[Tiered Storage]

---
// end::category-performance-cluster[]
