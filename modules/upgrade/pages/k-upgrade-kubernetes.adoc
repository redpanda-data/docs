= Upgrade Kubernetes on Nodes Running Redpanda
:description: Learn how to upgrade your Kubernetes control plane and worker nodes while ensuring that your Redpanda cluster remains fault tolerant and data safe.
:page-aliases: manage:kubernetes/upgrade-kubernetes.adoc, manage:kubernetes/k-upgrade-kubernetes.adoc
:page-categories: Management, Upgrades
:env-kubernetes: true

Upgrading the Kubernetes version in a cluster ensures that your infrastructure is up to date and secure. This process involves updating the Kubernetes control plane and worker nodes, and ensuring that your Redpanda cluster remains available and protected from data loss.

== Prerequisites

* A staging environment in which to test the upgrade procedure before performing it in production.
* Familiarity with your hosting platform (GKE, EKS, AKS, or self-managed) and any CLI tools your platform provides (for example, `gcloud` or `eksctl`).
* Confirmation that the new version of Kubernetes is compatible with the version of Redpanda you are using. See the xref:upgrade:k-compatibility.adoc[].
* A backup of your Redpanda data in case of unexpected data loss.

== Pre-upgrade considerations

When replacing Kubernetes nodes under a running Redpanda cluster, consider the following:

* **Cost:** Be aware that changes in broker count may impact infrastructure costs.
* **Data retention:** Storage capacity and retention values depend largely on the local disk capacity across brokers.
* **Durability:** Your broker count should be one more than your highest partition replication factor.
* **Partition count:** This value is generally determined by the overall CPU core count of your cluster.
* **Node groups:** If your cluster spans multiple node groups, update the tolerations and taints so that new brokers are scheduled on the appropriate node group.

NOTE: In clusters with only three brokers and a replication factor of 3, upgrading a node without additional capacity risks losing quorum. To mitigate this risk, you must temporarily add a fourth broker (a buffer broker) so that during the upgrade the replication factor remains satisfied and the cluster stays healthy.

== Upgrade the Redpanda Helm chart

Before upgrading Kubernetes, always upgrade your Redpanda Helm chart or Redpanda CRD to the latest version. This ensures that any deprecated Kubernetes resources (e.g. the `v1beta1` PodDisruptionBudget in Kubernetes 1.25) are replaced with updated ones. If you skip this step, you may see errors during or after the Kubernetes upgrade.

For more details, see xref:upgrade:k-rolling-upgrade.adoc[].

== Choose your storage strategy

When upgrading nodes, Kubernetes restarts or replaces each node's underlying virtual machine. This process can cause temporary unavailability of the Pods running on those nodes. Therefore, it's important to choose an upgrade procedure that takes into account the location of your Redpanda data to minimize disruption:

* <<networked-pv, Networked (remountable) PersistentVolumes>>: For example, EBS on AWS, Persistent Disks on GCP, or Azure Disks.
* <<local-pv, Ephemeral local storage>>: For example, local PersistentVolumes, `hostPath`, or `emptyDir`.

[[networked-pv]]
=== Networked (remountable) PersistentVolumes

When using network-backed PersistentVolumes (PVs), the data is stored on external storage systems such as EBS, Persistent Disks, or Azure Disks. This storage remains intact even if the node is replaced, making the upgrade process more straightforward than for ephemeral disks. Follow these steps to ensure a smooth upgrade while maintaining data integrity and cluster availability:

. Confirm that a safe Pod Disruption Budget (PDB) exists:
+
[source,bash]
----
kubectl get pdb --namespace <namespace>
----
+
Setting `maxUnavailable` to 1 ensures that only one broker Pod can be unavailable during the upgrade process. This setting helps maintain cluster availability and data integrity by preventing multiple Pods from being disrupted simultaneously.
+
Example output:
+
[.no-copy]
----
NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
redpanda   N/A             1                 1                     2m36s
----

. Ensure each topic has a replication factor of at least 3:
+
[source,bash]
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
A replication factor of 3 is recommended to ensure high availability and fault tolerance. With three replicas, the system can tolerate the failure of one broker without losing data or availability. This setup allows for one replica to be down for maintenance or due to failure, while still having two replicas available to serve read and write requests. This redundancy is crucial for maintaining the health and reliability of the cluster.

. If you only have 3 brokers, update your Redpanda resource/Helm overrides to temporarily scale the StatefulSet replica count to 4. Also, ensure that an extra node is available for the new broker.
+
:note-caption: Why add an extra node?
[NOTE]
====
The recommended topic replication factor is 3. This means each partition within a topic must have three replicas to maintain cluster health. To avoid making your cluster unhealthy, add a temporary broker before upgrading any of the three original nodes. This temporary broker is not needed if you have more brokers than your highest topic replication factor.
====
:note-caption: Note

. Disable controllers:
+
[tabs]
======
Helm + Operator::
+
--

Scale down the operator to 0 to temporarily disable it:

```bash
kubectl scale deployment redpanda-operator --replicas=0 --namespace <namespace>
```

--
Helm::
+
--
If you enabled the NodeWatcher or Decommission controller in the Redpanda Helm chart:

[loweralpha]
. Edit or remove the settings for the `controllers` sidecar in your Helm values overrides. For example, if you used a YAML values file:
+
.`remove-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: false
      run: []
rbac:
  enabled: false
----

. Run `helm upgrade` to apply the changes.

--
======

. Perform a rolling upgrade of your nodes.
+
Follow your hosting platform's guidelines:
+
* https://cloud.google.com/kubernetes-engine/docs-how-to/upgrading-a-cluster[Upgrade on GKE]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed]

. Wait for the nodes to be ready:
+
[source,bash]
----
kubectl wait --for=condition=Ready node/<node-name> --timeout=300s
----
+
Check each node's status and events for any issues.
+
[,bash]
----
kubectl describe node <node-name>
kubectl get events --namespace <namespace>
----

. If you disabled the controllers, re-enable them. For example:
+
[tabs]
======
Helm + Operator::
+
--

Scale up the operator to re-enable it:

```bash
kubectl scale deployment redpanda-operator --replicas=1 --namespace <namespace>
```

--
Helm::
+
--
.`enable-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: true
      run:
        - "nodeWatcher"
        - "decommission"
----
--
======

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.
+
NOTE: You may need to run this command a few times before consistently getting a healthy output. Each broker has its own view of the cluster state, and the controller leader may not have replicated the actual cluster state to the broker providing the results for this command. Eventually, all brokers should agree on the same cluster state.

. If you created a buffer broker, decommission that broker, scale the StatefulSet replica count back to 3, and delete the extra node.

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----

With network-backed storage, your Redpanda data volumes remain intact and available when a node goes offline. After a node reboots or a new node is brought up, the PV reattaches, ensuring your Redpanda cluster can recover without data loss.

[[local-pv]]
=== Ephemeral local storage (local PV, hostPath, or emptyDir)

When using ephemeral storage, such as local PersistentVolumes (PVs), `hostPath`, or `emptyDir`, the data is stored on the node's local disks. This means that when a node is replaced or restarted, the data on that node is lost. To ensure data integrity and cluster availability during the upgrade process, follow these steps:

[mermaid]
----
flowchart TB
  %% Define classes
  classDef userAction stroke:#374D7C, fill:#E2EBFF, font-weight:bold, rx:5, ry:5;
  classDef systemAction fill:#F6FBF6, stroke:#25855a, stroke-width:2px, color:#20293c, rx:5, ry:5;
  classDef decision fill:#FFF3CD, stroke:#FFEEBA, font-weight:bold, rx:5, ry:5;

  %% Main flow for a single broker upgrade (controllers remain disabled until all brokers are upgraded)
  A((Start)):::systemAction --> B["Verify PDB & replication factor"]:::userAction
  B --> C{Broker count is only 3?}:::decision
  C -- Yes --> D{Extra node available for temporary broker?}:::decision
  D -- Yes --> E["Scale up replicas to 4 (add buffer broker)"]:::userAction
  D -- No --> F["Provision extra node"]:::userAction
  E --> H["Disable controllers"]:::userAction
  F --> E
  C --> H
  H --> I["Decommission broker on node to be upgraded"]:::userAction
  I --> J["Wait for data migration to other replicas"]:::systemAction
  J --> K["Delete associated PVC (local PV)"]:::userAction
  K --> L["Upgrade node (VM replacement)"]:::userAction
  L --> M["Wait for node to be Ready & new broker Pod is scheduled"]:::systemAction
  M --> N["Verify cluster health"]:::userAction
  N --> O["Broker upgrade complete"]:::systemAction
  O --> P{More brokers to upgrade?}:::decision
  P -- Yes --> Q["Proceed with next broker upgrade"]:::userAction
  Q --> I
  P -- No --> R["Re-enable controllers"]:::userAction
  R --> S["Verify cluster health"]:::userAction
  S --> T((Process complete)):::systemAction
----

. Confirm that a safe Pod Disruption Budget (PDB) exists:
+
[source,bash]
----
kubectl get pdb --namespace <namespace>
----
+
Setting `maxUnavailable` to 1 ensures that only one broker Pod can be unavailable during the upgrade process. This setting helps maintain cluster availability and data integrity by preventing multiple Pods from being disrupted simultaneously.
+
Example output:
+
[.no-copy]
----
NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
redpanda   N/A             1                 1                     2m36s
----

. Ensure each topic has a replication factor of at least 3:
+
[source,bash]
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
A replication factor of 3 is recommended to ensure high availability and fault tolerance. With three replicas, the system can tolerate the failure of one broker without losing data or availability. This setup allows for one replica to be down for maintenance or due to failure, while still having two replicas available to serve read and write requests. This redundancy is crucial for maintaining the health and reliability of the cluster.


. If you only have 3 brokers, update your Redpanda resource/Helm overrides to temporarily scale the StatefulSet replica count to 4. Also, ensure that an extra node is available for the new broker.
+
:note-caption: Why add an extra node?
[NOTE]
====
The recommended topic replication factor is 3. This means each partition within a topic must have three replicas to maintain cluster health. To avoid making your cluster unhealthy, add a temporary broker before decommissioning any of the three original brokers. This temporary broker is not needed if you have more brokers than your highest topic replication factor.
====
:note-caption: Note

. Disable controllers:
+
[tabs]
======
Helm + Operator::
+
--

Scale down the operator to 0 to temporarily disable it:

```bash
kubectl scale deployment redpanda-operator --replicas=0 --namespace <namespace>
```

--
Helm::
+
--
If you enabled the NodeWatcher or Decommission controller in the Redpanda Helm chart:

[loweralpha]
Edit or remove the settings for the `controllers` sidecar in your Helm values overrides. For example, if you used a YAML values file:

.`controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: false
      run: []
rbac:
  enabled: false
----

[,bash]
----
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values controllers.yaml --reuse-values
----

--
======

. Decommission the broker on the target node to be upgraded. This moves its data and replicas to other brokers.
+
[source,bash]
----
rpk redpanda admin brokers decommission <broker-id>
----
+
The time required depends on the amount of data being migrated from the broker's partitions. If the broker has a large amount of data, this process can take hours. For more details, see xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers].

. Delete the associated PVC:
+
For local PVs, delete the PVC to allow Kubernetes to provision new storage for the rescheduled broker.
+
[source,bash]
----
kubectl get pvc --namespace <namespace>
kubectl delete pvc <pvc-name> --namespace <namespace> --wait=false
----
+
NOTE: Deleting the PVC sets a deletion timestamp. The actual removal occurs when the Pod terminates. The `--wait=false` flag ensures the associated PersistentVolume is not deleted until the associated Pod is deleted so that the Redpanda broker can shut down cleanly.

. Upgrade the target node. Follow your hosting platform's guidelines:
+
* https://cloud.google.com/kubernetes-engine/docs-how-to/upgrading-a-cluster[Upgrade on GKE]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed]

. Wait for the upgraded node to be ready and for the new broker Pod to be scheduled:
+
[source,bash]
----
kubectl wait --for=condition=Ready node/<node-name> --timeout=300s
----
+
When the node rejoins the cluster, Kubernetes can schedule a new broker Pod there. If you deleted the PVC, Kubernetes creates a new local PV to fulfill the storage requirement for the newly scheduled Pod.
+
If the Pod is not rescheduled, check the node's status and events for any issues.
+
[,bash]
----
kubectl describe node <node-name>
kubectl get events --namespace <namespace>
----

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.
+
NOTE: You may need to run this command a few times before consistently getting a healthy output. Each broker has its own view of the cluster state, and the controller leader may not have replicated the actual cluster state to the broker providing the results for the cluster health command. Eventually, all brokers should agree on the same cluster state.

. Repeat steps 6-10 for each broker in your cluster. Be sure to upgrade one node at a time before moving on to the next.

. If you disabled the controllers, re-enable them. For example:
+
[tabs]
======
Helm + Operator::
+
--

Scale up the operator to re-enable it:

```bash
kubectl scale deployment redpanda-operator --replicas=1 --namespace <namespace>
```

--
Helm::
+
--
.`controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: true
      run:
        - "nodeWatcher"
        - "decommission"
----

[,bash]
----
helm upgrade --install redpanda redpanda/redpanda --namespace <namespace> --create-namespace \
  --values controllers.yaml --reuse-values
----
--
======

. If you created a buffer broker, decommission that broker, scale the StatefulSet replica count back to 3, and delete the extra node.

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----

Because the data resides on ephemeral local disks, upgrading the node without decommissioning the broker or deleting PVCs can lead to data being stuck on that node or lost when the local disk is wiped. Ensuring a higher replication factor and decommissioning the broker moves your data to other nodes, so you can safely bring the node down and start fresh without losing or corrupting data.

include::shared:partial$suggested-reading.adoc[]

* xref:troubleshoot:errors-solutions/k-resolve-errors.adoc[]
* xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers]
* xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]
* https://kubernetes.io/docs/concepts/workloads/pods/disruptions/[Kubernetes Pod Disruption Budgets^]
* https://helm.sh/docs/[Helm documentation^]