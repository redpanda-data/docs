= Upgrade Kubernetes on Nodes Running Redpanda
:description: Learn how to upgrade your Kubernetes control plane and worker nodes while ensuring that Redpanda remains fault tolerant and data safe.
:page-aliases: manage:kubernetes/upgrade-kubernetes.adoc, manage:kubernetes/k-upgrade-kubernetes.adoc
:page-categories: Management, Upgrades
:env-kubernetes: true

Upgrading the Kubernetes version in a cluster ensures that your infrastructure is up to date and secure. This process involves updating the Kubernetes control plane and worker nodes while ensuring that your Redpanda cluster remains available and protected from data loss.

== Prerequisites

* A staging environment in which to test the upgrade procedure before performing it in production.
* A running Redpanda deployment on a Kubernetes cluster.
* Familiarity with your hosting platform (GKE, EKS, AKS, or self-managed) and any CLI tools your platform provides (for example, `gcloud` or `eksctl`).
* Confirmation that the new version of Kubernetes is compatible with the version of Redpanda you are using. See the xref:reference:k-redpanda-helm-spec.adoc#requirements[Helm chart requirements].
* Upgraded Redpanda Helm chart to the latest version. See the next section.

=== Upgrade the Redpanda Helm chart

Before upgrading Kubernetes, always upgrade your Redpanda Helm chart to the latest version. Doing this ensures that any deprecated Kubernetes resources (for example, the `v1beta1` PodDisruptionBudget in Kubernetes 1.25) are replaced with updated ones. If you skip this step, Helm may fail during or after the Kubernetes upgrade.

For more details, see xref:upgrade:k-rolling-upgrade.adoc[].

== Choose your storage strategy

When upgrading a node, Kubernetes restarts or replaces that node's underlying virtual machine, which can cause temporary unavailability for any Pods scheduled on that node.

Your upgrade procedure depends on where your Redpanda data resides:

* Networked PersistentVolumes. For example, EBS on AWS, Persistent Disks on GCP, or Azure Disks on Azure.
* Ephemeral local storage. For example, local PersistentVolumes, `hostPath`, or `emptyDir`.

=== If you use PersistentVolumes with network-backed storage

PersistentVolumes (PVs) that rely on a networked storage backend decouple your Redpanda data from a specific node. As a result, you can safely upgrade nodes without the risk of losing data stored locally on the node.

. Confirm that a Pod Disruption Budget (PDB) exists for your Redpanda StatefulSet. The Redpanda Helm chart creates one by default in xref:reference:k-redpanda-helm-spec.adoc#statefulsetbudgetmaxunavailable[`budget.maxUnavailable`].
+
A PDB limits the number of Pods that can be down simultaneously. This helps ensure your cluster maintains the desired level of redundancy during a node upgrade. Hosting platforms such as EKS and GKE also respect the PDB during node upgrades, providing a safer upgrade process.

. Check your replication factor:
+
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
Ensure each topic has a replication factor of at least 3. If you have topics with a replication factor of 1, increase it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`] and wait for all under-replicated partitions to become fully replicated.
+
If you increased the replication factor, xref:manage:kubernetes/monitoring/index.adoc#under-replicated-partitions[monitor for under-replicated partitions] and wait until all partitions are replicated.

. If you are using the Redpanda Operator, make sure that both the node watcher and decommission controllers are disabled before you drain and upgrade. This step avoids unintentional rebalancing or decommission events during node drain.

. Cordon and drain the node so that no new Redpanda Pods are scheduled there and existing Redpanda Pods are gracefully evicted.

. Upgrade the node.
+
Below are links to documentation from major hosting platforms describing how to upgrade Kubernetes:
+
* https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster[Upgrade on GKE (Google Kubernetes Engine)^]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS (Elastic Kubernetes Service)^]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS (Azure Kubernetes Service)^]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed^]

. Wait for the node to come back. Because your data resides on networked storage, the Redpanda Pods will reschedule and reconnect to the existing PV.

. If you disabled the node watcher or decommission controllers in step 3 re-enable them.

. Verify cluster health.
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.

With network-backed storage, your Redpanda data volumes remain intact and available when a node goes offline. After a node reboots or a new node is brought up, the PV reattaches, ensuring your Redpanda cluster can recover without data loss.

=== If you use ephemeral storage (local PV, `hostPath`, or `emptyDir`)

Ephemeral storage binds Redpanda data to the local disks of a specific node. When that node is upgraded or replaced, its local storage is typically lost. To protect your data and maintain fault tolerance, follow these steps.

Here's a high-level view of node upgrades when you use ephemeral storage:

[mermaid]
....
flowchart TB

%% Define classes
classDef userAction stroke:#374D7C, fill:#E2EBFF, font-weight:bold,rx:5,ry:5
classDef systemAction fill:#F6FBF6,stroke:#25855a,stroke-width:2px,color:#20293c,rx:5,ry:5

%% -- LEGEND SUBGRAPH --
subgraph Legend
  direction TB
  UA([User Action]):::userAction
  SE([System Event]):::systemAction
end

%% -- MAIN FLOWCHART --
A((Start)):::systemAction --> B["Check PDB and replication factor (≥3)"]:::userAction
B --> C["Decommission broker"]:::userAction
C --> D["Data migrates to other replicas"]:::systemAction
D --> E["Delete PVC (local PV)"]:::userAction
E --> F["Cordon and drain node"]:::userAction
F --> G["Broker shuts down"]:::systemAction
G --> H["Upgrade node"]:::userAction
H --> I["New broker Pod and new PVC (local PV)"]:::systemAction
I --> J["Data rebalances"]:::systemAction
J --> K((Cluster healthy)):::systemAction
....

. Confirm that a Pod Disruption Budget (PDB) exists for your Redpanda StatefulSet. The Redpanda Helm chart creates one by default in xref:reference:k-redpanda-helm-spec.adoc#statefulsetbudgetmaxunavailable[`budget.maxUnavailable`].
+
A PDB limits the number of Pods that can be down simultaneously. This helps ensure your cluster maintains the desired level of redundancy during a node upgrade. Hosting platforms such as EKS and GKE also respect the PDB during node upgrades, providing a safer upgrade process.

. Check your replication factor:
+
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
Ensure each topic has a replication factor of at least 3. If you have topics with a replication factor of 1, increase it using xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`] and wait for all under-replicated partitions to become fully replicated.
+
If you increased the replication factor, xref:manage:kubernetes/monitoring/index.adoc#under-replicated-partitions[monitor for under-replicated partitions] and wait until all partitions are replicated.

. If you are using the Redpanda Operator, make sure that both the node watcher and decommission controllers are disabled before you drain and upgrade. This avoids unintentional rebalancing or decommission events during node drain.

. Decommission the Redpanda broker Pod running on the node you plan to upgrade. This process migrates its data and replicas to other brokers. See xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers]. Wait for the process to complete before continuing.

. Delete the PersistentVolumeClaim (PVC).
+
If you use local PVs, you must delete the associated PVC so that Kubernetes can re-provision storage for the rescheduled broker.
+
NOTE: Deleting the PVC sets a `deletionTimestamp`, and the actual PVC removal won't happen until the Pod using it is terminated.

. Cordon and drain the node so that no new Redpanda Pods are scheduled there and existing Redpanda Pods are gracefully evicted.

. Upgrade the node.
+
Perform the node upgrade on your hosting platform. When the upgrade is complete, the node is rebooted or replaced. Below are links to documentation from major hosting platforms describing how to upgrade Kubernetes:
+
* https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster[Upgrade on GKE (Google Kubernetes Engine)^]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS (Elastic Kubernetes Service)^]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS (Azure Kubernetes Service)^]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed^]

. Wait for the node to come back.
+
When the node rejoins the cluster, Kubernetes can schedule a new broker Pod there. If you deleted the PVC, Kubernetes creates a new local PV to fulfill the storage requirement for the newly scheduled Pod.

. If you disabled the node watcher or decommission controllers in step 3 re-enable them.

. Verify cluster health.
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.

Because the data resides on ephemeral local disks, simply upgrading the node without decommissioning the broker or deleting PVCs can lead to data being “stuck” on that node or lost when the local disk is wiped. Ensuring a higher replication factor and decommissioning the broker moves your data to other nodes, so you can safely bring the node down and start fresh without losing or corrupting data.

== After the upgrade

After you have upgraded your Kubernetes cluster:

* Verify that your Redpanda cluster remains healthy and that data has been retained as expected.
* Confirm that all your brokers have rejoined the cluster and that there are no under-replicated partitions.

include::shared:partial$suggested-reading.adoc[]

* https://kubernetes.io/docs/concepts/workloads/pods/disruptions/[Kubernetes Pod Disruption Budgets^]
* https://helm.sh/docs/[Helm documentation^]
* xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers]
* xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]