= Upgrade Kubernetes on Nodes Running Redpanda
:description: Learn how to upgrade your Kubernetes control plane and worker nodes while ensuring that your Redpanda cluster remains fault tolerant and data safe.
:page-aliases: manage:kubernetes/upgrade-kubernetes.adoc, manage:kubernetes/k-upgrade-kubernetes.adoc
:page-categories: Management, Upgrades
:env-kubernetes: true

Upgrading the Kubernetes version in a cluster ensures that your infrastructure is up to date and secure. This process involves updating the Kubernetes control plane and worker nodes while ensuring that your Redpanda cluster remains available and protected from data loss.

== Prerequisites

* A staging environment in which to test the upgrade procedure before performing it in production.
* Familiarity with your hosting platform (GKE, EKS, AKS, or self-managed) and any CLI tools your platform provides (for example, `gcloud` or `eksctl`).
* Confirmation that the new version of Kubernetes is compatible with the version of Redpanda you are using. See the xref:upgrade:k-compatibility.adoc[].
* A backup of your Redpanda data in case of unexpected data loss.

== Pre-upgrade considerations

When replacing Kubernetes nodes under a running Redpanda cluster, consider the following:

* **Cost:** Be aware that changes in broker count may impact infrastructure costs.
* **Data retention:** Storage capacity and retention values depend largely on the local disk capacity across brokers.
* **Durability:** Your broker count should be one more than your highest partition replication factor.
* **Partition count:** This value is generally determined by the overall CPU core count of your cluster.
* **Node groups:** If your cluster spans multiple node groups, update the tolerations and taints so that new brokers are scheduled on the appropriate node group.

NOTE: In clusters with only three brokers and a replication factor of 3, upgrading a node without additional capacity risks losing quorum. To mitigate this risk, you must temporarily add a fourth broker (a buffer broker) so that during the upgrade the replication factor remains satisfied and the cluster stays healthy.

== Upgrade the Redpanda Helm chart

Before upgrading Kubernetes, always upgrade your Redpanda Helm chart to the latest version. This ensures that any deprecated Kubernetes resources (e.g. the `v1beta1` PodDisruptionBudget in Kubernetes 1.25) are replaced with updated ones. If you skip this step, Helm may fail during or after the Kubernetes upgrade.

For more details, see xref:upgrade:k-rolling-upgrade.adoc[].

== Choose your storage strategy

When upgrading a node, Kubernetes restarts or replaces that node's underlying virtual machine. This can temporarily make Pods unavailable. Choose an upgrade procedure based on where your Redpanda data resides:

* <<networked-pv, Networked (remountable) PersistentVolumes>>: For example, EBS on AWS, Persistent Disks on GCP, or Azure Disks.
* <<local-pv, Ephemeral local storage>>: For example, local PersistentVolumes, `hostPath`, or `emptyDir`.

=== If you use PersistentVolumes with network-backed storage

When using network-backed PersistentVolumes (PVs), the data is stored on external storage systems such as EBS, Persistent Disks, or Azure Disks. This storage remains intact even if the node is replaced, making the upgrade process more straightforward than for ephemeral disks. Follow these steps to ensure a smooth upgrade while maintaining data integrity and cluster availability:

[mermaid]
----
flowchart TB
  %% Define classes
  classDef userAction stroke:#374D7C, fill:#E2EBFF, font-weight:bold, rx:5, ry:5;
  classDef systemAction fill:#F6FBF6, stroke:#25855a, stroke-width:2px, color:#20293c, rx:5, ry:5;
  classDef decision fill:#FFF3CD, stroke:#FFEEBA, font-weight:bold, rx:5, ry:5;

  %% MAIN FLOW
  A((Start)):::systemAction --> B["Verify PDB\n& replication factor (≥3)"]:::userAction
  B --> C{Broker count is 3?}:::decision
  C -- Yes --> D{Extra node available for temporary broker?}:::decision
  D -- Yes --> E["Update StatefulSet to scale replicas to 4\n(add buffer broker)"]:::userAction
  D -- No --> F["Provision extra node for temporary broker,\nthen update StatefulSet to scale replicas to 4"]:::userAction
  C -- No --> G["Proceed with current replica count"]:::userAction
  E --> H["Disable controllers"]:::userAction
  F --> H
  G --> H
  H --> I["Cordon and drain node"]:::userAction
  I --> J["Upgrade node (VM replacement)"]:::userAction
  J --> K["Wait for node to be Ready"]:::userAction
  K --> L["Re-enable controllers"]:::userAction
  L --> M["Verify cluster health"]:::userAction
  M --> N((Cluster healthy)):::systemAction
----

. Confirm that a Pod Disruption Budget (PDB) exists for your Redpanda StatefulSet:
+
[source,bash]
----
kubectl get pdb --namespace <namespace>
----
+
Example output:
+
[.no-copy]
----
NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
redpanda   N/A             1                 1                     2m36s
----

. Check your replication factor:
+
[source,bash]
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
Ensure each topic's replication factor is at least 3.

. If you only have 3 brokers, update your Redpanda resource/Helm overrides to temporarily scale the StatefulSet replica count to 4. Also, ensure that an extra node is available for the new broker.

. Disable controllers:
+
[tabs]
======
Helm + Operator::
+
--

Scale down the operator to 0 to temporarily disable it:

```bash
kubectl scale deployment redpanda-operator --replicas=0 --namespace <namespace>
```

--
Helm::
+
--
If you enabled the NodeWatcher or Decommission controller in the Redpanda Helm chart:

[loweralpha]
. Edit or remove the settings for the `controllers` sidecar in your Helm values or command-line overrides. For example, if you used a YAML values file:
+
.`remove-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: false
      run: []
rbac:
  enabled: false
----

. Run `helm upgrade` to apply the changes.

--
======

. Upgrade all your nodes.
+
Follow your hosting platform's guidelines:
+
* https://cloud.google.com/kubernetes-engine/docs-how-to/upgrading-a-cluster[Upgrade on GKE]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed]

. Wait for all nodes to be ready:
+
[source,bash]
----
kubectl wait --for=condition=Ready node/<node-name> --timeout=300s
----

. If you disabled the controllers, re-enable them. For example:
+
[tabs]
======
Helm + Operator::
+
--

Scale up the operator to re-enable it:

```bash
kubectl scale deployment redpanda-operator --replicas=1 --namespace <namespace>
```

--
Helm::
+
--
.`enable-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: true
      run:
        - "nodeWatcher"
        - "decommission"
----
--
======

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.

. If you created a buffer broker, scale the StatefulSet replica count back to 3 and delete the extra node.

With network-backed storage, your Redpanda data volumes remain intact and available when a node goes offline. After a node reboots or a new node is brought up, the PV reattaches, ensuring your Redpanda cluster can recover without data loss.

=== If you use ephemeral storage (local PV, hostPath, or emptyDir)

When using ephemeral storage, such as local PersistentVolumes (PVs), `hostPath`, or `emptyDir`, the data is stored on the node's local disks. This means that when a node is replaced or restarted, the data on that node is lost. To ensure data integrity and cluster availability during the upgrade process, follow these steps:

[mermaid]
----
flowchart TB
  %% Define classes
  classDef userAction stroke:#374D7C, fill:#E2EBFF, font-weight:bold, rx:5, ry:5;
  classDef systemAction fill:#F6FBF6, stroke:#25855a, stroke-width:2px, color:#20293c, rx:5, ry:5;
  classDef decision fill:#FFF3CD, stroke:#FFEEBA, font-weight:bold, rx:5, ry:5;

  %% Main flow for a single broker upgrade (controllers remain disabled until all brokers are upgraded)
  A((Start)):::systemAction --> B["Verify PDB\n& replication factor (≥3)"]:::userAction
  B --> C{Broker count is 3?}:::decision
  C -- Yes --> D{Extra node available for temporary broker?}:::decision
  D -- Yes --> E["Update StatefulSet: scale replicas to 4\n(add buffer broker)"]:::userAction
  D -- No --> F["Provision extra node, then update StatefulSet:\nscale replicas to 4"]:::userAction
  C -- No --> G["Proceed with current replica count"]:::userAction
  E --> H["Disable controllers"]:::userAction
  F --> H
  G --> H
  H --> I["Decommission broker on node to be upgraded"]:::userAction
  I --> J["Wait for data migration to other replicas"]:::systemAction
  J --> K["Delete associated PVC (local PV)"]:::userAction
  K --> L["Cordon and drain node"]:::userAction
  L --> M["Upgrade node (VM replacement)"]:::userAction
  M --> N["Wait for node to be Ready & new broker Pod is scheduled"]:::systemAction
  N --> O["Verify cluster health"]:::userAction
  O --> P["Broker upgrade complete"]:::systemAction
  P --> Q{More brokers to upgrade?}:::decision
  Q -- Yes --> R["Proceed with next broker upgrade"]:::userAction
  R --> I
  Q -- No --> S["Re-enable controllers"]:::userAction
  S --> T["Verify cluster health"]:::userAction
  T --> U((Process complete)):::systemAction
----

. Confirm that a safe Pod Disruption Budget (PDB) exists:
+
[source,bash]
----
kubectl get pdb --namespace <namespace>
----
+
Example output:
+
[.no-copy]
----
NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
redpanda   N/A             1                 1                     2m36s
----

. Check your replication factor:
+
[source,bash]
----
rpk topic describe <topic-name>
rpk topic alter-config <topic-name> --set replication.factor=<replication-factor>
----
+
Ensure each topic has a replication factor of at least 3.

. If you only have 3 brokers, update your Redpanda resource/Helm overrides to temporarily scale the StatefulSet replica count to 4. Also, ensure that an extra node is available for the new broker.

. Disable controllers:
+
[tabs]
======
Helm + Operator::
+
--

Scale down the operator to 0 to temporarily disable it:

```bash
kubectl scale deployment redpanda-operator --replicas=0 --namespace <namespace>
```

--
Helm::
+
--
If you enabled the NodeWatcher or Decommission controller in the Redpanda Helm chart:

[loweralpha]
. Edit or remove the settings for the `controllers` sidecar in your Helm values or command-line overrides. For example, if you used a YAML values file:
+
.`remove-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: false
      run: []
rbac:
  enabled: false
----

. Run `helm upgrade` to apply the changes.

--
======

. Decommission the broker on the node to be upgraded. This moves its data and replicas to other brokers. See xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers].

. Wait for the decommissioning process to complete. The time required depends on the amount of data being migrated from the broker's partitions. If the broker has a large amount of data, this process can take hours.

. Delete the associated PVC:
+
For local PVs, delete the PVC to allow Kubernetes to provision new storage for the rescheduled broker.
+
[source,bash]
----
kubectl get pvc --namespace <namespace>
kubectl delete pvc <pvc-name> --namespace <namespace>
----
+
NOTE: Deleting the PVC sets a deletion timestamp. The actual removal occurs when the Pod terminates.

. Cordon and drain the node to prevent new Pods from scheduling and gracefully evict existing ones.
+
[,bash]
----
kubectl get nodes -o wide
kubectl cordon <node-name>
kubectl drain <node-name>
----

. Upgrade the node. Follow your hosting platform's guidelines:
+
* https://cloud.google.com/kubernetes-engine/docs-how-to/upgrading-a-cluster[Upgrade on GKE]
* https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html[Upgrade on EKS]
* https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli[Upgrade on AKS]
* https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/[Upgrade on Self-Managed]

. Wait for the node to be ready and for the new broker Pod to be scheduled:
+
[source,bash]
----
kubectl wait --for=condition=Ready node/<node-name> --timeout=300s
----
+
When the node rejoins the cluster, Kubernetes can schedule a new broker Pod there. If you deleted the PVC, Kubernetes creates a new local PV to fulfill the storage requirement for the newly scheduled Pod.

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.

. Repeat steps 5-11 for each broker in your cluster. Be sure to upgrade one node at a time before moving on to the next.

. If you disabled the controllers, re-enable them. For example:
+
[tabs]
======
Helm + Operator::
+
--

Scale up the operator to re-enable it:

```bash
kubectl scale deployment redpanda-operator --replicas=1 --namespace <namespace>
```

--
Helm::
+
--
.`enable-controllers.yaml`
[,yaml]
----
statefulset:
  sideCars:
    controllers:
      enabled: true
      run:
        - "nodeWatcher"
        - "decommission"
----
--
======

. If you created a buffer broker, scale the StatefulSet replica count back to 3 and delete the extra node.

. Verify cluster health:
+
[source,bash]
----
rpk cluster health
----
+
Use your monitoring stack or `rpk cluster health` to verify that all brokers are online and that data replication is healthy.

Because the data resides on ephemeral local disks, upgrading the node without decommissioning the broker or deleting PVCs can lead to data being stuck on that node or lost when the local disk is wiped. Ensuring a higher replication factor and decommissioning the broker moves your data to other nodes, so you can safely bring the node down and start fresh without losing or corrupting data.

include::shared:partial$suggested-reading.adoc[]

* https://kubernetes.io/docs/concepts/workloads/pods/disruptions/[Kubernetes Pod Disruption Budgets^]
* https://helm.sh/docs/[Helm documentation^]
* xref:manage:kubernetes/k-decommission-brokers.adoc[Decommission Brokers]
* xref:reference:rpk/rpk-topic/rpk-topic-alter-config.adoc[`rpk topic alter-config`]